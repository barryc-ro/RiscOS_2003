; converted to ObjAsm format commenting out original differences

;#include "fxptparams.h"
;#include "use_asm.h"

        GET     Hdr:ListOpts
        GET     Hdr:Macros
        GET     Hdr:APCS.<APCS>
	GET	hdr.fxptparams

	AREA |C$$CODE|, CODE, READONLY

;#if !defined(RISC_OS)
;fp	.req	r11
;ip	.req	r12
;sp	.req	r13
;lr	.req	r14
;pc	.req	r15
;#endif
fp	RN	r11
ip	RN	r12
sp	RN	r13
lr	RN	r14
pc	RN	r15

;	.text

; Support code for Byong Gi Lee IMDCT algorithm

;	.global	_sum8_asm, sum8_asm
;_sum8_asm:
;sum8_asm:
	EXPORT	sum8_asm
sum8_asm
;#if defined(PROF) && !defined(RISC_OS)
;	mov	ip, lr
;	bl	mcount
;
;	.word	sum8_asm_pr
;#endif
        FunctionEntry "r4,r5,r6,r7,r8"
	ldmia	r0, {r1,r2,r3,r4,r5,r6,r7,r8}
	add	r7, r7, r8
	add	r5, r5, r7
	add	r3, r3, r4
	add	r7, r7, r6
	add	r6, r6, r8
	add	lr, r4, r8
	add	ip, r4, r6
	add	r8, r2, r6
	add	r6, r3, r7
	add	r7, r2, r7
	add	r3, r3, r5
	add	r1, r1, r5
	stmia	r0, {r1,r3,r6,r7,r8,ip,lr}
        Return  "r4,r5,r6,r7,r8"

;#define XSIZE	(16+1-1)
	GBLA	XSIZE
XSIZE	SETA	(16+1-1)

;#if defined(RISC_OS)
;coeffadr: .word	bglcoeffs
;#else
;coeffadr: .word	_bglcoeffs
;#endif
	IMPORT	bglcoeffs
coeffadr	DCD	bglcoeffs

;#if USE_ASM
;	.global _bglgen_S, bglgen_S
;_bglgen_S:
;bglgen_S:
;#endif
	EXPORT	bglgen_S
bglgen_S

;#if defined(PROF) && !defined(RISC_OS)
;	mov	ip, lr
;	bl	mcount
;	.word	bglgen_S_pr
;#endif

        FunctionEntry "r1,r2,r4,r5,r6,r7,r8,r9,sl,fp"

	sub	sp, sp, #256			; only allocate space for a[] at first

	ldr	fp, coeffadr
	mov	r4, #0

	mov	r2, sp				; start by writing even stereo pairs in dest
	add	r8, sp, #32*4*2			; compute limit of dest array

	mov	sl, #512+(32<<10)
;10:	ldr	r3, [fp], #4			; get coefficient
10	ldr	r3, [fp], #4			; get coefficient
	mov	r7, r3, asr #10			; split into high part ...
	eor	r3, r3, r7, lsl #10		; ... and low part

	ldmia	r0!, {r5,r6,ip,lr}		; r5, r6, ip, lr def
	add	r5, r5, ip			; compute sum left
	add	r6, r6, lr			;     and sum right
	stmia	r2, {r5,r6}			; store even stereo pair (sums)
	add	r2, r2, #2*4*2			; move past this and next (odd) pair
	sub	r5, r5, ip, lsl #1		; compute diff left
	sub	r6, r6, lr, lsl #1		;     and diff right

	mla	lr, r5, r3, sl			; compute product (left)
	mov	lr, lr, asr #10			; using
	mla	lr, r5, r7, lr			; two multiplications
	mov	r5, lr, asr #6			; and shifts

	mla	r3, r6, r3, sl			; do same for right channel
	mov	r3, r3, asr #10			;
	mla	r3, r6, r7, r3			;
	mov	r6, r3, asr #6			;

	stmia	r2, {r5,r6}			; store odd stereo pair (diffs * K)
	add	r2, r2, #2*4*2			; move past this pair and next (even) pair
	cmp	r2, r8				; hit limit yet?
	blo	%b10 ;10b			; if still below, loop round
	addeq	r2, sp, #1*4*2			; if exactly equal, point r2 at odd pairs
	beq	%b10 ;10b			; and go round 8 more times

	mov	r0, sp
	sub	sp, sp, #256			; allocate space for b[]
	mov	r4, #0
;20:	tst	r4, #4
20	tst	r4, #4
	ldreq	r1, [fp], #4
	moveq	r6, r1, asr #10
	eoreq	r1, r1, r6, lsl #10

	ldmia	r0!, {r3,r5,ip,lr}
	add	r8, r3, ip
	add	r9, r5, lr
	add	r7, sp, r4, lsl #3
	stmia	r7, {r8, r9}

	sub	ip, r3, ip
	sub	lr, r5, lr

	mla	r5, ip, r1, sl
	mov	r5, r5, asr #10
	mla	r5, ip, r6, r5
	mov	ip, r5, asr #6

	mla	r5, lr, r1, sl
	mov	r5, r5, asr #10
	mla	r5, lr, r6, r5
	mov	lr, r5, asr #6

	add	r7, r7, #2*4*2
	stmia	r7, {ip, lr}

	add	r4, r4, #4
	cmp	r4, #32
	blt	%b20 ;20b
	moveq	r4, #1
	beq	%b20 ;20b

	mov	r0, sp
	mov	r4, #0
	add	r8, sp, #256
;30:	tst	r4, #12
30	tst	r4, #12
	ldreq	r1, [fp], #4
	moveq	r6, r1, asr #10
	eoreq	r1, r1, r6, lsl #10

	ldmia	r0!, {r3,r5,ip,lr}
	add	r3, r3, ip
	add	r5, r5, lr
	add	r7, r8, r4, lsl #3
	stmia	r7, {r3, r5}

	sub	ip, r3, ip, lsl #1
	sub	lr, r5, lr, lsl #1
	mla	r5, ip, r1, sl
	mov	r5, r5, asr #10
	mla	r5, ip, r6, r5
	mov	ip, r5, asr #6

	mla	r5, lr, r1, sl
	mov	r5, r5, asr #10
	mla	r5, lr, r6, r5
	mov	lr, r5, asr #6

	add	r7, r7, #2*4*2
	stmia	r7, {ip, lr}
	add	r4, r4, #4
	cmp	r4, #32
	blt	%b30 ;30b
	moveq	r4, #1
	beq	%b30 ;30b

	add	r0, sp, #256
	mov	r4, #0
;40:	ldr	r1, [fp], #4
40	ldr	r1, [fp], #4
	mov	r6, r1, asr #10
	eor	r1, r1, r6, lsl #10
;50:	ldmia	r0!, {r3,r5,ip,lr}
50	ldmia	r0!, {r3,r5,ip,lr}
	add	r3, r3, ip
	add	r5, r5, lr
	add	r7, sp, r4, lsl #3
	stmia	r7, {r3, r5}
	sub	ip, r3, ip, lsl #1
	sub	lr, r5, lr, lsl #1
	mla	r5, ip, r1, sl
	mov	r5, r5, asr #10
	mla	r5, ip, r6, r5
	mov	ip, r5, asr #6
	mla	r5, lr, r1, sl
	mov	r5, r5, asr #10
	mla	r5, lr, r6, r5
	mov	lr, r5, asr #6
	add	r7, r7, #2*4*2
	stmia	r7, {ip, lr}
	add	r4, r4, #4
	cmp	r4, #32
	blt	%b50 ;50b
	moveq	r4, #1
	beq	%b40 ;40b

	mov	r0, sp				; source in r0
	add	r1, sp, #256			; dest in r1
	add	r5, r1, #16*4			; limit for dest
;60:	ldmia	r0!, {r3,r4,ip,lr}
60	ldmia	r0!, {r3,r4,ip,lr}
	add	r6, r3, ip
	str	r6, [r1], #4
	add	r2, r4, lr
	str	r2, [r1, #32*4-4]
	sub	r3, r3, ip
	sub	lr, r4, lr

	add	r4, r3, r3, lsl #2		; r4 = r3 * 5
	add	r4, r4, r3, lsl #8		; r4 = r4 + r3 * 256 = r3 * 261
	add	r4, r4, sl			; r4 += bias ==== r4 = r3 * 261 + bias
	add	ip, r3, r3, lsl #1		; ip = r3 * 3
	rsb	ip, ip, ip, lsl #4		; ip = ip * 15 = r3 * 45
	add	ip, ip, r4, asr #10		; ip = ip + r4 >> 10
	mov	r3, ip, asr #6			; ==== r3 = orig r3 * 0.7071075 (for .7071068)

	add	r4, lr, lr, lsl #2
	add	r4, r4, lr, lsl #8
	add	r4, r4, sl
	add	lr, lr, lr, lsl #1
	rsb	lr, lr, lr, lsl #4
	add	lr, lr, r4, asr #10
	mov	lr, lr, asr #6

	str	r3, [r1, #16*4-4]
	str	lr, [r1, #32*4+16*4-4]
	cmp	r1, r5
	blt	%b60 ;60b

;#if 1
	add	sp, sp, #256			; clear off b[] from stack, now done with
	mov	lr, #3
	add	ip, sp, #2*4
;70:	ldmia	ip, {r1,r2,r3,r4,r5,r6}
70	ldmia	ip, {r1,r2,r3,r4,r5,r6}
	add	r1, r1, r2
	add	r8, r5, r6
	add	r3, r3, r8
	add	r5, r4, r6
	add	r4, r4, r8
	stmia	ip, {r1,r2,r3,r4,r5}
	add	ip, ip, #6*4
	ldmia	ip, {r0,r1,r2,r3,r4,r5,r6,r7}
	add	r6, r6, r7
	add	r4, r4, r6
	add	r2, r2, r3
	add	r6, r6, r5
	add	r5, r5, r7
	add	r9, r3, r7
	add	r8, r3, r5
	add	r7, r1, r5
	add	r5, r2, r6
	add	r6, r1, r6
	add	r2, r2, r4
	add	r0, r0, r4
	stmia	ip!, {r0,r2,r5,r6,r7,r8,r9}
	add	ip, ip, #3*4
	subs	lr, lr, #1
	bpl	%b70 ;70b

	; lr = -1 here

	add	r1, sp, #256
	ldmia	r1, {r1, r2}

	mov	r3, lr, lsr #16-3		; r3 := 0x0007FFFF, hi16:count (7), lo16:mask
	add	r5, sp, #31*4
	add	r6, sp, #8*4
	add	r1, r1, #XSIZE*2*(2*7+1)*4
	add	r2, r2, #XSIZE*2*(2*7+1)*4
;80:	ldr	r7, [r5, #32*4]
80	ldr	r7, [r5, #32*4]
	ldr	r8, [r5], #-4
	bic	lr, r8, r3, lsl #16
	orr	lr, lr, r7, lsl #16
	str	lr, [r1], #-XSIZE*2*2*4
	ldr	lr, [r6, #32*4]
	ldr	r9, [r6], #4
	add	r8, r9, r8
	add	lr, lr, r7
	rsb	r7, r8, #0
	bic	r7, r7, r3, lsl #16
	rsb	lr, lr, #0
	orr	lr, r7, lr, lsl #16
	str	lr, [r2], #-XSIZE*2*2*4
	subs	r3, r3, #1<<16
	bpl	%b80 ;80b

	; r3 ends up at -1

	mov	r3, r3, lsr #16-2		; r3 := 0x0003FFFF, hi16:count (3), lo16:mask
	add	r1, r1, #XSIZE*2*(4*3+2+1)*4
	add	r2, r2, #XSIZE*2*(4*3+2+1)*4
	add	r6, sp, #4*4
;90:	ldr	r8, [r5], #-4
90	ldr	r8, [r5], #-4
	ldr	r7, [r5, #32*4+4]
	bic	r4, r8, r3, lsl #16		; clean low 16 bits of r8 into r4
	orr	r4, r4, r7, lsl #16
	str	r4, [r1], #-XSIZE*2*4*4
	ldr	r9, [r6], #4
	ldr	r4, [r6, #32*4-4]
	add	r8, r9, r8
	add	r4, r4, r7
	rsb	r7, r8, #0
	rsb	r4, r4, #0
	bic	r7, r7, r3, lsl #16		; clear out upper 16 bits of r7
	orr	r4, r7, r4, lsl #16
	str	r4, [r2], #-XSIZE*2*4*4
	subs	r3, r3, #1<<16			; count down in upper half of r3
	bpl	%b90 ;90b			; until -ve, i.e. r3 = -1

	mov	r3, r3, lsr #16-1		; r3 := 0x0001FFFF, hi16:count (1), lo16:mask
	add	r1, r1, #XSIZE*2*(8*1+4)*4+XSIZE*2*2*4
	add	r2, r2, #XSIZE*2*(8*1+4)*4+XSIZE*2*2*4
	add	r6, sp, #2*4
;100:	ldr	r8, [r5], #-4
100	ldr	r8, [r5], #-4
	ldr	r7, [r5, #32*4+4]
	bic	r4, r8, r3, lsl #16
	orr	r4, r4, r7, lsl #16
	str	r4, [r1], #-XSIZE*2*8*4
	ldr	r9, [r6], #4
	ldr	r4, [r6, #32*4-4]
	add	r8, r9, r8
	add	r4, r4, r7
	rsb	r7, r8, #0
	rsb	r4, r4, #0
	bic	r7, r7, r3, lsl #16
	orr	r4, r7, r4, lsl #16
	str	r4, [r2], #-XSIZE*2*8*4
	subs	r3, r3, #1<<16
	bpl	%b100 ;100b

	add	r1, r1, #XSIZE*2*4*4
	add	r2, r2, #XSIZE*2*4*4

	mov	r3, r3, lsr #16			; r3 := 0x0000FFFF again for below

;#else
;
;	mov	lr, #3
;	add	ip, sp, #256+15*4
;70:	ldmda	ip!, {r1,r2,r3,r4,r5,r6,r7}
;	add	r6, r6, r7
;	add	r4, r4, r6
;	add	r2, r2, r3
;	add	r6, r6, r5
;	add	r5, r5, r7
;	add	sl, r3, r7			; compute a[13]
;	add	r7, r3, r5			; compute a[11]
;	add	r0, r1, r5			; compute a[9]
;	add	r5, r2, r6
;	add	r6, r1, r6
;	; oh oh, next line zaps r0!
;	ldmdb	ip, {r1,r3,r8,fp}		; load s0..s3;  a[14] = s3 (conveniently!)
;	add	r3, r3, fp			; a[10] = s1 + s3
;	ldr	r8, [ip, #-5*4]			; load a[12] = t1
;	stmib	ip, {r0,r3,r7,r8,sl,fp}		; store a[14], a[13], a[12], a[11], a[10], a[9]
;	sub	ip, ip, #8*4
;	ldmia	ip, {
;
;	add	r2, r2, r4			; compute a[3]
;	add	r0, r0, r4			; compute a[1]
;	# STUFF TO GO IN HERE!
;	stmia	ip, {r0,r2,r5,r6}		; store a[8], a[7], a[6], a[5],
;						; store a[4], a[3], a[2], a[1]
;	add	ip, ip, #3*4
;	subs	lr, lr, #1
;	bpl	70b
;
;;	THEN LOOP ONCE OVER  [1..15] rather than 3 loops of 1mod2,2mod4,4mod8 sets
;
;	mvn	ip, #~-17			; count and mask in ip
;	add	r4, sp, #31*4
;	add	r5, sp, # 1*4
;	add	r0, r4, #32*4*4
;	add	r3, r5, #32*4*4
;	add	r1, r1, #XSIZE*2*15*4
;	add	r2, r2, #XSIZE*2*15*4
;;80:
;80
;	ldmda	r4!, {r8,r9}
;	ldmda	r0!, {r7,sl}
;	and	lr, r9, ip, lsr #16
;	orr	lr, lr, sl, lsl #16
;	str	lr, [r1], #-XSIZE*2*1*4
;	and	lr, r8, ip, lsr #16
;	orr	lr, lr, r7, lsl #16
;	str	lr, [r1], #-XSIZE*2*1*4
;	ldmia	r5!, {fp,lr}
;	add	r8, lr, r8
;	add	r9, fp, r9
;	ldmia	r3!, {fp,lr}
;	add	r7, lr, r7
;	add	sl, fp, sl
;	rsb	r8, r8, #0
;	rsb	r7, r7, #0
;	and	r8, r8, ip, lsr #16
;	orr	lr, r8, r7, lsl #16
;	str	lr, [r2], #-XSIZE*2*1*4
;	rsb 	r9, r9, #0
;	rsb	sl, sl, #0
;	and	r9, r9, ip, lsr #16
;	orr	lr, r9, sl, lsl #16
;	str	lr, [r2], #-XSIZE*2*1*4
;	adds	ip, ip, #1
;	bmi	80b
;#endif

        ldr     r4, [sp, #17*4]
        ldr     r5, [sp, #32*4+17*4]
        and     r0, r4, r3
        orr     r0, r0, r5, lsl #16
        str     r0, [r1, #XSIZE*2*8*4]
        ldr     r0, [sp, #1*4]
        add     r4, r0, r4
        ldr     r0, [sp, #32*4+1*4]
        add     r5, r0, r5
        rsb     r0, r4, #0
        and     r0, r0, r3
        rsb     r5, r5, #0
        orr     r0, r0, r5, lsl #16
        str     r0, [r2, #XSIZE*2*8*4]

        ldr     r0, [sp, #0]
        ldr     r5, [sp, #32*4+0]
        rsb     r0, r0, #0
        and     r0, r0, r3
        rsb     r5, r5, #0
        orr     r0, r0, r5, lsl #16
        str     r0, [r2, #XSIZE*2*16*4]
        ldr     r4, [sp, #16*4]
        ldr     r5, [sp, #32*4+16*4]
        and     r0, r4, r3
        orr     r0, r0, r5, lsl #16
        str     r0, [r1, #XSIZE*2*0*4]
        rsb     r0, r4, #0
        and     r0, r0, r3
        rsb     r1, r5, #0
        orr     r0, r0, r1, lsl #16
        str     r0, [r2, #XSIZE*2*0*4]

	add	sp, sp, #256+2*4
        Return  "r4,r5,r6,r7,r8,r9,sl,fp"

;endbglgen_S:
endbglgen_S

;#if defined(PROF) && !defined(RISC_OS)
;
;	.data
;gen_M_pr:	.long	0
;gen_S_pr:	.long	0
;sum8_asm_pr:	.long	0
;sum8S_asm_pr:	.long	0
;bglgen_S_pr:.long	0
;
;#endif

	END
