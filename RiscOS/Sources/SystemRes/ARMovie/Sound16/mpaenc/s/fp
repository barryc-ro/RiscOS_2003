;
; Some assembler for the FP code
;
; (c) 1998 Paul LeBeau (paul@caverock.co.nz)
;


        AREA    |fp$$misccode|, CODE, READONLY


        GET     Hdr:ListOpts
        GET     Hdr:Macros
        GET     Hdr:APCS.<APCS>
        GET     s.RegNames_h



;=====================================================================
;       M A C R O S
;=====================================================================

; ---------------------------------------------------------------------
;       Macro for 64 bit addition
;       $rh,$rl = $rah,$ral + $rbh,$rbl
; ---------------------------------------------------------------------

        MACRO
$label  add64   $rl,$rh, $ral,$rah, $rbl,$rbh

$label  ADDS    $rl, $ral,$rbl     ; add low words, setting carry
        ADC     $rh, $rah,$rbh     ; add high words, using carry

        MEND


; ---------------------------------------------------------------------
;       Macro for 64 bit subtraction
;       $rh,$rl = $rah,$ral - $rbh,$rbl
; ---------------------------------------------------------------------

        MACRO
$label  sub64   $rl,$rh, $ral,$rah, $rbl,$rbh

$label  SUBS    $rl, $ral,$rbl     ; sub low words, getting borrow
        SBC     $rh, $rah,$rbh     ; sub high words, using borrow

        MEND


; ---------------------------------------------------------------------
;       Macro for 64 bit multiply
;       $rh,$rl = $ra * $rb
;       $rt = tmp reg
;       $ra,$rb,$rt = corrupt on exit
; ---------------------------------------------------------------------

        MACRO
$label  mul64   $rl, $rh, $ra, $rb, $rt

        ASSERT  "$ra" <> ""
        ASSERT  "$rb" <> ""
        ASSERT  "$rl" <> ""
        ASSERT  "$rh" <> ""
        ASSERT  "$rt" <> ""
        ASSERT  "$ra" <> "$rb"
        ASSERT  "$ra" <> "$rl"
        ASSERT  "$ra" <> "$rh"
        ASSERT  "$ra" <> "$rt"
        ASSERT  "$rb" <> "$rl"
        ASSERT  "$rb" <> "$rh"
        ASSERT  "$rb" <> "$rt"
        ASSERT  "$rl" <> "$rh"
        ASSERT  "$rl" <> "$rt"
        ASSERT  "$rh" <> "$rt"

$label
        MOVS    $rh, $ra, LSR #16              ; $rh is ms 16 bits of ra
        BIC     $ra, $ra, $rh, LSL #16         ; $ra is ls 16 bits
        MOV     $rt, $rb, LSR #16              ; $rt is ms 16 bits of rb
        BIC     $rb, $rb, $rt, LSL #16         ; $rb is ls 16 bits
        MUL     $rl, $ra, $rb                  ; low partial product
        MUL     $rb, $rh, $rb                  ; first middle partial product
        MUL     $ra, $rt, $ra                  ; second middle partial product
        MULNE   $rh, $rt, $rh                  ; High partial product
                                               ; - NE condition reduces time taken if $rh = 0
        ADDS    $ra, $ra, $rb                  ; add middle partial product
        ADDCS   $rh, $rh, #&10000              ; add in carry
        ADDS    $rl, $rl, $ra, LSL #16         ; add middle partial product sum into low
        ADC     $rh, $rh, $ra, LSR #16         ; and high worhs of result

        MEND


; ---------------------------------------------------------------------
;       Macro for unsigned 32 bit division
;       Fast partially-unrolled version purloined from ansilib
;       $rr = $ra / $rb
;       $rt,$ru = tmp regs
;       $ra on exit contains the remainder
;       NOTE: no check is made that rb != 0
; ---------------------------------------------------------------------

        MACRO
$label  udiv32  $rr, $ra, $rb, $rt,$ru

        ASSERT  "$rr" <> ""
        ASSERT  "$ra" <> ""
        ASSERT  "$rb" <> ""
        ASSERT  "$rt" <> ""
        ASSERT  "$ru" <> ""
        ASSERT  "$rr" <> "$ra"
        ASSERT  "$rr" <> "$rb"
        ASSERT  "$rr" <> "$rt"
        ASSERT  "$rr" <> "$ru"
        ASSERT  "$ra" <> "$rb"
        ASSERT  "$ra" <> "$rt"
        ASSERT  "$ra" <> "$ru"
        ASSERT  "$rb" <> "$rt"
        ASSERT  "$rb" <> "$ru"
        ASSERT  "$rt" <> "$ru"

$label
        MOV     $rr, #0
        MOV     $ru, #&80000000
        CMP     $ra, $ru
        MOVCC   $ru, $ra
10
        CMP     $ru, $rt
        BLS     %ft18
        CMP     $ru, $rt,LSL #1
        BLS     %ft17
        CMP     $ru, $rt,LSL #2
        BLS     %ft16
        CMP     $ru, $rt,LSL #3
        BLS     %ft15
        CMP     $ru, $rt,LSL #4
        BLS     %ft14
        CMP     $ru, $rt,LSL #5
        BLS     %ft13
        CMP     $ru, $rt,LSL #6
        BLS     %ft12
        CMP     $ru, $rt,LSL #7
        MOVHI   $rt, $rt,LSL #8
        BHI     %bt10
11
        CMP     $ra, $rt,LSL #7
        ADC     $rr, $rr,$rr
        SUBCS   $ra, $ra,$rt,LSL #7
12
        CMP     $ra, $rt,LSL #6
        ADC     $rr, $rr,$rr
        SUBCS   $ra, $ra,$rt,LSL #6
13
        CMP     $ra, $rt,LSL #5
        ADC     $rr, $rr,$rr
        SUBCS   $ra, $ra,$rt,LSL #5
14
        CMP     $ra, $rt,LSL #4
        ADC     $rr, $rr,$rr
        SUBCS   $ra, $ra,$rt,LSL #4
15
        CMP     $ra, $rt,LSL #3
        ADC     $rr, $rr,$rr
        SUBCS   $ra, $ra,$rt,LSL #3
16
        CMP     $ra, $rt,LSL #2
        ADC     $rr, $rr,$rr
        SUBCS   $ra, $ra,$rt,LSL #2
17
        CMP     $ra, $rt,LSL #1
        ADC     $rr, $rr,$rr
        SUBCS   $ra, $ra,$rt,LSL #1
18
        CMP     $ra, $rt
        ADC     $rr, $rr,$rr
        SUBCS   $ra, $ra,$rt
        CMP     $rb, $rt,LSR #1
        MOVLS   $rt, $rt,LSR #8
        BLS     %bt11

        MEND


;---------------------------------------------------------------------
;       pack FP components into an IEEE float
;       $rs = sign (in bit 31)
;       $re = exponent
;       $rm = mantissa
;       $co = condition flags
;       returns packed float in a1
;---------------------------------------------------------------------

        MACRO
$label  packfloat  $co, $rs,$re,$rm

$label
        ADD$co  $rm, $rm,$re,LSL #23
        ORR$co  a1, $rs,$rm

        MEND


;---------------------------------------------------------------------
;       round and pack FP components into an IEEE float
;       $rs = sign (in bit 31)
;       $re = exponent
;       $rm = mantissa
;       $rt = tmp reg
;       returns packed float in a1
;---------------------------------------------------------------------

        MACRO
$label  roundandpackfloat  $rs, $re, $rm, $rt

$label
        AND     $rt, $rm,#&7f      ; a4 = bits to be shifted out
        ADD     $rm, $rm,#&40
        TEQ     $rt, #&40
        BICEQ   $rm, $rm,#&80
        MOVS    $rm, $rm,LSR #7
        MOVEQ   $re, #0            ; if frac==0 exp=0

        packfloat ,$rs,$re,$rm

        MEND


;---------------------------------------------------------------------
;       shifts input right.  if any of the bits shifted off
;       were non-zero then the lsb of the result is set to 1
;       $v  = value to shift
;       $sh = shift to be applied
;       returns packed float in $r
;---------------------------------------------------------------------

        MACRO
$label  lsr_scrunched  $r, $v,$sh

        ASSERT  "$r" <> "$v"
        ASSERT  "$r" <> "$sh"
        ASSERT  "$v" <> "$sh"

$label
        CMP     $sh, #32
        MOVHI   $sh, #32         ; clamp shift to a max of 32

        MOV     $r, $v,LSR $sh   ; get $v shifted right by sh
        CMP     $r, $v,ROR $sh   ; if those bits were not all zero, then
                                 ; $v ROR $sh will be greater than $v LSR $sh
        ORRLO   $r, $r,#1

        MEND


;---------------------------------------------------------------------
;       normalise an exp and mantissa pair
;       $re = exponent
;       $rm = mantissa
;       returns updated registers (msb of mantissa in bit 30)
;---------------------------------------------------------------------

        MACRO
$label  normalise  $re,$rm

$label
        CMP     $rm, #&8000
        MOVLO   $rm, $rm,LSL #16
        SUBLO   $re, $re,#16
        ; msb of $rm should be somewhere in 30..15 now
        CMP     $rm, #&800000
        MOVLO   $rm, $rm,LSL #8
        SUBLO   $re, $re,#8
        ; msb of $rm should be somewhere in 30..23 now
        TST     $rm, #&78000000
        MOVEQ   $rm, $rm,LSL #4
        SUBEQ   $re, $re,#4
        ; msb of $rm should be somewhere in 30..27 now
        TST     $rm, #&60000000
        MOVEQ   $rm, $rm,LSL #2
        SUBEQ   $re, $re,#2
        ; msb of $rm should be somewhere in 30..29 now
        TST     $rm, #&40000000
        MOVEQ   $rm, $rm,LSL #1
        SUBEQ   $re, $re,#1

        MEND


;---------------------------------------------------------------------
;       Macro for estimating unsigned 64/32=32 bit division
;
;       Returns an approximation to the 32-bit integer quotient obtained
;       by dividing 'b' into the 64-bit value formed by concatenating 'ah' and
;       'al'.  The divisor 'b' must be at least 2^31.  If q is the exact quotient
;       truncated toward zero, the approximation returned lies between q and
;       q + 2 inclusive.  If the exact quotient q is larger than 32 bits, the
;       maximum positive 32-bit unsigned integer is returned.
;
;       $rr = $rh.rl / $rb
;       $rt1..$rt5 = tmp regs
;---------------------------------------------------------------------

        MACRO
$label  estdiv  $rr, $rl,$rh, $rb,  $rt1,$rt2,$rt3,$rt4,$rt5,$rt6,$rt7

$label
        ; $rl = al
        ; $rh = ah
        ; $rb = b

        CMP     $rb, $rh
        MOVLS   $rr, #-1
        BLS     %ft14

        ; $rt1 = bh
        MOV     $rt1, $rb,LSR #16
        MOV     $rt7, $rt1,ASL #16
        CMP     $rt7, $rh
        ; $rt2 = z
        LDRLS   $rt2, =&ffff0000
        BLS     %ft10

        MOV     $rt3, $rh                  ; preserve $rh cos udiv32 will corrupt it
        udiv32  $rt2, $rt3,$rt1, $rt7,$rt5
        MOV     $rt2, $rt2,LSL #16

10
        MOV     $rt5, $rt2                 ; preserve $rb & $rt2 cos mul64 will corrupt it
        MOV     $rt6, $rb
        mul64   $rt3,$rt4, $rt6,$rt5, $rt7
        ; here $rt3,$rt4 = terml,termh
        sub64   $rt3,$rt4, $rl,$rh, $rt3,$rt4
        ; now $rt3,$rt4 = reml,remh

        CMP     $rt4, #0
        BGE     %ft13
11
        SUB     $rt2, $rt2,#&10000
        ;add64   $rt3,$rt4, $rt3,$rt4, $rt7,$rt1,
        ADDS    $rt3, $rt3,$rb,ASL #16     ; reml,remh += bl,bh (bl=b<<16)
        ADCS    $rt4, $rt4,$rt1
;12
        BLT     %bt11
13

        MOV     $rt4, $rt4,ASL #16
        ORR     $rt4, $rt4,$rt3,LSR #16

        MOV     $rt7, $rt1,ASL #16
        CMP     $rt7, $rt4
        LDRLS   $rt3, =&ffff
        ORRLS   $rr, $rt2,$rt3
        BLS     %ft14

        udiv32  $rt5, $rt4,$rt1, $rt7,$rt3
        ORR     $rr, $rt2,$rt5

14
        MEND


;=====================================================================


;---------------------------------------------------------------------
;       multiples two IEEE single-precision FP values together
;
;       fpfloat  fpMul(fpfloat a, fpfloat b)
;---------------------------------------------------------------------

        EXPORT  fpMul

fpMul
        ; r0 (a1) = lhs
        ; r1 (a2) = rhs
        ; returns:
        ; r0 = result

        FunctionEntry "v1-v2"

        EOR     a3, a1,a2           ; zs(bit31) (sign of result)
        BIC     a1, a1,#&80000000
        BICS    a2, a2,#&80000000   ; is b = +-0 ?

        TEQNE   a1, #0              ; if not, what about a ?
        ANDEQ   a1, a3,#&80000000   ; yes.. return +0 or -0 dependent on zs
        Return  "v1-v2",, EQ

        MOV     a4, a1,LSR #23      ; exponent of a
        MOV     v1, a2,LSR #23      ; exponent of b

        ; here: a3=zs  a4,v1=exponents

        BIC     a1, a1,a4,LSL #23   ; fractional/mantissa part of a
        BIC     a2, a2,v1,LSL #23   ; fractional/mantissa part of b

        ADD     a4, a4,v1           ; zexp = aexp + bexp - &7f
        SUB     a4, a4,#&7f

        ORR     a1, a1,#&800000     ; reinsert msb of mantissas (mantissae? :)
        ORR     a2, a2,#&800000

        MOV     a1, a1,LSL #7       ; prepare for multiply
        MOV     a2, a2,LSL #8

        ; here: a1,a2=mants  a3=zsign  a4=zexp

        mul64   v1,v2, a1,a2, lr    ; multiply

        TEQ     v1, #0              ; low word non-zero?
        ORRNE   v2, v2,#1           ; 'round up' high word if low word non-zero

        MOVS    lr, v2,LSL #1       ; if resulth<<1 >= 0
        MOVPL   v2, lr              ; resulth <<= 1
        SUBPL   a4, a4,#1           ; zexp--

        AND     a3, a3,#&80000000
        roundandpackfloat  a3,a4,v2, lr

        Return  "v1-v2"



;---------------------------------------------------------------------
;       performs division of two IEEE single-precision FP values
;
;       fpfloat  fpDiv(fpfloat a, fpfloat b)
;---------------------------------------------------------------------

        EXPORT  fpDiv

fpDiv
        ; a1 = a
        ; a2 = b
        ; returns:
        ; a1 = result

        FunctionEntry "v1-v6"

        EOR     a3, a1,a2           ; zs(bit31) (sign of result)
        BIC     a1, a1,#&80000000
        BICS    a2, a2,#&80000000   ; is b = +-0 ?

        ANDEQ   a1, a3,#&80000000   ; yes.. return +-INF dependent on zs
        ;ORREQ   a1, a1,#(&ff<<23)     ;&7f800000
        Return  "v1-v6",, EQ

        TEQ     a1, #0              ; is a = +-0 ?
        ANDEQ   a1, a3,#&80000000   ; yes.. return +-0
        Return  "v1-v6",, EQ

        MOV     a4, a1,LSR #23      ; exponent of a
        MOV     v1, a2,LSR #23      ; exponent of b

        ; here: a3=zs  a4,v1=exponents

        BIC     a1, a1,a4,LSL #23   ; fractional/mantissa part of a
        BIC     a2, a2,v1,LSL #23   ; fractional/mantissa part of b

        SUB     a4, a4,v1           ; zexp = aexp - bexp + &7d
        ADD     a4, a4,#&7d

        ORR     a1, a1,#&800000     ; reinsert msb of mantissas (mantissae? :)
        ORR     a2, a2,#&800000

        MOV     a1, a1,LSL #7       ; prepare for multiply
        MOV     a2, a2,LSL #8

        ; here: a1,a2=mants  a3=zsign  a4=zexp

        CMP     a2, a1,LSL#1        ; adjust a to ensure afrac/bfrac is < 2
        MOVLS   a1, a1,LSR#1
        ADDLS   a4, a4,#1

        MOV     v1, #0
        estdiv  v1, v1,a1, a2, v2,v3,v4,v5,v6,ip,lr

        ; here: v1 = zfrac


        AND     v2, v1,#&3f
        CMP     v2, #2              ; if ((zfrac & 0x3f ) <= 2)
        BHI     _d_exit

        MOV     v4, a2              ; protect a2 & v1 from the corrupting
        MOV     v5, v1              ; influences of mul64
        mul64   v2,v3, v4,v5, lr
        ;sub64   v2,v3, 0,a1, v2,v3
        RSBS    v2, v2,#0
        RSCS    v3, v3,a1

        ; here: v2,v3 = reml,remh

        BGE     _d_b1
_d_lp1                              ; if (signed)remh < 0...
        SUB     v1, v1,#1           ; zfrac--
        ;add64   v2,v3, a2,0, v2,v3
        ADDS    v2, v2,a2
        ADCS    v3, v3,#0
        BLT     _d_lp1

_d_b1
        TEQ     v2, #0              ; if reml > 0
        ORRNE   v1, v1,#1           ; round up zfrac by 1

_d_exit
        AND     a3, a3,#&80000000
        roundandpackfloat  a3,a4,v1, lr

        Return  "v1-v6"



;---------------------------------------------------------------------
;       addition of two IEEE single-precision FP values
;
;       fpfloat  fpAdd(fpfloat a, fpfloat b)
;---------------------------------------------------------------------

        EXPORT  fpAdd

fpAdd
        ; a1 = a
        ; a2 = b
        ; returns:
        ; a1 = result

        FunctionEntry "v1"

        AND     a3, a1,#&80000000   ;asign (and will be zsign also)

        TEQ     a3, a2              ; a2 EOR bit31(a1)
                                    ; N will be set if signs were different
        BIC     a1, a1,#&80000000
        BIC     a2, a2,#&80000000

        MOV     a4, a1,LSR #23      ; exponent of a
        MOV     v1, a2,LSR #23      ; exponent of b
        BIC     a1, a1,a4,LSL #23   ; fractional/mantissa part of a
        BIC     a2, a2,v1,LSL #23   ; fractional/mantissa part of b

        ; here: a3=zs  a4,v1=exponents  a1,a2 = mantissae

        BMI     sub

add
        MOV     a1, a1,LSL #6
        MOV     a2, a2,LSL #6       ; fractions now occupy bits 28..0

        CMP     a4, v1
        BGT     a_abigger
        BLT     a_bbigger

        ; a and b have same exponent

        ADD     a1, a1,a2
        TEQ     a4, #0
        MOVEQ   a1, a1,LSR# 6       ; aexp/bexp=0 values are ultra-small so...
        packfloat EQ, a3,a4,a1      ; values are un-normal and rounding is not appropriate
        Return  "v1",, EQ

        ADD     a1, a1,#&40000000   ; reinsert ms bit of mantissa
        roundandpackfloat  a3,a4,a1, lr
        Return  "v1"


a_abigger
        SUB     v1, a4,v1           ; shift req to make exp's equal
        TEQ     v1, a4              ; if bexp==0 (b is ultra small)
        SUBEQ   v1, v1,#1           ; then ...
        ORRNE   a2, a2,#&20000000   ; reinsert ms bit

        lsr_scrunched  lr, a2,v1
        ; zexp = aexp = a4

        ORR     a1, a1,#&20000000   ; reinsert ms bit of a
        ; fractions now occupy bits 29..0
        ADD     a1, a1,lr

        ; addition may have 'overflowed' to bit 30 now...
        TST     a1, #&40000000      ; has it?
        MOVEQ   a1, a1,LSL #1       ; no - so we must normalise
        SUBEQ   a4, a4,#1

        roundandpackfloat  a3,a4,a1, lr
        Return  "v1"

a_bbigger
        SUB     a4, v1,a4
        TEQ     a4, v1
        SUBEQ   a4, a4,#1
        ORRNE   a1, a1,#&20000000

        lsr_scrunched  lr, a1,a4
        ; zexp = bexp = v1

        ORR     a2, a2,#&20000000
        ADD     a1, lr,a2

        TST     a1, #&40000000
        MOVEQ   a1, a1,LSL #1
        SUBEQ   v1, v1,#1

        roundandpackfloat  a3,v1,a1, lr
        Return  "v1"



;---------------------------------------------------------------------
;       subtraction of two IEEE single-precision FP values
;
;       fpfloat  fpSub(fpfloat a, fpfloat b)
;---------------------------------------------------------------------

        EXPORT  fpSub

fpSub
        ; a1 = a
        ; a2 = b
        ; returns:
        ; a1 = result

        FunctionEntry "v1"

        AND     a3, a1,#&80000000   ;asign (and will be zsign also)

        TEQ     a3, a2              ; a2 EOR bit31(a1)
                                    ; N will be set if signs were different
        BIC     a1, a1,#&80000000
        BIC     a2, a2,#&80000000

        MOV     a4, a1,LSR #23      ; exponent of a
        MOV     v1, a2,LSR #23      ; exponent of b
        BIC     a1, a1,a4,LSL #23   ; fractional/mantissa part of a
        BIC     a2, a2,v1,LSL #23   ; fractional/mantissa part of b

        ; here: a3=zs  a4,v1=exponents  a1,a2 = mantissae

        BMI     add

sub
        MOV     a1, a1,LSL #7
        MOV     a2, a2,LSL #7       ; fractions now occupy bits 29..0

        CMP     a4, v1
        BGT     s_abigger
        BLT     s_bbigger

        ; a and b have same exponent

        SUBS    a1, a1,a2           ; zfrac = afrac-bfrac
        Return  "v1",, EQ           ; result = 0, return

        SUBGT   lr, a4,#1           ; if (a>b) zexp = aexp-1 ...
        RSBLT   a1, a1,#0           ; else zfrac = |zfrac|
        EORLT   a3, a3,#&80000000   ;      switch zsign
        SUBLT   lr, v1,#1           ; and make zexp = bexp-1 instead

        CMP     lr,#&ffffffff       ; if exp was 0 before decrement, then val was an
        MOVEQ   lr,#0               ; ultra-small (un-normal) value, so leave exp alone

        normalise          lr,a1
        roundandpackfloat  a3,lr,a1, a4
        Return  "v1"


s_abigger
        SUB     v1, a4,v1           ; shift req to make exp's equal
        TEQ     v1, a4              ; if bexp==0 (b is ultra small)
        SUBEQ   v1, v1,#1           ; then ...
        ORRNE   a2, a2,#&40000000   ; reinsert ms bit

        lsr_scrunched  lr, a2,v1
        ; zexp = aexp = a4

        ORR     a1, a1,#&40000000   ; reinsert ms bit of a
        ; fractions now occupy bits 30..0
        SUB     a1, a1,lr           ; do sub operation on mantissae
        SUB     a4, a4,#1

        normalise          a4,a1
        roundandpackfloat  a3,a4,a1, lr
        Return  "v1"

s_bbigger
        SUB     a4, v1,a4
        TEQ     a4, v1
        SUBEQ   a4, a4,#1
        ORRNE   a1, a1,#&40000000

        lsr_scrunched  lr, a1,a4
        ; zexp = bexp = v1

        ORR     a2, a2,#&40000000
        SUB     a1, a2,lr
        SUB     v1, v1,#1
        EOR     a3, a3,#&80000000   ; switch zsign

        normalise          v1,a1
        roundandpackfloat  a3,v1,a1, lr
        Return  "v1"



;---------------------------------------------------------------------
;       convert IEEE single-precision FP value to a signed integer
;
;       int  fpFix(fpfloat a)
;---------------------------------------------------------------------

        EXPORT  fpFix

fpFix
        ; a1 = a
        ; returns:
        ; a1 = integer result

        AND     a2, a1,#&80000000   ; sign

        BIC     a1, a1,#&80000000
        MOV     a3, a1,LSR #23      ; exponent of a
        BIC     a1, a1,a3,LSL #23   ; fractional/mantissa part of a

        ; here: a2=zs  a3=exponent  a1=mantissa

        CMP     a3, #&7e            ; exp <= &7e? (ie fraction < 1)
        MOVLS   a1, #0              ; ... return 0
        Return  , LinkNotStacked, LS

        ORR     a1, a1,#&800000     ; reinsert msb of mantissa
        MOV     a1, a1,LSL #8
        RSB     a4, a3,#&9e         ; shift req (as given by exponent) (9e = 127+31)

        MOV     a1, a1,LSR a4       ; shift mantissa to correct value
        TEQ     a2, #0              ; check sign...
        RSBNE   a1, a1,#0           ; negate if necessary

        Return  , LinkNotStacked


;---------------------------------------------------------------------
;       convert an integer to an IEEE single-precision FP value
;
;       fpfloat  fpFlt(int i)
;---------------------------------------------------------------------

        EXPORT  fpFlt

fpFlt
        ; a1 = i (signed integer)
        ; returns:
        ; a1 = fpfloat result

        TEQ     a1, #0
        Return  , LinkNotStacked, EQ ; return 0 for 0 input

        ANDS    a2, a1,#&80000000   ; sign
        RSBNE   a1, a1,#0           ; get abs(i)

        MOV     a3, #&9c            ; exponent

        normalise          a3,a1
        roundandpackfloat  a2,a3,a1, a4

        Return  , LinkNotStacked


;---------------------------------------------------------------------
;       returns 1 if given FP values are equal
;
;       int  fpEQ(fpfloat a, fpfloat b)
;---------------------------------------------------------------------

        EXPORT  fpEQ

fpEQ
        ; a1 = a
        ; a2 = b
        ; returns:
        ; a1 = (a == b)

        TEQ     a1, a2              ; a == b ?

        ; else check for +/- 0
        ORRNE   a1, a1,a2
        BICNES  a1, a1,#&80000000

        MOVEQ   a1, #1
        MOVNE   a1, #0
        Return  , LinkNotStacked


;---------------------------------------------------------------------
;       performs a less-than-or-equal test on two FP values
;
;       int  fpLE(fpfloat a, fpfloat b)
;---------------------------------------------------------------------

        EXPORT  fpLE

fpLE
        ; a1 = a
        ; a2 = b
        ; returns:
        ; a1 = (a <= b)

        MOV     a3, a1,LSR #31      ; a sign
        TEQ     a3, a2,LSR #31      ; 1 if signs different

        BEQ     le_signs_equal

        ; signs are different...

        ; check for +/- 0
        ORR     a1, a1,a2
        BICS    a1, a1,#&80000000

        ORREQ   a1, a3,#1          ; return true if sign(a) -ve, or a and b are +-0
        MOVNE   a1, a3
        Return  , LinkNotStacked

le_signs_equal
        CMP     a1, a2
        EORLT   a1, a3,#1
        EORGT   a1, a3,#0
        MOVEQ   a1, #1

        Return  , LinkNotStacked


;---------------------------------------------------------------------
;       performs a less-than test on two FP values
;
;       int  fpLT(fpfloat a, fpfloat b)
;---------------------------------------------------------------------

        EXPORT  fpLT

fpLT
        ; a1 = a
        ; a2 = b
        ; returns:
        ; a1 = (a < b)

        MOV     a3, a1,LSR#31       ; a sign
        TEQ     a3, a2,LSR #31      ; 1 if signs different

        BEQ     lt_signs_equal

        ; signs are different...

        ; check for +/- 0
        ORR     a1, a1,a2
        BICS    a1, a1,#&80000000

        ANDNE   a1, a3,#1          ; return true if sign(a) -ve, and a and b are not +-0
        MOVEQ   a1, a3
        Return  , LinkNotStacked

lt_signs_equal
        CMP     a1, a2
        EORLT   a1, a3,#1
        EORGT   a1, a3,#0
        MOVEQ   a1, #0

        Return  , LinkNotStacked


;=====================================================================

        END
