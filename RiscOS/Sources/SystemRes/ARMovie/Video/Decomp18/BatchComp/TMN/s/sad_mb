        GET     Hdr:ListOpts
        GET     Hdr:Macros
        GET     Hdr:APCS.<APCS>

	AREA	|Test$$Code|,CODE,READONLY

	; Global vars
	EXPORT	cpels
	EXPORT	pels
	EXPORT	mv_outside_frame
	EXPORT	lines
	EXPORT	long_vectors
	; Routines
	EXPORT	SAD_Macroblock
	EXPORT	SAD_Macroblock0
	EXPORT	FindMB
	EXPORT	FindMBbytes
	EXPORT	Clip
	EXPORT	LoadArea
	;EXPORT	FindHalfPel
	EXPORT	FindHalfPelPlus
	EXPORT	ChooseMode
	EXPORT	FindPred
	EXPORT	FindPred0
	EXPORT	DoPredChrom_P
	;EXPORT	CalcError
	EXPORT	ReconImage
	EXPORT	InterpolateImage
	;EXPORT	BoundedDiv
	[ 0 = 1
SAD_Macroblock
	; r0 = unsigned char *ii
	; r1 = unsigned char *act_block
	; r2 = h_length
	; r3 = Min_Frame
        FunctionEntry "r4-r11"

	MOV	r12,#16		; r12 = i = 16
	MOV	r14,#255
	MOV	r10,r0		; r10 = ii
	MOV	r0,#0		; r0 = sad = 0
sadmblp1
	LDMIA	r1!,{r4,r7,r8,r9}
	; 0
	LDRB	r6,[r10],#1
	AND	r5,r14,r4
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 1
	LDRB	r6,[r10],#1
	AND	r5,r14,r4,LSR#8
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 2
	LDRB	r6,[r10],#1
	AND	r5,r14,r4,LSR#16
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 3
	LDRB	r6,[r10],#1
	AND	r5,r14,r4,LSR#24
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 4
	LDRB	r6,[r10],#1
	AND	r5,r14,r7
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 5
	LDRB	r6,[r10],#1
	AND	r5,r14,r7,LSR#8
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 6
	LDRB	r6,[r10],#1
	AND	r5,r14,r7,LSR#16
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 7
	LDRB	r6,[r10],#1
	AND	r5,r14,r7,LSR#24
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 8
	LDRB	r6,[r10],#1
	AND	r5,r14,r8
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 9
	LDRB	r6,[r10],#1
	AND	r5,r14,r8,LSR#8
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 10
	LDRB	r6,[r10],#1
	AND	r5,r14,r8,LSR#16
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 11
	LDRB	r6,[r10],#1
	AND	r5,r14,r8,LSR#24
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 12
	LDRB	r6,[r10],#1
	AND	r5,r14,r9
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 13
	LDRB	r6,[r10],#1
	AND	r5,r14,r9,LSR#8
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 14
	LDRB	r6,[r10],#1
	AND	r5,r14,r9,LSR#16
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6
	; 15
	LDRB	r6,[r10],#-15
	AND	r5,r14,r9,LSR#24
	SUBS	r6,r6,r5
	RSBLT	r6,r6,#0
	ADD	r0,r0,r6

	ADD	r10,r10,r2	; ii += h_length

	CMP	r0,r3
	BGT	overflow_exit

	SUBS	r12,r12,#1
	BGT	sadmblp1

        Return  "r4-r11"
overflow_exit
	MVN	r0,#1<<31
        Return  "r4-r11"
	|
	; Smart version
SAD_Macroblock
	; r0 = unsigned char *ii
	; r1 = unsigned char *act_block
	; r2 = h_length
	; r3 = Min_Frame
        FunctionEntry "r4-r11"

	MOV	r12,#16		; r12 = i = 16
	MOV	r14,#255
	AND	r4,r0,#3
	BIC	r10,r0,#3	; r10 = ii
	MOV	r0,#0		; r0 = sad = 0
	LDR	PC,[pc,r4,LSL#2]
	MOV	PC,#0
	DCD	SAD_MB_off0
	DCD	SAD_MB_off1
	DCD	SAD_MB_off2
	DCD	SAD_MB_off3
SAD_MB_off0
	SUB	r2,r2,#12
sadmblp1
	LDMIA	r1!,{r4,r7,r8,r9}
	; 0
	LDR	r6,[r10],#4
	AND	r5,r14,r4	; Possible opt here using <<24 values and C flag
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 1
	AND	r5,r14,r4,LSR#8
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 2
	AND	r5,r14,r4,LSR#16
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 3
	AND	r5,r14,r4,LSR#24
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 4
	LDR	r6,[r10],#4
	AND	r5,r14,r7
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 5
	AND	r5,r14,r7,LSR#8
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 6
	AND	r5,r14,r7,LSR#16
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 7
	AND	r5,r14,r7,LSR#24
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 8
	LDR	r6,[r10],#4
	AND	r5,r14,r8
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 9
	AND	r5,r14,r8,LSR#8
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 10
	AND	r5,r14,r8,LSR#16
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 11
	AND	r5,r14,r8,LSR#24
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 12
	LDR	r6,[r10],r2
	AND	r5,r14,r9
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 13
	AND	r5,r14,r9,LSR#8
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 14
	AND	r5,r14,r9,LSR#16
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 15
	AND	r5,r14,r9,LSR#24
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	;ADD	r10,r10,r2	; ii += h_length

	CMP	r0,r3
	BGT	overflow_exit

	SUBS	r12,r12,#1
	BGT	sadmblp1

        Return  "r4-r11"
SAD_MB_off1
	SUB	r2,r2,#16
sadmblp1_1
	LDMIA	r1!,{r4,r7,r8,r9}
	; 0
	LDR	r6,[r10],#4
	AND	r5,r14,r4
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 1
	AND	r5,r14,r4,LSR#8
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 2
	AND	r5,r14,r4,LSR#16
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	LDR	r6,[r10],#4
	ADD	r0,r0,r5
	; 3
	AND	r11,r14,r6
	RSBS	r5,r11,r4,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 4
	AND	r5,r14,r7
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 5
	AND	r5,r14,r7,LSR#8
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 6
	AND	r5,r14,r7,LSR#16
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	LDR	r6,[r10],#4
	ADD	r0,r0,r5
	; 7
	AND	r11,r14,r6
	RSBS	r5,r11,r7,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 8
	AND	r5,r14,r8
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 9
	AND	r5,r14,r8,LSR#8
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 10
	AND	r5,r14,r8,LSR#16
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	LDR	r6,[r10],#4
	ADD	r0,r0,r5
	; 11
	AND	r11,r14,r6
	RSBS	r5,r11,r8,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 12
	AND	r5,r14,r9
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 13
	AND	r5,r14,r9,LSR#8
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 14
	AND	r5,r14,r9,LSR#16
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	LDRB	r11,[r10],r2
	ADD	r0,r0,r5
	; 15
	RSBS	r5,r11,r9,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	;ADD	r10,r10,r2	; ii += h_length

	CMP	r0,r3
	BGT	overflow_exit

	SUBS	r12,r12,#1
	BGT	sadmblp1_1

        Return  "r4-r11"
SAD_MB_off2
	SUB	r2,r2,#16
sadmblp1_2
	LDMIA	r1!,{r4,r7,r8,r9}
	; 0
	LDR	r6,[r10],#4
	AND	r5,r14,r4
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 1
	AND	r5,r14,r4,LSR#8
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	LDR	r6,[r10],#4
	ADD	r0,r0,r5
	; 2
	AND	r5,r14,r4,LSR#16
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 3
	AND	r11,r14,r6,LSR#8
	RSBS	r5,r11,r4,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 4
	AND	r5,r14,r7
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 5
	AND	r5,r14,r7,LSR#8
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	LDR	r6,[r10],#4
	ADD	r0,r0,r5
	; 6
	AND	r5,r14,r7,LSR#16
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 7
	AND	r11,r14,r6,LSR#8
	RSBS	r5,r11,r7,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 8
	AND	r5,r14,r8
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 9
	AND	r5,r14,r8,LSR#8
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	LDR	r6,[r10],#4
	ADD	r0,r0,r5
	; 10
	AND	r5,r14,r8,LSR#16
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 11
	AND	r11,r14,r6,LSR#8
	RSBS	r5,r11,r8,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 12
	AND	r5,r14,r9
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 13
	AND	r5,r14,r9,LSR#8
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	LDR	r6,[r10],r2
	ADD	r0,r0,r5
	; 14
	AND	r5,r14,r9,LSR#16
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 15
	AND	r11,r14,r6,LSR#8
	RSBS	r5,r11,r9,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	;ADD	r10,r10,r2	; ii += h_length

	CMP	r0,r3
	BGT	overflow_exit

	SUBS	r12,r12,#1
	BGT	sadmblp1_2

        Return  "r4-r11"
SAD_MB_off3
	SUB	r2,r2,#16
sadmblp1_3
	LDMIA	r1!,{r4,r7,r8,r9}
	; 0
	LDR	r6,[r10],#4
	AND	r5,r14,r4
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	LDR	r6,[r10],#4
	ADD	r0,r0,r5
	; 1
	AND	r5,r14,r4,LSR#8
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 2
	AND	r5,r14,r4,LSR#16
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 3
	AND	r11,r14,r6,LSR#16
	RSBS	r5,r11,r4,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 4
	AND	r5,r14,r7
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	LDR	r6,[r10],#4
	ADD	r0,r0,r5
	; 5
	AND	r5,r14,r7,LSR#8
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 6
	AND	r5,r14,r7,LSR#16
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 7
	AND	r11,r14,r6,LSR#16
	RSBS	r5,r11,r7,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 8
	AND	r5,r14,r8
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	LDR	r6,[r10],#4
	ADD	r0,r0,r5
	; 9
	AND	r5,r14,r8,LSR#8
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 10
	AND	r5,r14,r8,LSR#16
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 11
	AND	r11,r14,r6,LSR#16
	RSBS	r5,r11,r8,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 12
	AND	r5,r14,r9
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	LDR	r6,[r10],r2
	ADD	r0,r0,r5
	; 13
	AND	r5,r14,r9,LSR#8
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 14
	AND	r5,r14,r9,LSR#16
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 15
	AND	r11,r14,r6,LSR#16
	RSBS	r5,r11,r9,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	;ADD	r10,r10,r2	; ii += h_length

	CMP	r0,r3
	BGT	overflow_exit

	SUBS	r12,r12,#1
	BGT	sadmblp1_3

        Return  "r4-r11"
overflow_exit
	MVN	r0,#1<<31
        Return  "r4-r11"
	]
	; matrix widths are equal and not necessarily 16
SAD_Macroblock0
	; r0 = unsigned char *ii
	; r1 = unsigned char *act_block
	; r2 = h_length
	; r3 = Min_Frame
        FunctionEntry "r4-r11"

	MOV	r12,#16		; r12 = i = 16
	MOV	r14,#255
	MOV	r10,r0  	; r10 = ii
	MOV	r0,#0		; r0 = sad = 0
	SUB	r2,r2,#16
sadmblp1_0
	LDMIA	r1!,{r4,r7,r8,r9}
	; 0
	LDR	r6,[r10],#4
	AND	r5,r14,r4	; Possible opt here using <<24 values and C flag
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 1
	AND	r5,r14,r4,LSR#8
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 2
	AND	r5,r14,r4,LSR#16
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 3
	AND	r5,r14,r4,LSR#24
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 4
	LDR	r6,[r10],#4
	AND	r5,r14,r7
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 5
	AND	r5,r14,r7,LSR#8
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 6
	AND	r5,r14,r7,LSR#16
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 7
	AND	r5,r14,r7,LSR#24
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 8
	LDR	r6,[r10],#4
	AND	r5,r14,r8
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 9
	AND	r5,r14,r8,LSR#8
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 10
	AND	r5,r14,r8,LSR#16
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 11
	AND	r5,r14,r8,LSR#24
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	; 12
	LDR	r6,[r10],#4
	AND	r5,r14,r9
	AND	r11,r14,r6
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 13
	AND	r5,r14,r9,LSR#8
	AND	r11,r14,r6,LSR#8
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 14
	AND	r5,r14,r9,LSR#16
	AND	r11,r14,r6,LSR#16
	SUBS	r5,r5,r11
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5
	; 15
	AND	r5,r14,r9,LSR#24
	SUBS	r5,r5,r6,LSR#24
	RSBLT	r5,r5,#0
	ADD	r0,r0,r5

	ADD	r10,r10,r2	; ii += h_length
	ADD	r1,r1,r2	; ii += h_length

	CMP	r0,r3
	BGT	overflow_exit_0

	SUBS	r12,r12,#1
	BGT	sadmblp1_0

        Return  "r4-r11"
overflow_exit_0
	MVN	r0,#1<<31
        Return  "r4-r11"

FindMB
	; r0 = int x
	; r1 = int y
	; r2 = unsigned char *image
	; r3 = int MB[16][16]
        FunctionEntry "r4,r5"

	LDR	r14,pels
	MOV	r12,#16

	ADD	r0,r0,r2	; r1 = image + x
	MLA	r1,r14,r1,r0	; r1 = image + x + y*pels
	SUB	r14,r14,#15
fmblp
	LDRB	r0,[r1],#1
	LDRB	r2,[r1],#1
	LDRB	r4,[r1],#1
	LDRB	r5,[r1],#1
	STMIA	r3!,{r0,r2,r4,r5}
	LDRB	r0,[r1],#1
	LDRB	r2,[r1],#1
	LDRB	r4,[r1],#1
	LDRB	r5,[r1],#1
	STMIA	r3!,{r0,r2,r4,r5}
	LDRB	r0,[r1],#1
	LDRB	r2,[r1],#1
	LDRB	r4,[r1],#1
	LDRB	r5,[r1],#1
	STMIA	r3!,{r0,r2,r4,r5}
	LDRB	r0,[r1],#1
	LDRB	r2,[r1],#1
	LDRB	r4,[r1],#1
	LDRB	r5,[r1],r14
	SUBS	r12,r12,#1
	STMIA	r3!,{r0,r2,r4,r5}
	BGT	fmblp

        Return  "r4,r5"
FindMBbytes
	; r0 = int x
	; r1 = int y
	; r2 = unsigned char *image
	; r3 = unsigned char MB[16][16]
        FunctionEntry "r4-r8"

	LDR	r14,pels
	MOV	r12,#16

	ADD	r0,r0,r2	; r1 = image + x
	MLA	r1,r14,r1,r0	; r1 = image + x + y*pels
	SUB	r14,r14,#15
fmbblp
	LDRB	r0,[r1],#1
	LDRB	r2,[r1],#1
	LDRB	r4,[r1],#1
	ORR	r0,r0,r2,LSL#8
	LDRB	r5,[r1],#1

	ORR	r0,r0,r4,LSL#16
	LDRB	r2,[r1],#1
	ORR	r0,r0,r5,LSL#24
	LDRB	r4,[r1],#1
	LDRB	r5,[r1],#1
	ORR	r2,r2,r4,LSL#8
	LDRB	r6,[r1],#1

	ORR	r2,r2,r5,LSL#16
	LDRB	r4,[r1],#1
	ORR	r2,r2,r6,LSL#24
	LDRB	r5,[r1],#1
	LDRB	r6,[r1],#1
	ORR	r4,r4,r5,LSL#8
	LDRB	r7,[r1],#1

	ORR	r4,r4,r6,LSL#16
	LDRB	r5,[r1],#1
	ORR	r4,r4,r7,LSL#24
	LDRB	r6,[r1],#1
	SUBS	r12,r12,#1
	LDRB	r7,[r1],#1
	ORR	r5,r5,r6,LSL#8
	LDRB	r8,[r1],r14
	ORR	r5,r5,r7,LSL#16
	ORR	r5,r5,r8,LSL#24
	STMIA	r3!,{r0,r2,r4,r5}
	BGT	fmbblp

        Return  "r4-r8"
	[ 0 = 1
Clip
        FunctionEntry "r4-r9"

	MOV	r9,r0
	MOV	r4,#(16*16+8*8*2)/8
	LDMIA	r0!,{r1,r2,r3,r12}
cliplp
	MOVS	r14,r1,ASR#8
	MOVGT	r1,#255
	MOVLT	r1,#0
	MOVS	r14,r2,ASR#8
	MOVGT	r2,#255
	MOVLT	r2,#0
	MOVS	r14,r3,ASR#8
	MOVGT	r3,#255
	MOVLT	r3,#0
	MOVS	r14,r12,ASR#8
	MOVGT	r12,#255
	MOVLT	r12,#0
	LDMIA	r0!,{r5,r6,r7,r8}
	STMIA	r9!,{r1,r2,r3,r12}
	MOVS	r14,r1,ASR#8
	MOVGT	r1,#255
	MOVLT	r1,#0
	MOVS	r14,r2,ASR#8
	MOVGT	r2,#255
	MOVLT	r2,#0
	MOVS	r14,r3,ASR#8
	MOVGT	r3,#255
	MOVLT	r3,#0
	MOVS	r14,r12,ASR#8
	MOVGT	r12,#255
	MOVLT	r12,#0
	SUBS	r4,r4,#1
	LDMNEIA	r0!,{r5,r6,r7,r8}
	STMIA	r9!,{r5,r6,r7,r8}
	BNE	cliplp

        Return  "r4-r9"
	]
Clip
        FunctionEntry "r4"

	MOV	r4,#(16*16+8*8*2)/4
cliplp
	LDMIA	r0,{r1,r2,r3,r12}
	MOVS	r14,r1,ASR#8
	MOVGT	r1,#255
	MOVLT	r1,#0
	MOVS	r14,r2,ASR#8
	MOVGT	r2,#255
	MOVLT	r2,#0
	MOVS	r14,r3,ASR#8
	MOVGT	r3,#255
	MOVLT	r3,#0
	MOVS	r14,r12,ASR#8
	MOVGT	r12,#255
	MOVLT	r12,#0
	STMIA	r0!,{r1,r2,r3,r12}
	SUBS	r4,r4,#1
	BNE	cliplp

        Return  "r4"
	; This fn really needs some more optimisation, but its hairy.
	[ 0 = 1
LoadArea
	; r0 = char *im
	; r1 = x
	; r2 = y
	; r3 = x_size
	; <> = y_size
	; <> = lx
	; <> = out
	MOV	r12,r13
        FunctionEntry "r4-r10"
	ADD	r0,r0,r1
	LDMFD	r12,{r1,r12,r14}	; r1  = y_size
					; r12 = lx
					; r14 = out
	MLA	r0,r2,r12,r0	; r0 = in = im + x + (y*lx)
	SUB	r12,r12,r3
lalp1
	LDRB	r2,[r0],#1
	LDRB	r5,[r0],#1
	LDRB	r6,[r0],#1
	ORR	r2,r2,r5,LSL#8
	LDRB	r7,[r0],#1
	ORR	r2,r2,r6,LSL#16

	LDRB	r5,[r0],#1
	ORR	r2,r2,r7,LSL#24
	LDRB	r6,[r0],#1
	LDRB	r7,[r0],#1
	ORR	r5,r5,r6,LSL#8
	LDRB	r8,[r0],#1
	ORR	r5,r5,r7,LSL#16

	LDRB	r6,[r0],#1
	ORR	r5,r5,r8,LSL#24
	LDRB	r7,[r0],#1
	LDRB	r8,[r0],#1
	ORR	r6,r6,r7,LSL#8
	LDRB	r9,[r0],#1
	ORR	r6,r6,r8,LSL#16

	LDRB	r7,[r0],#1
	ORR	r6,r6,r9,LSL#24
	LDRB	r8,[r0],#1
	SUBS	r4,r3,#16
	LDRB	r9,[r0],#1
	ORR	r7,r7,r8,LSL#8
	LDRB	r10,[r0],#1
	ORR	r7,r7,r9,LSL#16
	ORR	r7,r7,r10,LSL#24

	STMIA	r14!,{r2,r5,r6,r7}
	BEQ	la_extskip
lalp2
	LDRB	r2,[r0],#1
	LDRB	r5,[r0],#1
	SUBS	r4,r4,#4
	LDRB	r6,[r0],#1
	ORR	r2,r2,r5,LSL#8
	LDRB	r7,[r0],#1
	ORR	r2,r2,r6,LSL#16
	ORR	r2,r2,r7,LSL#24

	STR	r2,[r14],#4
	BNE	lalp2
la_extskip
	ADD	r0,r0,r12
	SUBS	r1,r1,#1
	BGE	lalp1

        Return  "r4-r10"
	|
LoadArea
	; r0 = char *im
	; r1 = x
	; r2 = y
	; r3 = x_size
	; <> = y_size
	; <> = lx
	; <> = out
	MOV	r12,r13
        FunctionEntry "r4-r10"
	ADD	r0,r0,r1
	LDMFD	r12,{r1,r4,r5}	; r1  = y_size
				; r4 = lx
				; r5 = out
	MLA	r0,r2,r4,r0	; r0 = in = im + x + (y*lx)
	SUB	r4,r4,r3
	AND	r6,r0,#3
	BIC	r0,r0,#3
	LDR	PC,[PC,r6,LSL#2]
	MOV	PC,#0
	DCD	la_off0
	DCD	la_off1
	DCD	la_off2
	DCD	la_off3
la_off0
lalp1_0
	LDMIA	r0!,{r2,r6,r7,r8}
	STMIA	r5!,{r2,r6,r7,r8}
	SUBS	r2,r3,#16
	BEQ	la_extskip_0
lalp2_0
	LDR	r6,[r0],#4
	SUBS	r2,r2,#4
	STR	r6,[r5],#4
	BNE	lalp2_0
la_extskip_0
	ADD	r0,r0,r4
	SUBS	r1,r1,#1
	BGE	lalp1_0

        Return  "r4-r10"
la_off1
	SUB	r4,r4,#4
lalp1_1
	LDMIA	r0!,{r2,r6,r7,r8,r9}
	MOV	r2,r2,LSR#8
	ORR	r2,r2,r6,LSL#24
	MOV	r6,r6,LSR#8
	ORR	r6,r6,r7,LSL#24
	MOV	r7,r7,LSR#8
	ORR	r7,r7,r8,LSL#24
	MOV	r8,r8,LSR#8
	ORR	r8,r8,r9,LSL#24
	STMIA	r5!,{r2,r6,r7,r8}
	SUBS	r2,r3,#16
	BEQ	la_extskip_1
lalp2_1
	LDR	r8,[r0],#4
	MOV	r9,r9,LSR#8
	ORR	r9,r9,r8,LSL#24
	STR	r9,[r5],#4
	MOV	r9,r8
	SUBS	r2,r2,#4
	BNE	lalp2_1
la_extskip_1
	ADD	r0,r0,r4
	SUBS	r1,r1,#1
	BGE	lalp1_1

        Return  "r4-r10"
la_off2
	SUB	r4,r4,#4
lalp1_2
	LDMIA	r0!,{r2,r6,r7,r8,r9}
	MOV	r2,r2,LSR#16
	ORR	r2,r2,r6,LSL#16
	MOV	r6,r6,LSR#16
	ORR	r6,r6,r7,LSL#16
	MOV	r7,r7,LSR#16
	ORR	r7,r7,r8,LSL#16
	MOV	r8,r8,LSR#16
	ORR	r8,r8,r9,LSL#16
	STMIA	r5!,{r2,r6,r7,r8}
	SUBS	r2,r3,#16
	BEQ	la_extskip_2
lalp2_2
	LDR	r8,[r0],#4
	MOV	r9,r9,LSR#16
	ORR	r9,r9,r8,LSL#16
	STR	r9,[r5],#4
	MOV	r9,r8
	SUBS	r2,r2,#4
	BNE	lalp2_2
la_extskip_2
	ADD	r0,r0,r4
	SUBS	r1,r1,#1
	BGE	lalp1_2

        Return  "r4-r10"
la_off3
	SUB	r4,r4,#4
lalp1_3
	LDMIA	r0!,{r2,r6,r7,r8,r9}
	MOV	r2,r2,LSR#24
	ORR	r2,r2,r6,LSL#8
	MOV	r6,r6,LSR#24
	ORR	r6,r6,r7,LSL#8
	MOV	r7,r7,LSR#24
	ORR	r7,r7,r8,LSL#8
	MOV	r8,r8,LSR#24
	ORR	r8,r8,r9,LSL#9
	STMIA	r5!,{r2,r6,r7,r8}
	SUBS	r2,r3,#16
	BEQ	la_extskip_3
lalp2_3
	LDR	r8,[r0],#4
	MOV	r9,r9,LSR#24
	ORR	r9,r9,r8,LSL#8
	STR	r9,[r5],#4
	MOV	r9,r8
	SUBS	r2,r2,#4
	BNE	lalp2_3
la_extskip_3
	ADD	r0,r0,r4
	SUBS	r1,r1,#1
	BGE	lalp1_3

        Return  "r4-r10"
	]
search
	% 9*8
	[ 0 = 1
	; Now superseded by FindHalfPelPlus
FindHalfPel
	; r0 = int x
	; r1 = int y
	; r2 = MotionVector *fr
	; r3 = unsigned char *prev
	; <> = char *curr
	; <> = int bs
	; <> = int comp
	MOV	r12,r13
        FunctionEntry "r4-r11"

	LDMIA	r2!,{r7,r8}	; r7 = fr->x
				; r8 = fr->y

	LDMFD	r12,{r4,r5,r6}	; r4 = curr
				; r5 = bs
				; r6 = comp
	ADD	r0,r0,r7	; r0 = new_x = x + fr->x
	ADD	r1,r1,r8	; r0 = new_y = y + fr->y

	LDR	r7,pels

	TST	r6,#1
	ADDNE	r0,r0,#8	; r0 = new_x += ((comp&1)<<3);
	TST	r6,#2
	ADDNE	r1,r1,#8	; r1 = new_y += ((comp&2)<<2);
	; scratch r6
	MVN	r9,#0		; r9  = start_x = -1
	MOV	r10,#1		; r10 = stop_x  =  1
	MVN	r11,#0		; r11 = start_y = -1

	LDR	r14,mv_outside_frame

	MOVS	r8,r14		; r8 = 0 if mv_outside_frame == 0, !=0 otherwise
	LDRNE	r8,long_vectors	; r8 = 0 if mv_outside_frame == 0,
				;	long_vectors otherwise
	MOV	r12,#1		; r12 = stop_y  =  1
	MOV	r8,r8,ASL#6	; r8 = 0 if mv_outside_frame == 0,
				;    = (long_vectors?64:0) otherwise
	ADDNE	r8,r8,#64	; r8 = 0 if mv_outside_frame == 0,
				;    = (long_vectors?128:64) otherwise
	ADD	r6,r8,r7,LSL#1	; r6 = lx*2 = pels*2 + r8
	BNE	mv_not_inside

	CMP	r0,#0		; if (new_x <= 0)
	MOVLE	r9,#0		;   r9 = new_x = 0
	CMP	r1,#0		; if (new_y <= 0)
	MOVLE	r11,#0		;   r11 = new_y = 0

	LDR	r14,lines	; r14 = lines
	SUB	r7,r7,r5	; r7 = pels-bs
	SUB	r14,r14,r5	; r14 = lines-bs
	CMP	r0,r7		; if (new_x >= pels-bs)
	MOVGE	r10,#0		;   stop_x = 0
	CMP	r1,r14		; if (new_y >= lines-bs)
	MOVGE	r12,#0		;   stop_y = 0
	; scratch r7,r14
mv_not_inside
	; So here we have:
	; r0 = new_x
	; r1 = new_y
	; r2 = fr+8
	; r3 = prev
	; r4 = curr
	; r5 = bs
	; r6 = lx*2
	; r9 = start_x
	; r10= stop_x
	; r11= start_y
	; r12= stop_y
	MOV	r8,#0
	MOV	r14,#0
	ADR	r7,search
	; And thanks to the wonder of the WBB, these should merge into
	; efficient 'bursty' writes...
	STMIA	r7!,{r8,r14}	;search[0].x,y = 0		0
	STMIA	r7!,{r9,r11}	;search[1].x,y = start_x	start_y
	STMIA	r7!,{r8,r11}	;search[2].x,y = 0		start_y
	STMIA	r7!,{r10,r11}	;search[3].x,y = stop_x		start_y
	STMIA	r7!,{r9,r14}	;search[4].x,y = start_x	0
	STMIA	r7!,{r10,r14}	;search[5].x,y = stop_x		0
	STMIA	r7!,{r9,r12}	;search[6].x,y = start_x	stop_y
	STMIA	r7!,{r8,r12}	;search[7].x,y = 0		stop_y
	STMIA	r7!,{r10,r12}	;search[8].x,y = stop_x		stop_y
	; scratch r8,r9,r10,r11,r12,r14
	; <FX: Breathe out>
	STMFD	r13!,{r2}
	ADD	r3,r3,r0,LSL#1	; r3 = prev + 2*new_x
	MUL	r1,r6,r1
	ADD	r3,r3,r1,LSL#1
	; scratch r0,r1,r2
	SUB	r7,r7,#9*8
	MVN	r10,#1<<31	; r10 = AE_min = INT_MAX
	STR	r10,AE_min
	MOV	r10,#9
	MOV	r2,#&FF
searchlp
	LDMIA	r7!,{r0,r1}	; r0 = search[i].x
				; r1 = search[i].y
	MOV	r11,#0		; r11 = AE
	MOV	r12,r5		; r12 = bs
	; We want to read from:
	; *(prev+2*new_x + 2*m+search[i].x+ (2*new_y + 2*n + search[i].y)*lx*2);
	; This is the same as:
	; *(prev+2*new_x+srch[i].x+(2*new_y+srch[i].y)*lx<<1+m<<1+n*lx<<2)
	; Which resolves to:
	; *(r3 + r0 + r1*lx<<1    +    m<<1 + n*lx<<2)
	; First bit constant...        ^^^^^^^^^^^^^^^ This bit variable.
	MLA	r1,r6,r1,r3	; r1 = r1*lx<<1+r3
	ADD	r8,r1,r0	; r8 = r3 + r0 + r1*lx<<1
	SUB	r8,r8,r6,LSL#1
	; scratch r3,r9,
sylp
	ADD	r8,r8,r6,LSL#1	; prev += 2*(lx*2)
	LDR	r14,[r4]	; r14 = *(curr +m + n*16)
	; 0
	LDRB	r0,[r8]		; r0 = *r8
	AND	r1,r2,r14
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 1
	LDRB	r0,[r8,#2]	; r0 = *r8
	AND	r1,r2,r14,LSR#8
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 2
	LDRB	r0,[r8,#4]	; r0 = *r8
	AND	r1,r2,r14,LSR#16
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 3
	LDRB	r0,[r8,#6]	; r0 = *r8
	AND	r1,r2,r14,LSR#24
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1

	LDR	r14,[r4,#4]	; r14 = *(curr +m + n*16)
	; 4
	LDRB	r0,[r8,#8]	; r0 = *r8
	AND	r1,r2,r14
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 5
	LDRB	r0,[r8,#10]	; r0 = *r8
	AND	r1,r2,r14,LSR#8
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 6
	LDRB	r0,[r8,#12]	; r0 = *r8
	AND	r1,r2,r14,LSR#16
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 7
	LDRB	r0,[r8,#14]	; r0 = *r8
	AND	r1,r2,r14,LSR#24
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1

	CMP	r5,#8
	BEQ	sskip

	LDR	r14,[r4,#8]	; r14 = *(curr +m + n*16)
	; 8
	LDRB	r0,[r8,#16]		; r0 = *r8
	AND	r1,r2,r14
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 9
	LDRB	r0,[r8,#18]	; r0 = *r8
	AND	r1,r2,r14,LSR#8
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; A
	LDRB	r0,[r8,#20]	; r0 = *r8
	AND	r1,r2,r14,LSR#16
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; B
	LDRB	r0,[r8,#22]	; r0 = *r8
	AND	r1,r2,r14,LSR#24
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1

	LDR	r14,[r4,#12]	; r14 = *(curr +m + n*16)
	; C
	LDRB	r0,[r8,#24]	; r0 = *r8
	AND	r1,r2,r14
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; D
	LDRB	r0,[r8,#26]	; r0 = *r8
	AND	r1,r2,r14,LSR#8
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; E
	LDRB	r0,[r8,#28]	; r0 = *r8
	AND	r1,r2,r14,LSR#16
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; F
	LDRB	r0,[r8,#30]	; r0 = *r8
	AND	r1,r2,r14,LSR#24
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
sskip
	ADD	r4,r4,#16	; curr += 16
	SUBS	r12,r12,#1
	BNE	sylp

	LDR	r9,AE_min
	CMP	r11,r9
	STRLT	r10,min_pos
	STRLT	r11,AE_min

	SUB	r4,r4,r5,LSL#4	; R4 -= 16*bs

	SUBS	r10,r10,#1
	BNE	searchlp

	LDMFD	r13!,{r2}

	LDR	r9,min_pos
	LDR	r14,AE_min
	SUB	r7,r7,r9,LSL#3
	LDMIA	r7,{r11,r12}
	STMIA	r2!,{r11,r12,r14}

        Return  "r4-r11"
	]
FindHalfPelPlus
	; r0 = int x
	; r1 = int y
	; r2 = MotionVector *fr
	; r3 = unsigned char *prev
	; <> = char *curr
	; <> = int bs
	; <> = int comp
	MOV	r12,r13
        FunctionEntry "r4-r11"

	LDMIA	r2!,{r8,r9}	; r8 = fr->x
				; r9 = fr->y

	LDMFD	r12,{r4,r5,r6}	; r4 = curr
				; r5 = bs
				; r6 = comp
	LDR	r7,pels
	ADD	r4,r4,r0	; r4 = curr + x
	MLA	r4,r1,r7,r4	; r4 = curr + x + y*pels

	ADD	r0,r0,r8	; r0 = new_x = x + fr->x
	ADD	r1,r1,r9	; r0 = new_y = y + fr->y


	TST	r6,#1
	ADDNE	r0,r0,#8	; r0 = new_x += ((comp&1)<<3);
	TST	r6,#2
	ADDNE	r1,r1,#8	; r1 = new_y += ((comp&2)<<2);
	; scratch r6
	MVN	r9,#0		; r9  = start_x = -1
	MOV	r10,#1		; r10 = stop_x  =  1

	LDR	r14,mv_outside_frame
	MOV	r12,#1		; r12 = stop_y  =  1
	MOVS	r8,r14		; r8 = 0 if mv_outside_frame == 0, !=0 otherwise
	LDRNE	r8,long_vectors	; r8 = 0 if mv_outside_frame == 0,
				;	long_vectors otherwise
	MVN	r11,#0		; r11 = start_y = -1
	MOV	r8,r8,ASL#6	; r8 = 0 if mv_outside_frame == 0,
				;    = (long_vectors?64:0) otherwise
	ADDNE	r8,r8,#64	; r8 = 0 if mv_outside_frame == 0,
				;    = (long_vectors?128:64) otherwise
	ADD	r6,r8,r7,LSL#1	; r6 = lx*2 = pels*2 + r8
	BNE	mv_not_inside2

	CMP	r0,#0		; if (new_x <= 0)
	MOVLE	r9,#0		;   r9 = new_x = 0
	CMP	r1,#0		; if (new_y <= 0)
	MOVLE	r11,#0		;   r11 = new_y = 0

	LDR	r14,lines	; r14 = lines
	SUB	r7,r7,r5	; r7 = pels-bs
	SUB	r14,r14,r5	; r14 = lines-bs
	CMP	r0,r7		; if (new_x >= pels-bs)
	MOVGE	r10,#0		;   stop_x = 0
	CMP	r1,r14		; if (new_y >= lines-bs)
	MOVGE	r12,#0		;   stop_y = 0
	; scratch r7,r14
mv_not_inside2
	; So here we have:
	; r0 = new_x
	; r1 = new_y
	; r2 = fr+8
	; r3 = prev
	; r4 = curr
	; r5 = bs
	; r6 = lx*2
	; r9 = start_x
	; r10= stop_x
	; r11= start_y
	; r12= stop_y
	MOV	r8,#0
	MOV	r14,#0
	ADR	r7,search
	; And thanks to the wonder of the WBB, these should merge into
	; efficient 'bursty' writes...
	STMIA	r7!,{r8,r14}	;search[0].x,y = 0		0
	STMIA	r7!,{r9,r11}	;search[1].x,y = start_x	start_y
	STMIA	r7!,{r8,r11}	;search[2].x,y = 0		start_y
	STMIA	r7!,{r10,r11}	;search[3].x,y = stop_x		start_y
	STMIA	r7!,{r9,r14}	;search[4].x,y = start_x	0
	STMIA	r7!,{r10,r14}	;search[5].x,y = stop_x		0
	STMIA	r7!,{r9,r12}	;search[6].x,y = start_x	stop_y
	STMIA	r7!,{r8,r12}	;search[7].x,y = 0		stop_y
	STMIA	r7!,{r10,r12}	;search[8].x,y = stop_x		stop_y
	; scratch r8,r9,r10,r11,r12,r14
	; <FX: Breathe out>
	STMFD	r13!,{r2}
	ADD	r3,r3,r0,LSL#1	; r3 = prev + 2*new_x
	MUL	r1,r6,r1
	ADD	r3,r3,r1,LSL#1
	; scratch r0,r1,r2
	SUB	r7,r7,#9*8
	MVN	r10,#1<<31	; r10 = AE_min = INT_MAX
	STR	r10,AE_min
	MOV	r10,#9
	MOV	r2,#&FF
	LDR	r9,pels
searchlp2
	LDMIA	r7!,{r0,r1}	; r0 = search[i].x
				; r1 = search[i].y
	MOV	r11,#0		; r11 = AE
	MOV	r12,r5		; r12 = bs
	; We want to read from:
	; *(prev+2*new_x + 2*m+search[i].x+ (2*new_y + 2*n + search[i].y)*lx*2);
	; This is the same as:
	; *(prev+2*new_x+srch[i].x+(2*new_y+srch[i].y)*lx<<1+m<<1+n*lx<<2)
	; Which resolves to:
	; *(r3 + r0 + r1*lx<<1    +    m<<1 + n*lx<<2)
	; First bit constant...        ^^^^^^^^^^^^^^^ This bit variable.
	MLA	r1,r6,r1,r3	; r1 = r1*lx<<1+r3
	ADD	r8,r1,r0	; r8 = r3 + r0 + r1*lx<<1
	SUB	r8,r8,r6,LSL#1
	; scratch r3,r9
sylp2
	ADD	r8,r8,r6,LSL#1	; prev += 2*(lx*2)
	LDR	r14,[r4]	; r14 = *(curr +m + n*16)
	; 0
	LDRB	r0,[r8]		; r0 = *r8
	AND	r1,r2,r14
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 1
	LDRB	r0,[r8,#2]	; r0 = *r8
	AND	r1,r2,r14,LSR#8
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 2
	LDRB	r0,[r8,#4]	; r0 = *r8
	AND	r1,r2,r14,LSR#16
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 3
	LDRB	r0,[r8,#6]	; r0 = *r8
	AND	r1,r2,r14,LSR#24
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1

	LDR	r14,[r4,#4]	; r14 = *(curr +m + n*16)
	; 4
	LDRB	r0,[r8,#8]	; r0 = *r8
	AND	r1,r2,r14
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 5
	LDRB	r0,[r8,#10]	; r0 = *r8
	AND	r1,r2,r14,LSR#8
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 6
	LDRB	r0,[r8,#12]	; r0 = *r8
	AND	r1,r2,r14,LSR#16
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 7
	LDRB	r0,[r8,#14]	; r0 = *r8
	AND	r1,r2,r14,LSR#24
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1

	CMP	r5,#8
	BEQ	sskip2

	LDR	r14,[r4,#8]	; r14 = *(curr +m + n*16)
	; 8
	LDRB	r0,[r8,#16]		; r0 = *r8
	AND	r1,r2,r14
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; 9
	LDRB	r0,[r8,#18]	; r0 = *r8
	AND	r1,r2,r14,LSR#8
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; A
	LDRB	r0,[r8,#20]	; r0 = *r8
	AND	r1,r2,r14,LSR#16
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; B
	LDRB	r0,[r8,#22]	; r0 = *r8
	AND	r1,r2,r14,LSR#24
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1

	LDR	r14,[r4,#12]	; r14 = *(curr +m + n*16)
	; C
	LDRB	r0,[r8,#24]	; r0 = *r8
	AND	r1,r2,r14
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; D
	LDRB	r0,[r8,#26]	; r0 = *r8
	AND	r1,r2,r14,LSR#8
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; E
	LDRB	r0,[r8,#28]	; r0 = *r8
	AND	r1,r2,r14,LSR#16
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
	; F
	LDRB	r0,[r8,#30]	; r0 = *r8
	AND	r1,r2,r14,LSR#24
	SUBS	r1,r1,r0
	RSBLT	r1,r1,#0
	ADD	r11,r11,r1
sskip2
	ADD	r4,r4,r9	; curr += 16
	SUBS	r12,r12,#1
	BNE	sylp2

	MUL	r0,r9,r5	; r12 = bs*pels
	LDR	r12,AE_min
	SUB	r4,r4,r0	; r4 = curr -= bs*pels
	CMP	r11,r12
	STRLT	r10,min_pos
	STRLT	r11,AE_min

	SUBS	r10,r10,#1
	BNE	searchlp2

	LDMFD	r13!,{r2}

	LDR	r9,min_pos
	LDR	r14,AE_min
	SUB	r7,r7,r9,LSL#3
	LDMIA	r7,{r11,r12}
	STMIA	r2!,{r11,r12,r14}

        Return  "r4-r11"

cpels
	DCD	0
pels
	DCD	0
lines
	DCD	0
long_vectors
	DCD	0
mv_outside_frame
	DCD	0
min_pos
	DCD	0
AE_min
	DCD	0
ChooseMode
	; r0 = unsigned char *curr
	; r1 = int x_pos
	; r2 = int y_pos
	; r3 = int min_SAD
        FunctionEntry "r4-r6,r10-r11"

	LDR	r12,pels
	MLA	r0,r2,r12,r0	; r0 = curr + yoff = curr + y_pos*pels
	ADD	r0,r0,r1	; r0 = curr + yoff + x_pos

	MOV	r11,#0		; r11 = MB_mean
	MOV	r14,#255
	ORR	r14,r14,r14,LSL#16
	MOV	r10,#16
cm_lp1
	LDMIA	r0,{r1,r2,r4,r5}
					; Max values possible in each nibble:
	AND	r6,r14,r1		; r6 = 00FF00FF
	AND	r1,r14,r1,LSR#8		; r1 = 00FF00FF
	ADD	r1,r6,r1		; r1 = 01FF01FF

	AND	r6,r14,r2		; r6 = 00FF00FF
	AND	r2,r14,r2,LSR#8		; r2 = 00FF00FF
	ADD	r2,r6,r2		; r2 = 01FF01FF

	AND	r6,r14,r4		; r6 = 00FF00FF
	AND	r4,r14,r4,LSR#8		; r4 = 00FF00FF
	ADD	r4,r6,r4		; r4 = 01FF01FF

	AND	r6,r14,r5		; r6 = 00FF00FF
	AND	r5,r14,r5,LSR#8		; r5 = 00FF00FF
	ADD	r5,r6,r5		; r5 = 01FF01FF

	ADD	r1,r1,r2
	ADD	r1,r1,r4
	ADD	r1,r1,r5		; r1 = 07FF07FF
	ADD	r1,r1,r1,LSL#16		; r1 = 0FFF0XXX

	ADD	r11,r11,r1,LSR#16	; r11 = MB_mean += (r1>>16)
	SUBS	r10,r10,#1
	ADDNE	r0,r0,r12
	BNE	cm_lp1

	; So r11 = MB_mean * 256
	MOV	r14,#255
	MOV	r10,#16
	MOV	r11,r11,ASR#8
cm_lp2
	LDMIA	r0,{r1,r2,r4,r5}
	SUBS	r0,r0,r12
	; 0
	AND	r6,r14,r1
	SUBS	r6,r6,r11
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 1
	AND	r6,r14,r1,LSR#8
	SUBS	r6,r6,r11
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 2
	AND	r6,r14,r1,LSR#16
	SUBS	r6,r6,r11
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 3
	RSBS	r6,r11,r1,LSR#24
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 4
	AND	r6,r14,r2
	SUBS	r6,r6,r11
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 5
	AND	r6,r14,r2,LSR#8
	SUBS	r6,r6,r11
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 6
	AND	r6,r14,r2,LSR#16
	SUBS	r6,r6,r11
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 7
	RSBS	r6,r11,r2,LSR#24
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 8
	AND	r6,r14,r4
	SUBS	r6,r6,r11
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 9
	AND	r6,r14,r4,LSR#8
	SUBS	r6,r6,r11
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 10
	AND	r6,r14,r4,LSR#16
	SUBS	r6,r6,r11
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 11
	RSBS	r6,r11,r4,LSR#24
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 12
	AND	r6,r14,r5
	SUBS	r6,r6,r11
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 13
	AND	r6,r14,r5,LSR#8
	SUBS	r6,r6,r11
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 14
	AND	r6,r14,r5,LSR#16
	SUBS	r6,r6,r11
	RSBLT	r6,r6,#0
	SUB	r3,r3,r6
	; 15
	RSBS	r6,r11,r5,LSR#24
	RSBLT	r6,r6,#0
	SUBS	r3,r3,r6

	SUBGTS	r10,r10,#1
	BGT	cm_lp2

	CMP	r3,#0
	MOVGT	r0,#3		; MODE_INTRA
	MOVLE	r0,#0		; MODE_INTER

        Return  "r4-r6,r10-r11"
FindPred
	; r0 = x
	; r1 = y
	; r2 = fr
	; r3 = prev
	; <> = curr
	; <> = pred
	MOV	r12,r13
        FunctionEntry "r4-r9"

	LDR	r7,mv_outside_frame
	MOVS	r14,r7
	LDRNE	r14,long_vectors; r14 = mv_outside_frame?(long_vectors      ):0
	LDR	r9,pels
	MOVNE	r14,r14,LSL#5	; r14 = mv_outside_frame?(long_vectors?32: 0):0
	ADDNE	r14,r14,#32	; r14 = mv_outside_frame?(long_vectors?64:32):0
	ADD	r7,r9,r14	; r7 = lx
	; scratch r14
	LDMFD	r12,{r12,r14}	; r12 = curr
 				; r14 = pred
	ADD	r12,r12,r0	; r12 = image + x
	MLA	r12,r9,r1,r12	; r12 = image + x + y*pels
	SUB	r9,r9,#14

	LDMIA	r2,{r2,r4,r5,r6}; r2 = fr->x
				; r4 = fr->y
				; r5 = fr->x_half
				; r6 = fr->y_half
	ADD	r0,r0,r2
	ADD	r1,r1,r4
	ADD	r3,r3,r0,LSL#1	; r3 = prev + new_x*2
	ADD	r3,r3,r5	; r3 = prev + new_x*2 + fr->x_half
	ADD	r1,r6,r1,LSL#1	; r1 =  (new_y*2) + fr->y_half
	MOV	r7,r7,LSL#1	; r7 = lx*2
	MLA	r1,r7,r1,r3	; r1 = prev + new_x*2 + fr->x_half +
				;	((new_y*2) + fr->y_half)*lx*2
	SUB	r7,r7,#14
	MOV	r6,#16
fp_lp1
	; scratch r0,r2,r3,r4,r5,r6
	LDRB	r0,[r1],#2
	LDRB	r8,[r12],#1
	LDRB	r2,[r1],#2
	SUB	r0,r8,r0
	LDRB	r8,[r12],#1
	LDRB	r3,[r1],#2
	SUB	r2,r8,r2
	LDRB	r8,[r12],#1
	LDRB	r4,[r1],#2
	SUB	r3,r8,r3
	LDRB	r8,[r12],#1
	LDRB	r5,[r1,#6]
	SUB	r4,r8,r4
	LDRB	r8,[r12,#3]
	STMIA	r14!,{r0,r2,r3,r4}
	SUB	r5,r8,r5
	LDRB	r8,[r12],#1
	LDRB	r0,[r1],#2
	LDRB	r2,[r1],#2
	SUB	r0,r8,r0
	LDRB	r8,[r12],#1
	LDRB	r3,[r1],#4
	SUB	r2,r8,r2
	LDRB	r8,[r12],#2
	; r5 already loaded
	LDRB	r4,[r1,#6]
	SUB	r3,r8,r3
	LDRB	r8,[r12,#3]
	STMIA	r14!,{r0,r2,r3,r5}
	SUB	r4,r8,r4
	LDRB	r8,[r12],#1
	LDRB	r0,[r1],#2
	LDRB	r2,[r1],#2
	SUB	r0,r8,r0
	LDRB	r8,[r12],#1
	LDRB	r3,[r1],#4
	SUB	r2,r8,r2
	LDRB	r8,[r12],#2
	; r4 already loaded
	LDRB	r5,[r1,#6]
	SUB	r3,r8,r3
	LDRB	r8,[r12,#3]
	STMIA	r14!,{r0,r2,r3,r4}
	SUB	r5,r8,r5
	LDRB	r8,[r12],#1
	LDRB	r0,[r1],#2
	LDRB	r2,[r1],#2
	SUB	r0,r8,r0
	LDRB	r8,[r12],#1
	LDRB	r3,[r1],r7,LSL#1
	SUB	r2,r8,r2
	LDRB	r8,[r12],r9
	SUBS	r6,r6,#1
	SUB	r3,r8,r3
	STMIA	r14!,{r0,r2,r3,r5}

	BGT	fp_lp1

        Return  "r4-r9"
FindPred0
	; r0 = x
	; r1 = y
	; r2 = prev
	; r3 = curr
	; <> = pred
	LDMFD	r13,{r12}	; r12 = pred
        FunctionEntry "r4-r8"

	LDR	r7,pels

	MLA	r1,r7,r1,r0	; r1 = x + y*pels
	ADD	r14,r3,r1	; r14 = image + x

	ADD	r1,r2,r1	; r3 = prev + x
	SUB	r7,r7,#14
	MOV	r6,#16
fp0_lp1
	; scratch r0,r2,r3,r4,r5,r6
	LDRB	r0,[r1],#1
	LDRB	r8,[r14],#1
	LDRB	r2,[r1],#1
	SUB	r0,r8,r0
	LDRB	r8,[r14],#1
	LDRB	r3,[r1],#1
	SUB	r2,r8,r2
	LDRB	r8,[r14],#1
	LDRB	r4,[r1],#1
	SUB	r3,r8,r3
	LDRB	r8,[r14],#1
	LDRB	r5,[r1,#3]
	SUB	r4,r8,r4
	LDRB	r8,[r14,#3]
	STMIA	r12!,{r0,r2,r3,r4}
	SUB	r5,r8,r5
	LDRB	r8,[r14],#1
	LDRB	r0,[r1],#1
	LDRB	r2,[r1],#1
	SUB	r0,r8,r0
	LDRB	r8,[r14],#1
	LDRB	r3,[r1],#2
	SUB	r2,r8,r2
	LDRB	r8,[r14],#2
	; r5 already loaded
	LDRB	r4,[r1,#3]
	SUB	r3,r8,r3
	LDRB	r8,[r14,#3]
	STMIA	r12!,{r0,r2,r3,r5}
	SUB	r4,r8,r4
	LDRB	r8,[r14],#1
	LDRB	r0,[r1],#1
	LDRB	r2,[r1],#1
	SUB	r0,r8,r0
	LDRB	r8,[r14],#1
	LDRB	r3,[r1],#2
	SUB	r2,r8,r2
	LDRB	r8,[r14],#2
	; r4 already loaded
	LDRB	r5,[r1,#3]
	SUB	r3,r8,r3
	LDRB	r8,[r14,#3]
	STMIA	r12!,{r0,r2,r3,r4}
	SUB	r5,r8,r5
	LDRB	r8,[r14],#1
	LDRB	r0,[r1],#1
	LDRB	r2,[r1],#1
	SUB	r0,r8,r0
	LDRB	r8,[r14],#1
	LDRB	r3,[r1],r7
	SUB	r2,r8,r2
	LDRB	r8,[r14],r7
	SUBS	r6,r6,#1
	SUB	r3,r8,r3
	STMIA	r12!,{r0,r2,r3,r5}

	BGT	fp0_lp1

        Return  "r4-r8"
DoPredChrom_P
	; r0 = x_curr
	; r1 = y_curr
	; r2 = dx
	; r3 = dy
	; <> = curr
	; <> = prev
	; <> = pred_error
	MOV	r12,r13
        FunctionEntry "r4-r11"

	AND	r4,r2,#1
	AND	r5,r3,#1
	ORR	r4,r4,r5,LSL#1	; r4 = xh + (yh<<1)
	MOV	r0,r0,ASR#1	; x = x_curr>>1
	MOV	r1,r1,ASR#1	; y = y_curr>>1

	LDMFD	r12,{r6,r7,r12}	; r6 = curr
				; r7 = prev
				; r12= pred_error
	LDMIB	r6,{r6,r14}	; r6 = curr->Cr
				; r14= curr->Cb
	LDMIB	r7,{r7,r10}	; r7 = prev->Cr
				; r10= prev->Cb
	; scratch r14
	LDR	r8,cpels
	MLA	r5,r8,r1,r0	; r5 = x + y*cpels + x
	SUB	r14,r14,r6	; curr->Cb - curr->Cr
	ADD	r6,r6,r5	; r6 = curr->Cr + x + y*cpels

	ADD	r0,r0,r2,ASR#1	; r0 = x + xint
	ADD	r1,r1,r3,ASR#1	; r1 = y + yint

	LDR	r3,mv_outside_frame
	MOVS	r5,r3
	LDRNE	r5,long_vectors	; r5 = mv_outside_frame?(long_vectors      ):0
	LDR	r3,pels
	MOVNE	r5,r5,LSL#4	; r5 = mv_outside_frame?(long_vectors?16: 0):0
	ADDNE	r5,r5,#16	; r5 = mv_outside_frame?(long_vectors?32:16):0
	ADD	r3,r5,r3,LSR#1	; r3= lx

	MLA	r1,r3,r1,r0	; r1 = x + xint + (y + yint)*lx
	SUB	r10,r10,r7	; r10 = prev->Cb - prev->Cr
	ADD	r7,r7,r1	; r7 = prev->Cr + x + xint

	ADD	r12,r12,#16*16*4	; r3 = pred_error->Cr
	MOV	r0,#8
	SUB	r8,r8,#8
	; scratch r1,r2,r5
	; r0 = 8 (y counter)
	; r3 = lx
	; r4 = jump_indx
	; r6 = curr->Cr + ...
	; r7 = prev->Cr + ...
	; r8 = cpels-8
	; r10= prev->Cb - prev->Cr
	; r12= pred_error->Cr
	; r14= curr->Cb - curr->Cr
	LDR	PC,[PC,r4,LSL#2]
	MOV	PC,#0
	DCD	dpc_p_off00
	DCD	dpc_p_off10
	DCD	dpc_p_off01
	DCD	dpc_p_off11
dpc_p_off00
	; xh = 0, yh = 0
	; scratch r1,r2,r4,r5
	; r0 = 8 (y counter)
	; r3 = lx
	; r6 = curr->Cr + ...
	; r7 = prev->Cr + ...
	; r8 = cpels-8
	; r10= prev->Cb - prev->Cr
	; r12= pred_error->Cr
	; r14= curr->Cb - curr->Cr
	SUB	r3,r3,#8
dpc_p_lp0y
	MOV	r1,#8
dpc_p_lp0			; 	USED	FREE
	LDRB	r4,[r7,r10]	; Cb	r4
	LDRB	r2,[r6,r14]	;	r2
	LDRB	r5,[r7],#1	; Cr	r5
	SUB	r4,r2,r4	;	r4	r2
	LDRB	r2,[r6],#1	;	r2
	STR	r4,[r12,#64*4]	;		r4
	SUB	r2,r2,r5	;	r2	r5
	STR	r2,[r12],#4	;		r2
	SUBS	r1,r1,#1
	BNE	dpc_p_lp0

	ADD	r7,r7,r3
	ADD	r6,r6,r8
	SUBS	r0,r0,#1
	BNE	dpc_p_lp0y
testexit
        Return  "r4-r11"
dpc_p_off10
	; xh = 1, yh = 0
	; scratch r1,r2,r4,r5,r11
	; r0 = 8 (y counter)
	; r3 = lx
	; r6 = curr->Cr + ...
	; r7 = prev->Cr + ...
	; r8 = cpels-8
	; r10= prev->Cb - prev->Cr
	; r12= pred_error->Cr
	; r14= curr->Cb - curr->Cr
	SUBS	r3,r3,#8
dpc_p_lp1y
	; C flag now set!
	MOV	r1,#8		;	USED	FREE
	LDRB	r11,[r7,r10]	; pCb0	r11
	LDRB	r4,[r7]		; pCr0	r4
dpc_p_lp1
	LDRB	r9,[r7,#1]!	; pCr1	r9
	LDRB	r5,[r7,r10]	; pCb1	r5
	ADC	r4,r9,r4	; pCr	r4	(NOT r9)
	ADC	r11,r11,r5	; pCb	r11	(NOT r5)
	LDRB	r2,[r6,r14]	; cCb	r2
	SUB	r11,r2,r11,ASR#1; Cb	r11	r2
	LDRB	r2,[r6],#1	; cCr	r2
	STR	r11,[r12,#64*4]	;	r11
	SUB	r2,r2,r4,ASR#1	; Cr	r2	r4
	STR	r2,[r12],#4	;	r2
	MOV	r11,r5
	MOV	r4,r9
	SUBS	r1,r1,#1
	BNE	dpc_p_lp1

	ADD	r7,r7,r3
	ADD	r6,r6,r8
	SUBS	r0,r0,#1
	BNE	dpc_p_lp1y

        Return  "r4-r11"
dpc_p_off01
	; xh = 0, yh = 1
	; scratch r1,r2,r4,r5,r11
	; r0 = 8 (y counter)
	; r3 = lx
	; r6 = curr->Cr + ...
	; r7 = prev->Cr + ...
	; r8 = cpels-8
	; r10= prev->Cb - prev->Cr
	; r12= pred_error->Cr
	; r14= curr->Cb - curr->Cr
	ADD	r10,r10,r7
	CMP	r0,#0
	; C flag set!
dpc_p_lp2y
	MOV	r1,#8
dpc_p_lp2			;	USED	FREE
	LDRB	r5,[r10,r3]	; pCb0	r5
	LDRB	r9,[r10],#1	; pCb1	r9
	LDRB	r4,[r7,r3]	; pCr0	r4
	LDRB	r2,[r7],#1	; pCr1	r2
	ADC	r5,r9,r5	; pCb	r5	r9
	LDRB	r9,[r6,r14]	; cCb	r9
	ADC	r4,r2,r4	; pCr	r4	r2
	SUB	r2,r9,r5,ASR#1	; Cb	r2	r9,r5
	LDRB	r5,[r6],#1	; cCr	r5
	STR	r2,[r12,#64*4]	;		r2
	SUB	r5,r5,r4,ASR#1	; Cr	r5	r4
	STR	r5,[r12],#4	;		r5
	SUBS	r1,r1,#1
	BNE	dpc_p_lp2

	SUB	r7,r7,#8
	ADD	r7,r7,r3
	SUB	r10,r10,#8
	ADD	r10,r10,r3
	ADD	r6,r6,r8
	SUBS	r0,r0,#1
	BNE	dpc_p_lp2y

        Return  "r4-r11"
dpc_p_off11
	; xh = 1, yh = 1
	; scratch r1,r2,r4,r5,r9,r11
	; r0 = 8 (y counter)
	; r3 = lx
	; r6 = curr->Cr + ...
	; r7 = prev->Cr + ...
	; r8 = cpels-8
	; r10= prev->Cb - prev->Cr
	; r12= pred_error->Cr
	; r14= curr->Cb - curr->Cr
	ADD	r10,r10,r7
	CMP	r0,#0
	; C flag set!
dpc_p_lp3y
	MOV	r1,#8		;	USED	FREE
	LDRB	r11,[r10,r3]	;pCb01	r11
	LDRB	r5,[r10]	;pCb00	r5
	ADC	r11,r11,r5	;pCb0	r11	r5
	LDRB	r5,[r7,r3]	;pCr01	r5
	LDRB	r4,[r7]		;pCr00	r4
	ADC	r5,r5,r4	;pCr0	r5	r4
dpc_p_lp3
	LDRB	r4,[r10,#1]!	; pCb10	r4
	LDRB	r9,[r10,r3]	; pCb11	r9
	LDRB	r2,[r7,#1]!	; pCr10	r2
	ADC	r4,r4,r9	; pCb1	r4	r9
	LDRB	r9,[r7,r3]	; pCr11	r9
	ADD	r11,r11,r4	; pCb	r11	(NOT r4)
	ADC	r2,r2,r9	; pCr1	r2	r9
	LDRB	r9,[r6,r14]	; cCb	r9
	ADD	r5,r5,r2	; pCr	r5	(NOT r2)
	SUB	r9,r9,r11,ASR#2	; Cb	r9	r5
	LDRB	r11,[r6],#1	; cCr
	STR	r9,[r12,#64*4]
	SUB	r11,r11,r5,ASR#2; Cr
	STR	r11,[r12],#4
	MOV	r11,r4
	MOV	r5,r2
	SUBS	r1,r1,#1
	BNE	dpc_p_lp3

	SUB	r7,r7,#8
	ADD	r7,r7,r3
	SUB	r10,r10,#8
	ADD	r10,r10,r3
	ADD	r6,r6,r8
	SUBS	r0,r0,#1
	BNE	dpc_p_lp3y

        Return  "r4-r11"
	[ 0 = 1
CalcError
	; r0 = MB_Structure *pred_error
	; r1 = unsigned      char[16][16]
	; r2 = int           pred[16][16]
        FunctionEntry "r4-r10"
	MOV	r14,#255

	MOV	r10,#256
	LDR	r8,[r1],#4
	LDMIA	r2!,{r4,r5,r6,r7}
	B 	ce_lpenter
ce_lp
	LDR	r8,[r1],#4
	LDMIA	r2!,{r4,r5,r6,r7}
	STMIA	r0!,{r3,r12}
ce_lpenter
	AND	r9,r14,r8
	RSB	r3,r4,r9
	AND	r9,r14,r8,LSR#8
	RSB	r12,r5,r9
	STMIA	r0!,{r3,r12}		; With luck these will merge with above
	AND	r9,r14,r8,LSR#16
	RSB	r3,r6,r9
	RSB	r12,r7,r8,LSR#24
	SUBS	r10,r10,#4
	BNE	ce_lp
	LDMFD	r13!,{r4-r10,r14}
	STMIA	r0!,{r3,r12}
        Return  , LinkNotStacked
	]
ReconImage
	; r0 = i
	; r1 = j
	; r2 = data
	; r3 = recon
        FunctionEntry "r4-r8"

	LDR	r12,pels
	MOV	r7,#16
	MOV	r4,r12,ASR#1
	MLA	r4,r1,r4,r0	; r4 = i + cpels*j
	MLA	r0,r1,r12,r0	; r0 = i + pels*j

	LDMIA	r3,{r1,r3,r14}	; r1 = recon->lum
				; r3 = recon->Cr
				; r14= recon->Cb
	ADD	r1,r1,r0,LSL#4
	ADD	r3,r3,r4,LSL#3
	ADD	r14,r14,r4,LSL#3

	SUBS	r12,r12,#12
ri_lp1
	LDMIA	r2!,{r0,r4,r5,r8}
	ORR	r6,r0,r4,LSL#8
	ORR	r6,r6,r5,LSL#16
	ORR	r6,r6,r8,LSL#24
	LDMIA	r2!,{r0,r4,r5,r8}
	STR	r6,[r1],#4
	ORR	r6,r0,r4,LSL#8
	ORR	r6,r6,r5,LSL#16
	ORR	r6,r6,r8,LSL#24
	LDMIA	r2!,{r0,r4,r5,r8}
	STR	r6,[r1],#4
	ORR	r6,r0,r4,LSL#8
	ORR	r6,r6,r5,LSL#16
	ORR	r6,r6,r8,LSL#24
	LDMIA	r2!,{r0,r4,r5,r8}
	STR	r6,[r1],#4
	ORR	r6,r0,r4,LSL#8
	ORR	r6,r6,r5,LSL#16
	ORR	r6,r6,r8,LSL#24
	STR	r6,[r1],r12
	SUBS	r7,r7,#1
	BGT	ri_lp1

	MOV	r12,r12,ASR#1
	MOV	r7,#8
	ADD	r12,r12,#2
ri_lp2
	LDMIA	r2!,{r0,r4,r5,r8}
	ORR	r6,r0,r4,LSL#8
	ORR	r6,r6,r5,LSL#16
	ORR	r6,r6,r8,LSL#24
	LDMIA	r2!,{r0,r4,r5,r8}
	STR	r6,[r3],#4
	ORR	r6,r0,r4,LSL#8
	ORR	r6,r6,r5,LSL#16
	ORR	r6,r6,r8,LSL#24
	STR	r6,[r3],r12
	SUBS	r7,r7,#1
	BGT	ri_lp2

	MOV	r7,#8
ri_lp3
	LDMIA	r2!,{r0,r4,r5,r8}
	ORR	r6,r0,r4,LSL#8
	ORR	r6,r6,r5,LSL#16
	ORR	r6,r6,r8,LSL#24
	LDMIA	r2!,{r0,r4,r5,r8}
	STR	r6,[r14],#4
	ORR	r6,r0,r4,LSL#8
	ORR	r6,r6,r5,LSL#16
	ORR	r6,r6,r8,LSL#24
	STR	r6,[r14],r12
	SUBS	r7,r7,#1
	BGT	ri_lp3

        Return  "r4-r8"
InterpolateImage
	; r0 = unsigned char *image
	; r1 = int width
	; r2 = int height
	; r3 = unsigned char *ipol_image
        FunctionEntry "r4-r11"

	MOV	r14,#255
	SUB	r0,r0,#4
	SUB	r2,r2,#1
ii_lp2
	LDR	r4,[r0,#4]!		; r4 = s3s2s1s0 = d6d4d2d0
	LDR	r5,[r0,r1]		; r5 = S3S2S1S0 = D6D4D2D0
	AND	r10,r14,r4		; r10= s0
	SUBS	r12,r1,#4
	; C flag set
ii_lp1
	LDR	r6,[r0,#4]!
	AND	r9,r14,r4,LSR#8		; r9 = s1, d2
	ORR	r8,r10,r9,LSL#16	; r8 = d200d0
	ADC	r11,r10,r9		; r11= s0 + s1 + 1 = d1<<1
	MOV	r11,r11,LSR#1		; r11= d1
	ORR	r8,r8,r11,LSL#8		; r8 = d2d1d0
	AND	r11,r14,r4,LSR#16	; r11= s2, d4
	ADC	r7,r11,r9		; r7 = s1 + s2 + 1 = d3<<1
	MOV	r7,r7,LSR#1		; r7 = d3
	ORR	r8,r8,r7,LSL#24		; r8 = d3d2d1d0
	STR	r8,[r3],#4

	ADC	r7,r11,r4,LSR#24	; r7 = d4 + d6 + 1 = d5<<1
	MOV	r7,r7,LSR#1
	ORR	r11,r11,r7,LSL#8	; r11= d5d4
	MOV	r7,r4,LSR#24		; r7 = s3, d6
	ORR	r11,r11,r7,LSL#16	; r11= d6d5d4
	AND	r8,r14,r6		; r8 = s4
	ADC	r7,r7,r8		; r7 = d8 + d6 + 1 = d7<<1
	MOV	r7,r7,LSR#1
	ORR	r11,r11,r7,LSL#24	; r7 = d7d6d5d4
	LDR	r7,[r0,r1]
	STR	r11,[r3],r1,LSL#1
	; r4 = s3s2s1s0 = d6d4d2d0
	; r5 = S3S2S1S0 = D6D4D2D0
	; r7 = S3S2S1S0
	; r8 = s4 (unused)
	; r9 = s1
	; r10= s0
	AND	r11,r14,r5		; r11= S0
	ADC	r10,r11,r10		; r10= s0+S0+1 = D0<<1
	AND	r11,r14,r5,LSR#8	; r8 = S1
	ADC	r9,r9,r11		; r9 = s1 + S1 + 1 = D2<<1
	MOV	r11,r10,LSR#1		; r11= D0
	ADD	r10,r10,r9		; r10= s0 + s1 + S0 + S1 + 2 = D1<<2
	MOV	r10,r10,LSR#2		; r10= D1
	ORR	r11,r11,r10,LSL#8	; r11= D1D0
	MOV	r10,r9,LSR#1		; r10= D2
	ORR	r11,r11,r10,LSL#16	; r11= D2D1D0
	AND	r10,r14,r5,LSR#16	; r10= S2
	AND	r8,r14,r4,LSR#16	; r8 = s2
	ADC	r8,r8,r10		; r8 = s2 + S2 + 1 = D4<<1
	ADD	r9,r9,r8		; r10= s1 + s2 + S1 + S2 + 2 = D3<<2
	MOV	r9,r9,LSR#2		; r10= D3
	ORR	r11,r11,r9,LSL#24	; r11= D3D2D1D0
	STR	r11,[r3,#-4]

	MOV	r11,r8,LSR#1		; r11= D4
	MOV	r5,r5,LSR#24		; r5 = S3
	ADC	r5,r5,r4,LSR#24		; r5 = S3 + s3 + 1 = D6<<1
	MOV	r9,r5,LSR#1		; r8 = D6
	ADD	r8,r8,r5		; r9 = S2 + s2 + S3 + s3 + 2 = D5<<2
	MOV	r8,r8,LSR#2
	ORR	r11,r11,r9,LSL#16	; r11= D600D4
	ORR	r11,r11,r8,LSL#8	; r11= D6D5D4
	AND	r9,r14,r7		; r8 = S4
	AND	r10,r14,r6		; r10= s4
	ADC	r9,r9,r10		; r8 = S4 + s4 + 1 = D8<<1
	ADD	r9,r9,r5		; r8 = S4 + s4 + S3 + s3 + 2 = D7<<2
	MOV	r9,r9,LSR#2
	ORR	r11,r11,r9,LSL#24
	STR	r11,[r3],#4
	SUB	r3,r3,r1,LSL#1

	MOV	r4,r6
	MOV	r5,r7
	SUBS	r12,r12,#4
	BGT	ii_lp1
	; Now the last 2 pixels are a special case, so do the next 6 as
	; usual (but outside the loop), then fiddle the last two.
	AND	r9,r14,r4,LSR#8		; r9 = s1, d2
	ORR	r8,r10,r9,LSL#16	; r8 = d200d0
	ADC	r11,r10,r9		; r11= s0 + s1 + 1 = d1<<1
	MOV	r11,r11,LSR#1		; r11= d1
	ORR	r8,r8,r11,LSL#8		; r8 = d2d1d0
	AND	r11,r14,r4,LSR#16	; r11= s2, d4
	ADC	r7,r11,r9		; r7 = s1 + s2 + 1 = d3<<1
	MOV	r7,r7,LSR#1		; r7 = d3
	ORR	r8,r8,r7,LSL#24		; r8 = d3d2d1d0
	STR	r8,[r3],#4

	ADC	r7,r11,r4,LSR#24	; r7 = d4 + d6 + 1 = d5<<1
	MOV	r7,r7,LSR#1
	ORR	r11,r11,r7,LSL#8	; r11= d5d4
	MOV	r7,r4,LSR#24		; r7 = s3, d6
	ORR	r11,r11,r7,LSL#16	; r11= d6d5d4
	ORR	r11,r11,r7,LSL#24	; r11= d6d6d5d4
	STR	r11,[r3],r1,LSL#1
	; r4 = s3s2s1s0 = d6d4d2d0
	; r5 = S3S2S1S0 = D6D4D2D0
	; r7 = S3S2S1S0
	; r8 = s4 (unused)
	; r9 = s1
	; r10= s0
	AND	r11,r14,r5		; r11= S0
	ADC	r10,r11,r10		; r10= s0+S0+1 = D0<<1
	AND	r11,r14,r5,LSR#8	; r8 = S1
	ADC	r9,r9,r11		; r9 = s1 + S1 + 1 = D2<<1
	MOV	r11,r10,LSR#1		; r11= D0
	ADD	r10,r10,r9		; r10= s0 + s1 + S0 + S1 + 2 = D1<<2
	MOV	r10,r10,LSR#2		; r10= D1
	ORR	r11,r11,r10,LSL#8	; r11= D1D0
	MOV	r10,r9,LSR#1		; r10= D2
	ORR	r11,r11,r10,LSL#16	; r11= D2D1D0
	AND	r10,r14,r5,LSR#16	; r10= S2
	AND	r8,r14,r4,LSR#16	; r8 = s2
	ADC	r8,r8,r10		; r8 = s2 + S2 + 1 = D4<<1
	ADD	r9,r9,r8		; r10= s1 + s2 + S1 + S2 + 2 = D3<<2
	MOV	r9,r9,LSR#2		; r10= D3
	ORR	r11,r11,r9,LSL#24	; r11= D3D2D1D0
	STR	r11,[r3,#-4]

	MOV	r11,r8,LSR#1		; r11= D4
	MOV	r5,r5,LSR#24		; r5 = S3
	ADC	r5,r5,r4,LSR#24		; r5 = S3 + s3 + 1 = D6<<1
	MOV	r9,r5,LSR#1		; r9 = D6
	ADD	r8,r8,r5		; r8 = S2 + s2 + S3 + s3 + 2 = D5<<2
	MOV	r8,r8,LSR#2
	ORR	r11,r11,r9,LSL#16	; r11= D600D4
	ORR	r11,r11,r8,LSL#8	; r11= D6D5D4
	ORR	r11,r11,r9,LSL#24	; r11= D6D6D5D4
	STR	r11,[r3],#4

	SUBS	r2,r2,#1
	BGT	ii_lp2
	;Now the last 2 lines...
	MOV	r2,#2
ii_lp4
	LDR	r4,[r0,#4]!		; r4 = s3s2s1s0 = d6d4d2d0
	AND	r10,r14,r4		; r10= s0
	SUBS	r12,r1,#4
	; C flag set
ii_lp3
	LDR	r6,[r0,#4]!
	AND	r9,r14,r4,LSR#8		; r9 = s1, d2
	ORR	r8,r10,r9,LSL#16	; r8 = d200d0
	ADC	r11,r10,r9		; r11= s0 + s1 + 1 = d1<<1
	MOV	r11,r11,LSR#1		; r11= d1
	ORR	r8,r8,r11,LSL#8		; r8 = d2d1d0
	AND	r11,r14,r4,LSR#16	; r11= s2, d4
	ADC	r7,r11,r9		; r7 = s1 + s2 + 1 = d3<<1
	MOV	r7,r7,LSR#1		; r7 = d3
	ORR	r8,r8,r7,LSL#24		; r8 = d3d2d1d0
	STR	r8,[r3],#4

	ADC	r7,r11,r4,LSR#24	; r7 = d4 + d6 + 1 = d5<<1
	MOV	r7,r7,LSR#1
	ORR	r11,r11,r7,LSL#8	; r11= d5d4
	MOV	r7,r4,LSR#24		; r7 = s3, d6
	ORR	r11,r11,r7,LSL#16	; r11= d6d5d4
	AND	r10,r14,r6		; r10= s4
	ADC	r7,r7,r10		; r7 = d8 + d6 + 1 = d7<<1
	MOV	r7,r7,LSR#1
	ORR	r11,r11,r7,LSL#24	; r7 = d7d6d5d4
	STR	r11,[r3],#4
	MOV	r4,r6
	MOV	r5,r7

	SUBS	r12,r12,#4
	BGT	ii_lp3
	; Now the last 2 pixels are a special case, so do the next 6 as
	; usual (but outside the loop), then fiddle the last two.
	AND	r9,r14,r4,LSR#8		; r9 = s1, d2
	ORR	r8,r10,r9,LSL#16	; r8 = d200d0
	ADC	r11,r10,r9		; r11= s0 + s1 + 1 = d1<<1
	MOV	r11,r11,LSR#1		; r11= d1
	ORR	r8,r8,r11,LSL#8		; r8 = d2d1d0
	AND	r11,r14,r4,LSR#16	; r11= s2, d4
	ADC	r7,r11,r9		; r7 = s1 + s2 + 1 = d3<<1
	MOV	r7,r7,LSR#1		; r7 = d3
	ORR	r8,r8,r7,LSL#24		; r8 = d3d2d1d0
	STR	r8,[r3],#4

	ADC	r7,r11,r4,LSR#24	; r7 = d4 + d6 + 1 = d5<<1
	MOV	r7,r7,LSR#1
	ORR	r11,r11,r7,LSL#8	; r11= d5d4
	MOV	r7,r4,LSR#24		; r7 = s3, d6
	ORR	r11,r11,r7,LSL#16	; r11= d6d5d4
	ORR	r11,r11,r7,LSL#24	; r11= d6d6d5d4
	STR	r11,[r3],#4

	SUBS	r2,r2,#1
	BGT	ii_lp4

        Return  "r4-r11"
	[ 0 = 1
BoundedDiv
	; r0 = Thing to divide
	; r1 = Value to divide by
	MOV	r2,#0

	SUBS	r0,r0,r1,LSL#16
	ADDLT	r0,r0,r1,LSL#16
	ADDGE	r2,r2,#1<<16

	SUBS	r0,r0,r1,LSL#15
	ADDLT	r0,r0,r1,LSL#15
	ADDGE	r2,r2,#1<<15

	SUBS	r0,r0,r1,LSL#14
	ADDLT	r0,r0,r1,LSL#14
	ADDGE	r2,r2,#1<<14

	SUBS	r0,r0,r1,LSL#13
	ADDLT	r0,r0,r1,LSL#13
	ADDGE	r2,r2,#1<<13

	SUBS	r0,r0,r1,LSL#12
	ADDLT	r0,r0,r1,LSL#12
	ADDGE	r2,r2,#1<<12

	SUBS	r0,r0,r1,LSL#11
	ADDLT	r0,r0,r1,LSL#11
	ADDGE	r2,r2,#1<<11

	SUBS	r0,r0,r1,LSL#10
	ADDLT	r0,r0,r1,LSL#10
	ADDGE	r2,r2,#1<<10

	SUBS	r0,r0,r1,LSL#9
	ADDLT	r0,r0,r1,LSL#9
	ADDGE	r2,r2,#1<<9

	SUBS	r0,r0,r1,LSL#8
	ADDLT	r0,r0,r1,LSL#8
	ADDGE	r2,r2,#1<<8

	SUBS	r0,r0,r1,LSL#7
	ADDLT	r0,r0,r1,LSL#7
	ADDGE	r2,r2,#1<<7

	SUBS	r0,r0,r1,LSL#6
	ADDLT	r0,r0,r1,LSL#6
	ADDGE	r2,r2,#1<<6

	SUBS	r0,r0,r1,LSL#5
	ADDLT	r0,r0,r1,LSL#5
	ADDGE	r2,r2,#1<<5

	SUBS	r0,r0,r1,LSL#4
	ADDLT	r0,r0,r1,LSL#4
	ADDGE	r2,r2,#1<<4

	SUBS	r0,r0,r1,LSL#3
	ADDLT	r0,r0,r1,LSL#3
	ADDGE	r2,r2,#1<<3

	SUBS	r0,r0,r1,LSL#2
	ADDLT	r0,r0,r1,LSL#2
	ADDGE	r2,r2,#1<<2

	SUBS	r0,r0,r1,LSL#1
	ADDLT	r0,r0,r1,LSL#1
	ADDGE	r2,r2,#1<<1

	SUBS	r0,r0,r1
	;ADDLT	r0,r0,r1
	ADDGE	r2,r2,#1

	MOV	r0,r2
        Return  , LinkNotStacked
	]

	END
