Thoughts:

(1) tension between average size (spread over all codes) and good performance
in worst case situations (mimise the codes in these cases). This applies to
both the codes for motion and to those for block definition.

(2) motion search area doesn't need to be square. Indeed, it doesn't even need
to be rectangular, though the efficiency of coding the search algorithm at
compression time would suffer.

(3) adapting the q factor to get the chunk size right need not be done on a per
frame basis: it could be done on a block basis or even a sub-block basis (e.g.
if the frame is 'just too big' then change the q on sub-blocks where some of
the other members match).

(4) quality adaption works better if done slowly - all I care about is getting
the size of the chunk right, not of the frame. But if I look only every n
frames (with some problem if n doesn't divide fpc) then I'll get a lot of
backtracking if it didn't match - particularly on the 'sequence of stills'
world. MovingLines' adaption rate (per frame) doesn't produce good results
since it lowers the quality too early (bytes, bytes+1, bytes-1) on marginal
increases in size. On the other hand, it does increase the quality as soon as
possible (subject to the size heuristic for speeding up compression). [you
really need to code a sequence of frames at all qualities (culling out the
impossible ones as soon as you can predict that its definitely over budget)
with the successor frames tried at all qualities for each of the initial
frames (try f at 10 q levels, for each result, try f+1 at each q level...).
Now this is HARD: could reduce to 3 q levels (current, up and down) each
step]





