/* Copyright (c) 1996 by Oracle Corporation.  All Rights Reserved.
 *
 * ysfo.c - Fixed-Object Memory Manager
 */

#ifndef SYSI_ORACLE
#include <sysi.h>
#endif
#ifndef YSFO_ORACLE
#include <ysfo.h>
#endif

/*
 * YSFOPFX - prefix allocation size
 */
#define YSFOPFX  max(SYSM_STRICT_ALIGN, sizeof(dvoid *))

/*
 * ysFOCreate - create a fixed-object pool
 */
void ysFOCreate(ysfop *fop, size_t osz, ysmhp *hp, CONST char *tag)
{
  /* the space used for objects is also used to hold pointers when the
   * object is on the free chain.  Therefore, not only must the size be
   * large enough to hold a pointer, but it also must be suitably aligned.
   */
  osz = (size_t) ysRoundVal((ub4) osz, sizeof(dvoid *));
  if (osz == 0 || osz > 4096)
    yseThrow(YS_EX_BADPARAM);

  /* initialize the descriptor */
  fop->chn = (dvoid *) 0;
  fop->stash = (ub1 *) 0;
  fop->scnt = (sword) 0;
  fop->ccnt = (sword) 0;

  fop->osz = osz;
  fop->hp = hp;
  fop->tag = tag;

  /* our initial chunk size is small; we will grow this exponentially for
   * each subsequent chunk up to a maximum of 64 chunks at a time depending
   * on the object size (object sizes 128 bytes or smaller are subject to
   * this limit; otherwise, they experience our 16k allocation limit).
   * The initial chunk size is shifted by 1 because ysFOAlloc() shifts
   * before allocating.
   */
  fop->csz = (ub4) 2;               /* our initial chunk size (shifted by 1) */
  fop->mcsz = ((ub4) 8192) / osz;
  if (fop->mcsz > (ub4) 64)
    fop->mcsz = (ub4) 64;
}

/*
 * ysFODestroy - destroy a fixed object pool
 */
void ysFODestroy(ysfop *fop)
{
  ub1 *oldstash;

  /* if we never allocated, then do nothing */
  if (!fop->stash)
    return;

  /* go the beginning of the current stash */
  fop->stash -= (fop->osz * fop->scnt);

  do
    {
      /* go back over our prefix and retrieve the back pointer */
      fop->stash -= YSFOPFX;
      oldstash = *((ub1 **) fop->stash);

      /* free the current buffer */
      ysmFree(fop->hp, (dvoid *) fop->stash);

      /* continue backwards */
      fop->stash = oldstash;
    } while (fop->stash);
}

/*
 * ysFOAlloc - allocate an object
 */
dvoid *ysFOAlloc(ysfop *fop)
{
  dvoid *ptr;
  ub1   *oldstash;
  size_t amt;

  if (fop->chn)
    {
      /* we have something on the free list chain; simply pop it off */
      ptr = fop->chn;
      fop->chn = *((dvoid **) fop->chn);
      return ptr;
    }
  else if (!fop->scnt)
    {
      /* our chain is empty, and our stash is empty; we need to try to
       * allocate a new stash
       */
      oldstash = fop->stash;

      /* we adjust our chunk size exponentially to get more and more
       * elements at one time.  we do this based on the following belief:
       * that small lists should not waste excessive memory by allocating
       * in large chunk sizes, and that large lists should not penalized
       * for small chunk sizes
       */
      if (fop->csz < fop->mcsz)
	fop->csz <<= 1;

      /* compute space to allocate */
      amt = YSFOPFX + (size_t) (fop->osz * fop->csz);

      fop->stash = (ub1 *) ysmAlloc(fop->hp, amt, fop->tag);

      if (!fop->stash)
	{
	  /* we failed; restore the pool and return the error */
	  fop->stash = oldstash;
	  return (dvoid *) 0;
	}
      else
	{
	  /* the oldstash points to the beginning of the currently allocated
	   * buffer; we stick this in the beginning of our newly allocated
	   * buffer, thus creating a chain of the buffers for when we need
	   * to free them
	   */
	  *((ub1 **) fop->stash) = oldstash;

	  /* we start our stash pointer at the end and work backwards */
	  fop->stash += amt;
	  fop->scnt = (sword) fop->csz;
	}
    }

  /* if we made it here, we have free objects in the stash; allocate one */
  fop->scnt--;
  fop->stash -= fop->osz;
  return (dvoid *) fop->stash;
}
