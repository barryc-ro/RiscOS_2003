/******	IRQ.c **************************************************************

Project:	STB-400
Component:	MPEGDriver
This file:	Interrupt handling

Copyright 1999-2000 Pace Micro Technology plc. All rights reserved.

This material is the confidential trade secret and proprietary information
of Pace Micro Technology plc. It may not be reproduced, used, sold, or
transferred to any third party without the prior written consent of
Pace Micro Technology plc.

History:
Date		Who	Change
----------------------------------------------------------------------------
06/10/1999	BJGA	Created
07/10/1999	BJGA	Implemented IRQ_EnableCommonIRQs, IRQ_DisableCommonIRQs
11/10/1999	BJGA	Changed to use nINT4; implemented IRQ dispatch
20/10/1999	BJGA	Implemented video header parser coroutine
26/10/1999	BJGA	Uses new STB-400 interrupt allocation symbols;
			items renamed as per the reorganisation of Types.h;
			picture list implemented (added NewPictureDecode());
			reconstruction frame pointers set up for P- and B-pictures;
			sequence and GOP header items persist when necessary
09/11/1999	BJGA	Video decode now works as far as the issuing of SWI
			MPEGControl_Play at the end of video prefill. Too many
			details to list...
11/11/1999	BJGA	Prefill picture skips now use PID rather than VSync to
			trigger restart - this is possible with type 3 skips, and
			increases the commonality with prefill picture decodes;
			removal of frame struct in prefill skips moved from BBF
			to PID - this allows testing of picture type in PID
			independently of any state variable; delayed issuing of
			MPEGControl_Play until the headers of the first
			non-prefill picture have been decoded; moved slice header
			actions into separate function
12/11/1999	BJGA	Simplified video prefill state machine; added support
			for field pictures (including the consequences of the
			necessary changes to Types.h)
17/11/199	BJGA	Implemented normal-play moving video: this involved
			increasing the number of frame structs to 5 and
			dyncmically allocating frame buffers to them,
			implememting a frame display list, and adding to the
			functionality of INT_PID, INT_VS and INT_PSD;
			worked around a 3520 bug that required VID_FFP to be
			written before VID_BFP; separated WriteDecodeInstruction
			and RemoveFrameFromList into separate functions
23/11/1999	BJGA	Redesigned state machine for greater commonality between
			video prefill and normal play, also introducing stalling
			during video play to prevent buffer underrun; introduced
			error, skip/repeat, stall, underrun counting; simple
			horizontal scaling calculation added; field display
			control support started
25/11/1999	BJGA	Field display control implemented; fixed bug in display
			of certain clips by changing frame buffer allocation
			algorithm; attempt to support field-picture video
			(thwarted by a bug in the 3520)
30/11/1999	BJGA	Now decodes field pictures 1 field ahead of display
			(frame pictures retain 1 frame delay to avoid timing
			issues when downscaling) - works around 3520 bug.
			Has a surprisingly large number of side-effects (eg
			start-code hit now has to be re-enabled on PSD interrupt)
01/12/1999	BJGA	Can now handle transitions between frame pictures and
			field pictures (either way) in the same clip, by either
			inserting a field delay into decoding, or by squeezing
			a frame decode into one field interval; also has more
			robust header parsing, to deal with stream corruption
			within a single set of headers
02/12/1999	BJGA	Only starts a new frame struct if the previous headers
			have finished parsing (more resilient against bitstream
			errors); resets pipeline after a while, if it fails to
			return to idle state, or if no slice header is parsed
09/12/1999	BJGA	Different values of XDO/XDS needed for PAL and NTSC
25/01/2000	BJGA	Unused f_codes initialised to 0xF instead of 0xFF;
			inverted sense of VID_HDS.QMI to suit hardware
			rather than data sheet - fixes video blocking bug;
			implemented audio start/stop; enabled VSyncs between
			waiting_for_play_command and waiting_for_presentation_start;
			adjusted VID_XDO and VID_XDS as per new hardware timing scheme
31/01/2000	BJGA	Split video and audio IRQ handlers; implemented audio header
			parsing, in order to set sample frequencies correctly;
			adjusted VID_XDO and VID_XDS back again
09/02/2000	BJGA	Mutes for one audio frame across sampling frequency changes
			(masks nasty audio glitch); added support for dual mono audio;
			implemented IRQ_UpdateMuteState() to manage the various things
			that need to mute audio; rewrote code that assigns video frame
			buffers so that two of them are only ever used for B-frames
			(fix for 3520 bug) even though the possibility of us needing
			three buffers simultaneously for anchor frames means that we
			now need five frame buffers in total!!
17/02/2000	BJGA	Counts audio CRC errors and PCM output buffer underflows;
			for added resilience to bitstream errors, any pictures with
			illegal picture_coding_types, and all pictures immediately
			following a GOP header, are interpreted as I-pictures;
			picture_coding_type is set to I-picture before initial sequence
			header search, to prevent corruption of the 3520 B-frame
			pointers during the now unblanked ResetStream interval;
			implemented deferred SetSpeed calls
22/02/2000	BJGA	Implemented trickle-decoding of audio data during slow-motion play;
			only waits for a single prefill anchor frame decode if the
			trick-play ResetStream flag or low_delay bitstream flag is set;
			delays enabling BBE interrupt until prefill end, rather than end of
			initial header parsing
10/04/2000	BJGA	Removed IRQ_AudioFast() and IRQ_AudioSlow() - not used;
			implemented deferred video start when first video PTS is significantly
			later than first audio PTS; implemented clock reference latching,
			relatching (on speed transitions) and comparisons, and triggering and
			performing frame skips and repeats for frame- and field-picture streams;
			implemented PCM clock adjustment on VSyncs, in order to effect audio
			sync-to-stream; removed old deferred set-speed code, as it only worked
			for cases when we moved to a slower speed; issues internal resets when
			decoder is judged to have locked up beyond the ability of pipeline resets
			to recover the situation; video threshold checking is no longer performed
			when playing normal speed video, in order to avoid skip-stall-skip-stall
			patterns; emergency fallback framebuffer pointers corrected (had been a
			factor of 256 out); audio decoding is stopped and audio is muted after an
			underrun, pending reattainment of a minimal audio buffer level; now counts
			audio frame-sync losses; manually searches for start-code prefixes between
			adjacent video headers - works around unreliability of VID_SCM
02/05/2000	BJGA	Implemented video scaling and positioning; accommodates differences between
			PAL and NTSC displays automatically; counts internal resets
12/05/2000	BJGA	Frame-based audio fast/slow code removed (smooth fasts/slows done in Stuffing.c);
			auto-reset tests now slowed down by a factor of video.speed_indicator
			(helps particularly with slow-motion reverse trick-play streams);
			automatic pipeline resets disabled (seemed to be causing problems with trick play
			streams when the new quick-prefill algorithm was in use);
			scaling code no longer tries to use widescreen modes where the electron beam
			doesn't cover the whole tube, except when viewing mode Box is in use;
			video is now correctly positioned when AFD indicates a top-aligned active area;
			values are now cached for use in constructing Stat &1011, and UpCall_MPEG &1002
			is called when they change
18/05/2000	BJGA	Fixed divide-by-zero if a lock-up condition was detected while a speed-change to
			pause was pending; added a few extra checks to avoid division-by-zero in video
			scaling code; uses defined symbols for UpCall_MPEG sub-reason codes
25/05/2000	BJGA	Now correctly recognises AFDs (magic word byte order was wrong); new multi-cycle
			register read macro shaves a few instructions off header data FIFO reading;
			fixed code that chooses TV scaling mode to discard modes that don't cover the whole
			tube if they would mean losing an otherwise visible shoot-and-protect region
			(this was due to considering the protected area rather than the active area);
			now uses interpolation forms of chroma reconstruction where possible (ie where it
			doesn't coincide with bugs in the STi3520L)
16/06/2000	BJGA	Lipsync error now stored in Stream for access from *PacketInfo; does a pipeline
			reset before every decode in trick-play mode for added stability
20/06/2000	BJGA	Video PTSs now obtained from software PTS list, rather from broken hardware PES parser
07/07/2000	BJGA	support for SWIs MPEGVideo_Play 1 and MPEGAudio_Play 1 added (calls MPEGControl back
			when video and/or audio display stops); now catches cases where the MPEG chip locks up
			during prefilling (including prefilling as part of an internal reset) and fires off
			an(other) internal reset; satisfies _swix() type checking of new version of cc;
			blanking the first line of video to hide half-line artefacts was erroneously being
			applied even in non-letterbox modes - now done only in letterbox mode, and uses the
			specific hardware feature for the task instead - also fixes 1-line vertical positioning fault;
			always assumes that the picture after a first-field picture is a second-field picture
			(should make buffer allocation algorithm more robust to bitstream errors);
			removed special-case handling of low_delay streams (reduces frequency of video-tear artefacts);
			widescreen TVs can now be signalled to switch to 16:9 zoom mode as well as anamorphic mode in viewing mode Stretch
09/08/2000	BJGA	Fixed a divide-by-zero in SetUpScalingRegisters due to horizontal_downscaling being < 0x100;
			fixed header parsing to skip slice start codes until at least one of sequence header, gop header or
			picture header had been received, and moved a few critical operations from the picture start code handler
			to the slice start code handler - so now we are robust against the picture start code being lost from the stream
10/07/2000	BJGA	Last version broke MPEG-1 decode due to the code moved from the picture coding extension handler assuming we were
			dealing with an MPEG-2 stream. Also fixed a couple of debug statements to reflect the moves!
14/07/2000	BJGA	Altered display of 2x enlarged interlaced pictures and 4x enlarged progressive pictures to avoid display artefacts;
			takes advantage of fixed MPEG sequence header fields to cope with corruption of sequence header data;
			ignores picture_display_extension unless sequence_display_extension is present
09/08/2000	BJGA	Parses DiviCom and C-Cube format closed caption user data; once per frame decode (not display, ie gets in before
			frame skips) calls caption handlers; added support for 3:2 pulldown (23.976 fps material on NTSC displays)
04/09/2000	BJGA	Doesn't do internal resets when video bitbuffer is empty
15/09/2000	BJGA	Only flags pcm_output_idle on *second* INT_FNP, since unless we've reached this point, we don't seem to get INT_POU,
			and therefore never detect the end of audio decoding activity

***************************************************************************/

/************/
/* Includes */
/************/

#include <stdlib.h>
#include "kernel.h"
#include "swis.h"

#include "Global/DevNos.h"
#include "Global/RISCOS.h"
#include "Global/SWIs.h"
#include "Global/UpCall.h"
#include "Global/IO/GenericIO.h"
#include "Global/IO/IOMD.h"
#include "DebugLib/DebugLib.h"
#include "MPEG/MPEG2cCard.h"
#include "MPEG/MPEGCtrl.h"
#include "MPEG/UpCallMPEG.h"

#include "Clk.h"
#include "Co.h"
#include "Defs.h"
#include "Fixed.h"
#include "IRQ.h"
#include "MiscAsm.h"
#include "ModHdr.h"
#include "Module.h"
#include "Registers.h"
#include "STi3520L.h"
#include "Types.h"
#include "WSS.h"

#include "Debug.h"
#if Debugging==1
#include "DWrite.h"
#include "ReEntDebug.h"
/* The following CLib functions aren't defined in any header files! */
void _memcpy (void *dest, void *source, int n);
void _memset (void *dest, int w, int n);
#endif

/*****************/
/* Private types */
/*****************/

typedef enum
{
  vsf_x0_5,
  vsf_x0_75,
  vsf_x1,
  vsf_x1_5, /* not available for interlaced video */
  vsf_x2,
  vsf_x4,   /* not available for interlaced video */
  vsf_LIMIT
}
vertical_scaling_factor;

typedef enum
{
  tvm_collapsed,  /* 4:3 TVs only (16:9 mode) */
  tvm_full_frame, /* traditional academy ratio */
  tvm_anamorphic, /* 16:9 TVs only */
  tvm_14_9,       /* 16:9 TVs only */
  tvm_16_9,       /* 16:9 TVs only */
  /* below 3x vertical upscaling, 2.35:1 mode only gains us scales very similar to others with better horizontal bandwidth - and 2.35:1 isn't supported on all widescreen TVs anyway */
  tvm_LIMIT
}
tv_mode;

/**********************/
/* Private prototypes */
/**********************/

static void static_INT_SCH (unsigned int *video_ints);
static void static_INT_BBF (unsigned int *video_ints);
static void static_INT_VS (bool field, unsigned int *video_ints);
static void static_INT_PSD (unsigned int *video_ints);
static void static_INT_PID (unsigned int *video_ints);
static void static_INT_HFF (unsigned int *video_ints);
static void static_INT_BBE (void);
static void static_INT_PER (void);
static void static_INT_SER (void);
static void static_INT_PDE (void);
static void static_INT_CSS (void);
static void static_INT_VHR (void);
static void static_INT_VPR (void);
static void static_INT_CRC (void);
static void static_INT_POU (void);
static void static_INT_FNP (void);
static void static_ProcessVideoHdr (void *argument);
static void static_NewPictureDecode (void);
static void static_ProcessVideoSequenceHdr (void);
static void static_ProcessRepeatVideoSequenceHdr (void);
static void static_ProcessVideoSequenceExt (void);
static void static_ProcessVideoSequenceDisplayExt (void);
static void static_ProcessVideoGOPHdr (void);
static void static_ProcessVideoPictureHdr (void);
static void static_ProcessVideoPictureCodingExt (void);
static void static_ProcessVideoQuantMatrixExt (void);
static void static_ProcessVideoPictureDisplayExt (void);
static bool static_ProcessVideoUserData (void);
static void static_ProcessVideoDTG1UserData (void);
static void static_ProcessVideoDiviComUserData (unsigned char dataLength, unsigned char dataType, unsigned char extDataType);
static void static_ProcessVideoSliceHdr (void);
static unsigned int static_GetVideoHdrBits (unsigned char bits);
static void static_PerhapsWriteNewInstruction (void);
static void static_SetUpDisplayForNewAU (void);
static void static_WriteDecodeInstruction (bool FIS);
static void static_WriteSkipInstruction (bool FIS);
static void static_RemoveFrameFromList (frame_t *removed_frame, bool D_list);
static void static_SetNextDisplayField (bool top_field);
static void static_PositionVideo (signed short frame_centre_horizontal_offset, signed short frame_centre_vertical_offset, unsigned char active_format);
static void static_SetUpScalingRegisters (signed int centre_x, signed int centre_y, fixed horizontal_downscaling, vertical_scaling_factor vertical_scaling_index);
static void static_SetDFP (void);
static void static_ProcessCCData (unsigned int handler, unsigned int flags, const char *data);

/********************/
/* Public variables */
/********************/

#if Debugging==1 && DebugProfiling==1
unsigned int IRQ_IRQCount [profileirq_LIMIT];
unsigned int IRQ_IRQTime [profileirq_LIMIT];
#endif

/*********************/
/* Private variables */
/*********************/

static int CommonIRQsClaimedCount = 0;
static int VideoPrefillSize = -1; /* in units of 256 bytes */
static char Deferred_VID_TIS;
static bool Deferred_VID_DCF_LB_Pending;
static char Deferred_VID_DCF_LB;
static int DisplayFrameHackyScanOffset; /* in units of macroblock row pairs */
#if Debugging==1
#if DebugProfiling==1
static unsigned int IRQStartTime;
static unsigned int Working_IRQCount [profileirq_LIMIT];
static unsigned int Working_IRQTime [profileirq_LIMIT];
#endif
#if DebugReentrancy==1
static unsigned int IrqStackDepthOnEntry;
#endif
#if DebugVideoHeaderFIFO==1
static void (*DADebug_WriteC) (char);
#endif
#endif

/*********************/
/* Private constants */
/*********************/

#define PICTURE_START_CODE      0x00
#define SLICE_START_CODE_MIN    0x01
#define SLICE_START_CODE_MAX    0xAF
#define USER_DATA_START_CODE    0xB2
#define SEQUENCE_HEADER_CODE    0xB3
#define SEQUENCE_ERROR_CODE     0xB4
#define EXTENSION_START_CODE    0xB5
#define SEQUENCE_END_CODE       0xB7
#define GROUP_START_CODE        0xB8

#define SEQUENCE_EXTENSION                      0x1
#define SEQUENCE_DISPLAY_EXTENSION              0x2
#define QUANT_MATRIX_EXTENSION                  0x3
#define COPYRIGHT_EXTENSION                     0x4
#define SEQUENCE_SCALABLE_EXTENSION             0x5
#define PICTURE_DISPLAY_EXTENSION               0x7
#define PICTURE_CODING_EXTENSION                0x8
#define PICTURE_SPATIAL_SCALABLE_EXTENSION      0x9
#define PICTURE_TEMPORAL_SCALABLE_EXTENSION     0xA

#define I_PICTURE       0x1
#define P_PICTURE       0x2
#define B_PICTURE       0x3

#define TOP_FIELD_PICTURE       0x1
#define BOTTOM_FIELD_PICTURE    0x2
#define FRAME_PICTURE           0x3

#define ASPECT_4_TO_3           ((fixed) 0x15555)
#define ASPECT_14_TO_9          ((fixed) 0x18E39)
#define ASPECT_16_TO_9          ((fixed) 0x1C71C)
#define ASPECT_2_21_TO_1        ((fixed) 0x235C3)
#define ASPECT_2_35_TO_1        ((fixed) 0x2599A)

static const unsigned int frame_rate_table [16] =
{
  0, 23976, 24000, 25000, 29970, 30000, 50000, 59940, 60000, 0, 0, 0, 0, 0, 0, 0
};

static const fixed mpeg_1_pixel_aspect_ratio_table [16] =
{
  0x10000,
  (fixed) (0x10000 / 1.0000),
  (fixed) (0x10000 / 0.6735),
  (fixed) (0x10000 / 0.7031),
  (fixed) (0x10000 / 0.7615),
  (fixed) (0x10000 / 0.8055),
  (fixed) (0x10000 / 0.8437),
  (fixed) (0x10000 / 0.8935),
  (fixed) (0x10000 / 0.9157),
  (fixed) (0x10000 / 0.9815),
  (fixed) (0x10000 / 1.0255),
  (fixed) (0x10000 / 1.0695),
  (fixed) (0x10000 / 1.0950),
  (fixed) (0x10000 / 1.1575),
  (fixed) (0x10000 / 1.2015),
  0x10000
};

static const struct { fixed active; fixed protect; bool top; } active_formats [16] =
{
  { 0,                0,                false },
  { 0,                0,                false },
  { ASPECT_16_TO_9,   ASPECT_16_TO_9,   true  },
  { ASPECT_14_TO_9,   ASPECT_14_TO_9,   true  },
  { ASPECT_2_35_TO_1, ASPECT_2_35_TO_1, false },
  { 0,                0,                false },
  { 0,                0,                false },
  { 0,                0,                false },
  { 0,                0,                false }, /* official as-the-coded-frame value */
  { ASPECT_4_TO_3,    ASPECT_4_TO_3,    false },
  { ASPECT_16_TO_9,   ASPECT_16_TO_9,   false },
  { ASPECT_14_TO_9,   ASPECT_14_TO_9,   false },
  { ASPECT_4_TO_3,    ASPECT_16_TO_9,   false }, /* not officially defined, but useful for open matte or Super35 movies */
  { ASPECT_4_TO_3,    ASPECT_14_TO_9,   false },
  { ASPECT_16_TO_9,   ASPECT_14_TO_9,   false },
  { ASPECT_16_TO_9,   ASPECT_4_TO_3,    false }
};

static const unsigned char default_intra_quantizer_matrix [64] =
{
  0x08, 0x10, 0x10, 0x13, 0x10, 0x13, 0x16, 0x16,
  0x16, 0x16, 0x16, 0x16, 0x1A, 0x18, 0x1A, 0x1B,
  0x1B, 0x1B, 0x1A, 0x1A, 0x1A, 0x1A, 0x1B, 0x1B,
  0x1B, 0x1D, 0x1D, 0x1D, 0x22, 0x22, 0x22, 0x1D,
  0x1D, 0x1D, 0x1B, 0x1B, 0x1D, 0x1D, 0x20, 0x20,
  0x22, 0x22, 0x25, 0x26, 0x25, 0x23, 0x23, 0x22,
  0x23, 0x26, 0x26, 0x28, 0x28, 0x28, 0x30, 0x30,
  0x2E, 0x2E, 0x38, 0x38, 0x3A, 0x45, 0x45, 0x53
};

static const fixed upscaling_lut [vsf_LIMIT] =
{
  0x08000,
  0x0C000,
  0x10000,
  0x18000,
  0x20000,
  0x40000
};

/*******************************************************/
/* Nasty hacky macros - only active in profiling builds */
/*******************************************************/

#if Debugging==1 && DebugProfiling==1
#define START_IRQ(TLA) IRQStartTime=Clk_GetHiResTime();
#define END_IRQ(TLA) Working_IRQCount[profileirq_##TLA]++; Working_IRQTime[profileirq_##TLA]+=(Clk_GetHiResTime()-IRQStartTime)/2;
#else
#define START_IRQ(TLA)
#define END_IRQ(TLA)
#endif

/********************/
/* Public functions */
/********************/

/******	IRQ_HandleVideoIRQ() ***********************************************

Purpose:	Video IRQ entry point

***************************************************************************/

void IRQ_HandleVideoIRQ (void)
{
  static unsigned int video_ints; /* must be static because the coroutine only takes a copy of the address on entry */
#if Debugging==1 && DebugReentrancy==1
  static unsigned int threaded = 0;
//static int counter=100;
//if (counter-- == 0)
//{
//  ReEntDebug_OutputDebug (1);
//  _swix (OS_CallAVector, _INR(0,2)|_IN(4)|_IN(9), 0, 24, 0xff00, 2, 0x23); /* set border red if re-entered */
//}
  if (threaded > 0)
  {
    int *ptr = *(int **) 0x108;
    unsigned int IrqStackDepthWhenReEntered = 0;
    while (ptr != NULL)
    {
      IrqStackDepthWhenReEntered++;
      ptr = (int *) *ptr;
    }
    ReEntDebug_OutputDebug (IrqStackDepthWhenReEntered - IrqStackDepthOnEntry);
    * (volatile unsigned char *) (IOC + IOCIRQMSKA) &= ~(1<<7);
    _swix (OS_CallAVector, _INR(0,2)|_IN(4)|_IN(9), 0, 24, 0xff00, 2, 0x23); /* set border red if re-entered */
  }
  else
  {
    int *ptr = *(int **) 0x108;
    IrqStackDepthOnEntry = 0;
    while (ptr != NULL)
    {
      IrqStackDepthOnEntry++;
      ptr = (int *) *ptr;
    }
    * (volatile unsigned char *) (IOC + IOCIRQMSKA) |= 1<<7;
  }
  threaded++;
#endif
  do
  {
    video_ints = VID_ITS;
//    dprintf (("IRQ", "IRQ_HandleVideoIRQ: VID_ITS before masking = %06X\n", video_ints));
    video_ints  &= VID_ITM;
    
    if (video_ints != 0)  /* quick reject if no video interrupts */
    {
      /* First deal with interrupt handlers that may need to update          */
      /* |video_ints|. These must clear their own bit in |video_ints| before */
      /* returning, unless they wish to be re-called immediately. Only one   */
      /* interrupt handler should be called for each iteration of the loop.  */
      while ((video_ints & (VIDEO_INT_SCH | VIDEO_INT_BBF | VIDEO_INT_VSB | VIDEO_INT_VST | VIDEO_INT_PSD | VIDEO_INT_PID | VIDEO_INT_HFF)) != 0)
      {
        if (video_ints & VIDEO_INT_SCH)
        {
          START_IRQ(SCH);
          static_INT_SCH (&video_ints);
          END_IRQ(SCH);
        }
        else if (video_ints & VIDEO_INT_BBF)
        {
          START_IRQ(BBF);
          static_INT_BBF (&video_ints);
          END_IRQ(BBF);
        }
        else if (video_ints & VIDEO_INT_VSB)
        {
          START_IRQ(VSB);
          static_INT_VS (false, &video_ints);
          END_IRQ(VSB);
        }
        else if (video_ints & VIDEO_INT_VST)
        {
          START_IRQ(VST);
          static_INT_VS (true, &video_ints);
          END_IRQ(VST);
        }
        else if (video_ints & VIDEO_INT_PSD)
        {
          START_IRQ(PSD);
          static_INT_PSD (&video_ints);
          END_IRQ(PSD);
        }
        else if (video_ints & VIDEO_INT_PID)
        {
          START_IRQ(PID);
          static_INT_PID (&video_ints);
          END_IRQ(PID);
        }
        else if (video_ints & VIDEO_INT_HFF)
        {
          START_IRQ(HFF);
          static_INT_HFF (&video_ints);
          END_IRQ(HFF);
        }
//        dprintf (("IRQ", "IRQ_HandleVideoIRQ: video_ints = %06X\n", video_ints));
      }
      /* Now deal with the other interrupts. */
/*    if (video_ints & VIDEO_INT_BFF) static_INT_BFF ();*/
/*    if (video_ints & VIDEO_INT_HFE) static_INT_HFE ();*/
      if (video_ints & VIDEO_INT_BBE)
      {
        START_IRQ(BBE);
        static_INT_BBE ();
        END_IRQ(BBE);
      }
      if (video_ints & VIDEO_INT_PER)
      {
        START_IRQ(PER);
        static_INT_PER ();
        END_IRQ(PER);
      }
/*    if (video_ints & VIDEO_INT_WFE) static_INT_WFE ();*/
/*    if (video_ints & VIDEO_INT_RFF) static_INT_RFF ();*/
/*    if (video_ints & VIDEO_INT_BMI) static_INT_BMI ();*/
      if (video_ints & VIDEO_INT_SER)
      {
        START_IRQ(SER);
        static_INT_SER ();
        END_IRQ(SER);
      }
      if (video_ints & VIDEO_INT_PDE)
      {
        START_IRQ(PDE);
        static_INT_PDE ();
        END_IRQ(PDE);
      }
/*    if (video_ints & VIDEO_INT_ABE) static_INT_ABE ();*/
/*    if (video_ints & VIDEO_INT_WFN) static_INT_WFN ();*/
/*    if (video_ints & VIDEO_INT_RFN) static_INT_RFN ();*/
/*    if (video_ints & VIDEO_INT_ABF) static_INT_ABF ();*/
/*    if (video_ints & VIDEO_INT_SCR) static_INT_SCR ();*/
/*    if (video_ints & VIDEO_INT_ERR) static_INT_ERR ();*/
    }
  }
  /* Only exit when we're satisfied that we're not about to raise another IRQ. */
  /* This also allows for the clearing of any interrupts that may have been */
  /* triggered, but already serviced within an interrupt handler (having been */
  /* detected by using reads from VID_STA rather than VID_ITS). */
  while (video_ints != 0);
#if Debugging==1 && DebugReentrancy==1
  _kernel_irqs_off(); /* in case they've been enabled! */
  threaded--;
  * (volatile unsigned char *) (IOC + IOCIRQMSKA) &= ~(1<<7);
#endif
}

/******	IRQ_HandleAudioIRQ() ***********************************************

Purpose:	Audio IRQ entry point

***************************************************************************/

void IRQ_HandleAudioIRQ (void)
{
  static unsigned int audio_ints;
  do
  {
    audio_ints = AUD_ITR;
//    dprintf (("IRQ", "IRQ_HandleAudioIRQ: AUD_ITR before masking = %04X\n", audio_ints));
    audio_ints &= AUD_ITM;
    
    if (audio_ints != 0)  /* quick reject if no audio interrupts */
    {
      if (audio_ints & AUDIO_INT_CSS)
      {
        START_IRQ(CSS);
        static_INT_CSS ();
        END_IRQ(CSS);
      }
      if (audio_ints & AUDIO_INT_VHR)
      {
        START_IRQ(VHR);
        static_INT_VHR ();
        END_IRQ(VHR);
      }
      if (audio_ints & AUDIO_INT_VPR)
      {
        START_IRQ(VPR);
        static_INT_VPR ();
        END_IRQ(VPR);
      }
      if (audio_ints & AUDIO_INT_CRC)
      {
        START_IRQ(CRC);
        static_INT_CRC ();
        END_IRQ(CRC);
      }
/*    if (audio_ints & AUDIO_INT_ADF) static_INT_ADF ();*/
      if (audio_ints & AUDIO_INT_POU)
      {
        START_IRQ(POU);
        static_INT_POU ();
        END_IRQ(POU);
      }
/*    if (audio_ints & AUDIO_INT_SFC) static_INT_SFC ();*/
/*    if (audio_ints & AUDIO_INT_DCH) static_INT_DCH ();*/
/*    if (audio_ints & AUDIO_INT_IFT) static_INT_IFT ();*/
/*    if (audio_ints & AUDIO_INT_IFF) static_INT_IFF ();*/
      if (audio_ints & AUDIO_INT_FNP)
      {
        START_IRQ(FNP);
        static_INT_FNP ();
        END_IRQ(FNP);
      }
    }
  }
  /* Only exit when we're satisfied that we're not about to raise another IRQ. */
  while (audio_ints != 0);
}

/******	IRQ_EnableCommonIRQs() *********************************************

Purpose:	Performs actions common to video and audio IRQ claiming
Out:		Pointer to error block
Notes:		Usage is counted so that it may be called more than once

***************************************************************************/

_kernel_oserror *IRQ_EnableCommonIRQs (void)
{
  _kernel_oserror *e = NULL;
  if (CommonIRQsClaimedCount == 0)
  {
    /* Get on TickerV */
    e = _swix (OS_Claim, _INR(0,2), TickerV, tickerv_handler, Module_PrivateWord);
  }
  CommonIRQsClaimedCount++;
  if (e)
  {
    /* If something went wrong, decrement the count again, and release interrupts */
    IRQ_DisableCommonIRQs ();
  }
  return e;
}

/******	IRQ_DisableCommonIRQs() ********************************************

Purpose:	Decrements common-IRQ usage counter, and releases common
		interrupts when it reaches zero

***************************************************************************/

void IRQ_DisableCommonIRQs (void)
{
  CommonIRQsClaimedCount--;
  if (CommonIRQsClaimedCount <= 0)
  {
    CommonIRQsClaimedCount = 0;
    /* Get off TickerV */
    _swix (OS_Release, _INR(0,2), TickerV, tickerv_handler, Module_PrivateWord);
  }
}

/******	IRQ_EnableVideoPrefillIRQs() ***************************************

Purpose:	Enables interrupts needed in video prefilling
Out:		Pointer to error block

***************************************************************************/

_kernel_oserror *IRQ_EnableVideoPrefillIRQs (void)
{
  _kernel_oserror *e = NULL;
  bool irqs_were_enabled = !_kernel_irqs_disabled ();
  int discard;
  dprintf (("IRQ", "EnableVideoPrefillIRQs\n"));
  
#if Debugging==1 && DebugVideoHeaderFIFO==1
  {
    void (*routine) (char);
    if (_swix(0x531C0, _OUT(0), &routine))
    {
      DADebug_WriteC = NULL;
    }
    else
    {
      DADebug_WriteC = routine;
    }
  }
#endif
  
  /* Set a CallAfter to catch cases where the MPEG chip jams during prefill */
  e = _swix (OS_CallAfter, _INR(0,2), 1000, video_prefill_stiff_detected, Module_PrivateWord);
  
  if (!e)
  {
    /* The following trick appears to be sufficient to ensure that the 3520 is properly reinitialised (!!!!!) */
    unsigned int ints = VID_ITS_0; /* clear any outstanding interrupts */
    unsigned int vsyncs = 0; /* used to wait for both interrupt phases (emergency timeout) */
    VID_CTL &= ~VID_CTL_EDC; /* don't decode, only do sequence header search */
    VID_PPR1 = I_PICTURE << 4; /* don't scramble internal B-frame pointers */
    VID_TIS = VID_TIS_FIS | VID_TIS_EXE; /* start sequence header search */
    *(unsigned int *) (MPEG_Base_Address + Offset_MPEG_Video_Data_Write_32) = 0x000001B3; /* send a sequence header */
    while (((ints = VID_ITS_0) & VIDEO_INT_SCH) == 0)
    {
      /* Wait for SCH interrupt - meanwhile, check vsyncs in the unlikely event that it never happens */
      vsyncs |= ints & (VIDEO_INT_VSB | VIDEO_INT_VST);
      if (vsyncs == (VIDEO_INT_VSB | VIDEO_INT_VST)) break;
    };
    STi3520L_SoftReset (true, false);    
    
    /* Mask off all interrupts inside 3520 */
    WRITE_VID_ITM(0);
    
    /* Clear all the interrupt bits in case any erroneous interrupts are generated as soon as we enable them */
    READ_VID_ITS(discard);
    
    /* Claim 3520 video IRQs */
    e = _swix (OS_ClaimDeviceVector, _INR(0,2), IOMDr_MPEGVideo_DevNo /* aka 12, aka IRQRQB bit 4, aka nINT4 */,
      sti3520l_video_irq_handler, Module_PrivateWord);
  }
  if (!e)
  {
#if Debugging==1 && DebugReentrancy==1
    e = _swix (OS_ClaimDeviceVector, _INR(0,2), IOMDr_FIQDowngrade_DevNo, fiq_downgrade_handler, Module_PrivateWord);
  }
  if (!e)
  {
#endif
    
    /* Enable nINT4 in IOMD */
    if (irqs_were_enabled) _kernel_irqs_off ();
    * (volatile unsigned char *) (IOC + IOCIRQMSKB) |= IOMDr_MPEGVideo_IRQ_bit;
    if (irqs_were_enabled) _kernel_irqs_on ();
    
    /* Initialise things local to this source file */
    Stream.video.header_parser = Co_Create (&static_ProcessVideoHdr); /* initialise coroutine */
    VideoPrefillSize = -1; /* special value to flag that it hasn't been calculated yet */
    Deferred_VID_TIS = 0;
    Deferred_VID_DCF_LB_Pending = false;
    
    /* Enable appropriate interrupts (BBE is deferred until we've parsed the first headers, PSD until we enable decoding) */
    WRITE_VID_ITM(VID_ITM | VIDEO_INT_SCH | VIDEO_INT_BBF | VIDEO_INT_PER | VIDEO_INT_PID | VIDEO_INT_SER | VIDEO_INT_PDE);
    VID_CTL &= ~VID_CTL_EDC;              /* don't start decoding the first frame until we've reached buffer threshold */
    VID_PPR1 = I_PICTURE << 4;            /* make sure we don't scramble the internal B-frame pointers in the next instruction - fixes occasional glitches on stream boundaries */
    VID_TIS = VID_TIS_FIS | VID_TIS_EXE;  /* kick off the sequence header search */
  }
  
  return e;
}

/******	IRQ_EnableVideoIRQs() **********************************************

Purpose:	Enables interrupts needed in video play mode
Out:		Pointer to error block

***************************************************************************/

_kernel_oserror *IRQ_EnableVideoIRQs (void)
{
  _kernel_oserror *e = NULL;
  
  /* Update state machine */
  Stream.video.prefill_state = ps_waiting_for_presentation_start;
  dprintf (("IRQ", "EnableVideoIRQs: video.prefill_state = waiting_for_presentation_start\n"));
  
  return e;
}

/******	IRQ_DisableVideoIRQs() *********************************************

Purpose:	Disables video interrupts

***************************************************************************/

void IRQ_DisableVideoIRQs (void)
{
  dprintf (("IRQ", "DisableVideoIRQs\n"));
  WRITE_VID_ITM(0);  /* mask off all video interrupts */
  VID_CTL &= ~VID_CTL_EDC;  /* stop video decoding */
  
  Co_Destroy (&Stream.video.header_parser); /* finalise coroutine */
  
  /* Release 3520 video IRQs */
  _swix (OS_ReleaseDeviceVector, _INR(0,2), IOMDr_MPEGVideo_DevNo /* aka 12, aka IRQRQB bit 4, aka nINT4 */,
    sti3520l_video_irq_handler, Module_PrivateWord);
#if Debugging==1 && DebugReentrancy==1
  _swix (OS_ReleaseDeviceVector, _INR(0,2), IOMDr_FIQDowngrade_DevNo, fiq_downgrade_handler, Module_PrivateWord);
#endif
  
  /* Remove the CallAfter, in case it hasn't already been removed or gone off */
  _swix (OS_RemoveTickerEvent, _INR(0,1), video_prefill_stiff_detected, Module_PrivateWord);
}

/******	IRQ_EnableAudioPrefillIRQs() ***************************************

Purpose:	Enables interrupts needed in audio prefilling
Out:		Pointer to error block

***************************************************************************/

_kernel_oserror *IRQ_EnableAudioPrefillIRQs (void)
{
  _kernel_oserror *e = NULL;
  bool irqs_were_enabled = !_kernel_irqs_disabled ();
  int discard;
  clk discard_clk;
  dprintf (("IRQ", "EnableAudioPrefillIRQs\n"));
  
  /* Mask off all interrupts inside 3520 */
  WRITE_AUD_ITM(0);
  WRITE_AUD_IMS(0);
  
  /* Clear all the interrupt bits in case any erroneous interrupts are generated as soon as we enable them */
  READ_AUD_ITR(discard);
  READ_AUD_ANC(discard);
  READ_AUD_PTS(discard_clk);
  READ_AUD_HDR(discard);
  READ_AUD_SYS(discard);
  READ_AUD_ITS(discard);
  
  /* Claim 3520 audio IRQs */
  e = _swix (OS_ClaimDeviceVector, _INR(0,2), IOMDr_MPEGAudio_DevNo /* aka 10, aka IRQRQB bit 2, aka nINT6 */,
    sti3520l_audio_irq_handler, Module_PrivateWord);
  
  if (!e)
  {
    /* Assume a 44.1 kHz sample rate unless told otherwise */
    STi3520L_SetPCMClock (sr_44k1, false, false);
    
    /* Enable nINT6 in IOMD */
    if (irqs_were_enabled) _kernel_irqs_off ();
    * (volatile unsigned char *) (IOC + IOCIRQMSKB) |= IOMDr_MPEGAudio_IRQ_bit;
    if (irqs_were_enabled) _kernel_irqs_on ();
    
    /* Enable appropriate interrupts */
    WRITE_AUD_ITM(AUDIO_INT_CSS | AUDIO_INT_VHR | AUDIO_INT_VPR | AUDIO_INT_CRC | AUDIO_INT_POU | AUDIO_INT_FNP);
  }
  
  return e;
}

/******	IRQ_EnableAudioIRQs() **********************************************

Purpose:	Enables interrupts needed in audio play mode
Out:		Pointer to error block

***************************************************************************/

_kernel_oserror *IRQ_EnableAudioIRQs (void)
{
  _kernel_oserror *e = NULL;
  dprintf (("IRQ", "EnableAudioIRQs\n"));
  return e;
}

/******	IRQ_DisableAudioIRQs() *********************************************

Purpose:	Disables audio interrupts

***************************************************************************/

void IRQ_DisableAudioIRQs (void)
{
  dprintf (("IRQ", "DisableAudioIRQs\n"));
  
  WRITE_AUD_ITM(0);  /* mask off all audio interrupts */
  WRITE_AUD_IMS(0);
  
  /* Release 3520 audio IRQs */
  _swix (OS_ReleaseDeviceVector, _INR(0,2), IOMDr_MPEGAudio_DevNo /* aka 10, aka IRQRQB bit 2, aka nINT6 */,
    sti3520l_audio_irq_handler, Module_PrivateWord);
}

/******	IRQ_UpdateMuteState() **********************************************

Purpose:	Mutes or unmutes, taking account of all reasons for muting
Notes:		Is called whenever something has changed that could affect muting

***************************************************************************/

void IRQ_UpdateMuteState (void)
{
  if (Stream.rs_flags.audio_present == false ||
      Stream.audio.state == stream_closed ||
      Stream.audio.state == stream_prefilling ||
      Stream.aud_flags.muted == true ||
      Stream.volume == 0 ||
      Stream.audio.speed_indicator != 1 ||
      Stream.aud_flags.muted_across_sampling_frequency_transition == true ||
      Stream.aud_flags.recovering_from_underrun == true)
  {
    AUD_MUT = 1;
  }
  else
  {
    AUD_MUT = 0;
  }
}

#if Debugging==1 && DebugProfiling==1
/******	IRQ_ProfileInit() **************************************************

Purpose:	Initialises IRQ profiling

***************************************************************************/

void IRQ_ProfileInit (void)
{
  _memset (IRQ_IRQCount, 0, profileirq_LIMIT * 4);
  _memset (IRQ_IRQTime, 0, profileirq_LIMIT * 4);
  _memset (Working_IRQCount, 0, profileirq_LIMIT * 4);
  _memset (Working_IRQTime, 0, profileirq_LIMIT * 4);
}
#endif

/******	IRQ_ProfileCallEvery() *********************************************

Purpose:	IRQ profiling CallEvery handler
Notes:		Entered in SVC mode with IRQs disabled

***************************************************************************/

_kernel_oserror *IRQ_ProfileCallEvery(_kernel_swi_regs *r, void *pw)
{
  IGNORE(r);
  IGNORE(pw);
#if Debugging==1 && DebugProfiling==1
  _memcpy (IRQ_IRQCount, Working_IRQCount, profileirq_LIMIT * 4);
  _memcpy (IRQ_IRQTime, Working_IRQTime, profileirq_LIMIT * 4);
  _memset (Working_IRQCount, 0, profileirq_LIMIT * 4);
  _memset (Working_IRQTime, 0, profileirq_LIMIT * 4);
#endif
  return NULL;
}

/******	IRQ_HandleFIQDowngrade() *******************************************

Purpose:	FIQ-downgrade IRQ entry point

***************************************************************************/

_kernel_oserror *IRQ_HandleFIQDowngrade (_kernel_swi_regs *r, void *pw)
{
#if Debugging==1 && DebugReentrancy==1
  int *ptr = *(int **) 0x108;
  unsigned int IrqStackDepthWhenReEntered = 0;
  IGNORE(r);
  IGNORE(pw);
  while (ptr != NULL)
  {
    IrqStackDepthWhenReEntered++;
    ptr = (int *) *ptr;
  }
  DWrite_Write0("FiqDowngrade: ");
  ReEntDebug_OutputDebug (IrqStackDepthWhenReEntered - IrqStackDepthOnEntry);
  * (volatile unsigned char *) (IOC + IOCIRQMSKA) &= ~(1<<7);
#else
  IGNORE(r);
  IGNORE(pw);
#endif
  return NULL;
}


/*********************/
/* Private functions */
/*********************/

/******	static_INT_SCH() ***************************************************

Purpose:	Handles Start Code Hit video IRQ
In:		Pointer to IRQ_Handler()'s |video_ints| variable
Notes:		This is enabled whenever we launch a task instruction with
		the EXE bit set, and is disabled as soon as it is triggered.
		During parsing of headers, VID_STA.SCH is repeatedly toggled,
		but we don't want this to cause an interrupt as soon as header
		parsing is complete, so VID_ITS is re-read at the end of
		header parsing to clear the SCH bit.

***************************************************************************/

static void static_INT_SCH (unsigned int *video_ints)
{
  dprintf (("IRQ", "INT_SCH\n"));
  VID_ITM_0 &= ~(VIDEO_INT_SCH>>0); /* disable start-code hit IRQ */
  static_GetVideoHdrBits (0); /* discard any cached header FIFO data */
  if ((VID_SCM & 1) == 0) static_GetVideoHdrBits (8); /* skip the leading '\1' byte if neccesary */
  /* Enter coroutine */
  Co_SwitchTo (&Stream.video.header_parser, video_ints);
  /* Clear out our bit in |video_ints| */
  *video_ints &= ~VIDEO_INT_SCH;
}

/******	static_INT_BBF() ***************************************************

Purpose:	Handles video BitBuffer Full video IRQ
Notes:		This is active throughout prefill and normal play. It is
		primarily used to signal that one of the conditions for
		decoding to continue has become true.

***************************************************************************/

static void static_INT_BBF (unsigned int *video_ints)
{
  dprintf (("IRQ", "INT_BBF\n"));
  static_PerhapsWriteNewInstruction ();
  /* Clear out our bit in |video_ints| */
  *video_ints &= ~VIDEO_INT_BBF;
}

/******	static_INT_VS() ****************************************************

Purpose:	Handles VSync Top and VSync Bottom video IRQs
In:		Top-field (not-bottom-field) flag
Notes:		This is enabled when video goes into normal play mode. It is used
		for updating display parameters, and for pacing the decode process
		during normal play so it stays in step with the display process.

***************************************************************************/

static void static_INT_VS (bool field, unsigned int *video_ints)
{
  static const clk PAL_frame_time = { 3600, 0 };
  static const clk NTSC_frame_time = { 3003, 0 };
  clk one_frame_time = Module_Display525_60 ? NTSC_frame_time : PAL_frame_time;
  
  dprintf (("IRQ", "INT_VS%c\n", field ? 'T' : 'B'));
  
  /* Latch STC as soon as possible after receipt of VSync interrupt */
  Stream.video.latched_STC = Clk_ReadRawSTC ();
  
#ifdef DEBUGLIB
  {
    frame_t *frame = Stream.display_frames;
    int items = 0;
    dprintf (("IRQ", "INT_VS: display frames list = "));
    while (frame != NULL)
    {
      dprintf (("IRQ", "%d ", ((int) frame - (int) Stream.frame) / sizeof (frame_t)));
      frame = frame->display_link;
      if (++items > 10) break;
    }
  }
  dprintf (("IRQ", "\n"));
#endif
  
  /* See if it's time to start */
  if (Stream.video.state == stream_prefilling)
  {
    /* Do nothing if we're still waiting for the video play command */
    if (Stream.video.prefill_state == ps_waiting_for_presentation_start)
    {
      /* Do nothing if there is an audio stream, but the audio play command hasn't yet been received (because that's when PCR is initialised) */
      if (!Stream.rs_flags.audio_present || Stream.audio.state == stream_open)
      {
        /* If both video and audio PTSs have been received, but there's more than 1 frame-time until first video PTS, then wait */
        clk time_to_wait = Clk_Subtract (Stream.video.first_PTS, Clk_ConvertSTCtoPCR (Stream.video.latched_STC));
        clk two_seconds = { 180000, 0 }; /* in case of badly corrupted timestamps */
        if (!Stream.rs_flags.audio_present || (Stream.video.first_PTS.msb & 0xFE) || (Stream.audio.first_PTS.msb & 0xFE) || Stream.vid_flags.internal_reset ||
            Clk_Less (time_to_wait, one_frame_time) || Clk_Greater (time_to_wait, two_seconds))
        {
          /* If the *next* VSync is the correct interlace phase for our */
          /* first frame, then kickstart the video play state machine   */
          if (Stream.display_frames->top_field_first != field)
          {
            /* Update state machine */
            Stream.fields_till_next_AU = 1;
            Stream.vid_flags.first_frame_displayed = false;
            Stream.video.state = stream_open;
            dprintf (("IRQ", "INT_VS: video.state = stream_open\n"));
            if (!Stream.rs_flags.trick_play_mode)
            {
              WRITE_VID_VBT(0); /* remove the threshold from now on, to limit skip-stall-skip-stall loops */
            }
            /* Decode the first B-frame */
            static_PerhapsWriteNewInstruction ();
            /* Remove the prefill-stiff CallAfter, since we can now rely on the normal stiff-detect code */
            _swix (OS_RemoveTickerEvent, _INR(0,1), video_prefill_stiff_detected, Module_PrivateWord);
          }
        }
      }
    }
  }
  else if (Stream.video.state == stream_open || Stream.video.state == stream_closing)
  {
    /* Update state machine again once we've started displaying the first frame */
    if (Stream.vid_flags.first_frame_displayed == false) Stream.vid_flags.first_frame_displayed = true;
    
    /* Now more generic stuff */
    Stream.fields_into_this_PU ++;
    Stream.fields_till_next_AU --;
    
    if (Stream.video.fast_slow_state == fss_this_frame_fast && Deferred_VID_TIS)
    {
      /* Kick the decoder back into life after a video skip */
      dprintf (("IRQ", "INT_VS: Kicking decoder\n"));
      VID_TIS = Deferred_VID_TIS;
      Deferred_VID_TIS = 0;
      Stream.video.fast_slow_state = fss_normal_speed;
      dprintf (("IRQ", "INT_VS: video.fast_slow_state = normal_speed\n"));
    }
    
    if (Deferred_VID_DCF_LB_Pending)
    {
      VID_DCF_16 = (VID_DCF_16 & 0x2F) | Deferred_VID_DCF_LB;
      Deferred_VID_DCF_LB_Pending = false;
    }
    
    if (Stream.video.speed_indicator == 1 && Stream.rs_flags.trick_play_mode == false)
    {
      /* If the frame centre offsets change within a frame, or if the second of two field  */
      /* pictures has a different AFD, then the display scaling/positioning needs updating */
      if (Stream.vid_flags.display_needs_setting_up
          || (Stream.fields_into_this_PU < 2 &&
                 (Stream.display_frames->frame_centre_horizontal_offset [Stream.fields_into_this_PU + 1] != Stream.display_frames->frame_centre_horizontal_offset [Stream.fields_into_this_PU] ||
                  Stream.display_frames->frame_centre_vertical_offset [Stream.fields_into_this_PU + 1] != Stream.display_frames->frame_centre_vertical_offset [Stream.fields_into_this_PU]))
          || (Stream.fields_into_this_PU == 0 &&
                 Stream.display_frames->lastpic->active_format != Stream.display_frames->pic1.active_format))
      {
        static_PositionVideo (Stream.display_frames->frame_centre_horizontal_offset [Stream.fields_into_this_PU + 1],
                              Stream.display_frames->frame_centre_vertical_offset [Stream.fields_into_this_PU + 1],
                              Stream.display_frames->lastpic->active_format);
      }
    }
    else
    {
      if (Stream.vid_flags.display_needs_setting_up)
      {
        static_PositionVideo (Stream.display_frames->frame_centre_horizontal_offset [0],
                              Stream.display_frames->frame_centre_vertical_offset [0],
                              Stream.display_frames->pic1.active_format);
      }
    }
    /* Select appropriate field display pattern */
    if (Stream.current_picture_structure == ps_frame_picture || Stream.current_picture_structure == ps_frame_picture_extension)
    {
      if (Stream.display_frames->progressive_frame == true)
      {
        /* For progressive frames, we turn on automatic field selection (top-bottom alternating regardless) */
        VID_DCF_0 &= ~VID_DCF_USR; /* note this bit isn't latched, so has immediate effect */
      }
      else
      {
        /* For non-progressive frames, we turn on user field selection */
        VID_DCF_0 |= VID_DCF_USR; /* note this bit isn't latched, so has immediate effect */
        /* In trick play, slow-mo and pause, we just leave the first field displaying throughout */
        /* Otherwise, the second field is displayed on the second (ie next-latched) and subsequent fields (ie for repeat-frames and stalls) */
        /* (When a frame decode follows a field decode, so the frame decode doesn't start until the second VSync, this is taken care of during writing of the task instruction) */
        if (Stream.video.speed_indicator == 1 && Stream.rs_flags.trick_play_mode == false && Stream.vid_flags.up_scaling_used == false && Stream.fields_into_this_PU == 0)
        {
          static_SetNextDisplayField (!Stream.display_frames->top_field_first); /* here, the relevant bits *are* latched next VSync */
        }
      }
    }
    else
    {
      /* Whilst decoding field pictures, always enable user field selection, but only change the choice of field when writing the task instruction */
      /* This enables us to avoid displaying any part of a frame buffer that the 3520 is currently using as scratch workspace! */
      VID_DCF_0 |= VID_DCF_USR; /* note this bit isn't latched, so has immediate effect */
    }
    
    if (Stream.rs_flags.audio_present)
    {
      /* Deal with audio slow-motion start/stop */
      if (Stream.audio.speed_indicator != 1 && !Stream.aud_flags.recovering_from_underrun)
      {
        Stream.audio_slowmo_field_count ++;
        if (Stream.audio_slowmo_field_count == Stream.audio.speed_indicator)
        {
          Stream.audio_slowmo_field_count = 0;
          AUD_PLY = 1;
        }
        else if (Stream.audio_slowmo_field_count == 1)
        {
          AUD_PLY = 0;
        }
      }
      /* If video speed has changed since the last PU, restart the audio slow-motion state machine */
      if (Stream.fields_into_this_PU == 0)
      {
        if (Stream.video.speed_indicator != Stream.audio.speed_indicator)
        {
          /* First adjust the reference timestamps so that we get a smooth transition between speeds */
          clk new_reference_PCR = Clk_ConvertSTCtoPCR (Stream.video.latched_STC);
          Stream.reference_PCR = new_reference_PCR;
          Stream.reference_STC = Stream.video.latched_STC;
          /* Then actually change the audio state */
          Stream.audio.speed_indicator = Stream.video.speed_indicator;
          Stream.audio_slowmo_field_count = 0;
          if (!Stream.aud_flags.recovering_from_underrun)
          {
            AUD_PLY = 1;
          }
          IRQ_UpdateMuteState ();
        }
      }
    }
    
    dprintf (("IRQ", "INT_VS: fields_till_next_AU = %d\n", Stream.fields_till_next_AU));
    if (Stream.fields_till_next_AU == 0)
    {
      if (Stream.vid_flags.final_instruction_has_been_written_during_this_AU == false)
      {
        /* Oh dear, we've stalled. Try again in two fields' time */
        Stream.video.since_open.stalls += 1;
        Stream.video.since_reset.stalls += 1;
        Stream.fields_till_next_AU = 2;
        if (Stream.current_picture_structure == ps_second_field_picture) /* ie first decode within the PU */ Stream.field_to_trigger_second_decode_on += 2;
        if (Stream.video.state == stream_closing)
        {
          Stream.vid_flags.stalled_while_closing = true;
        }
        Stream.vid_flags.display_idle = true;
        if (Stream.vid_flags.eos_callback_requested)
        {
          Stream.vid_flags.eos_callback_requested = false;
          _swix (MPEGControl_Play, _INR(0,1), PlayFlags_VideoIdle, Stream.csh);
        }
        
        /* If this happens too many times, we assume the chip has entered a locked state, and apply a reset */
        Stream.consecutive_pipeline_stalls ++;
        if (Stream.video.speed_indicator != 0) /* don't risk divide-by-zero condition!! */
        {
          unsigned int timeout;
          if (Stream.rs_flags.trick_play_mode)
          {
            timeout = 8 * Stream.video.speed_indicator;
          }
          else
          {
            timeout = 2 * Stream.video.speed_indicator;
          }
          if ((VID_VBL & 0x3FFF) != 0) /* if bitbuffer is empty, then it probably doesn't need kicking (yet) */
          {
            if (Stream.consecutive_pipeline_stalls % (3 * timeout) == 0)
            {
#if Debugging==1 && DebugDecoderErrors==1
              DWrite_Write0("SR ");
#endif
              /* Badly locked up - do a complete video reset */
              _swix (OS_AddCallBack, _INR(0,1), video_reset_callback, Module_PrivateWord);
              Stream.video.since_open.internal_resets ++;
              Stream.video.since_reset.internal_resets ++;
            }
//            else if ((Stream.consecutive_pipeline_stalls % timeout == 0) && ((VID_STA_8 & (VIDEO_INT_PID >> 8)) == 0))
//            {
//#if Debugging==1 && DebugDecoderErrors==1
//              DWrite_Write0("PR ");
//#endif
//              STi3520L_PipelineReset ();
//            }
          }
        }
      }
      else
      {
        dprintf (("IRQ", "INT_VS: current_picture_structure = %d\n", Stream.current_picture_structure));
        Stream.consecutive_pipeline_stalls = 0;
        if (Stream.fields_into_this_PU == 0)
        {
          /* First or only decode within the PU */
          /* Check to see if lipsync has to be done - only if there's audio in the stream, */
          /* and we're playing at normal speed, and a skip/repeat isn't already pending    */
          if (Stream.rs_flags.audio_present && Stream.video.fast_slow_state == fss_normal_speed
              && Stream.display_frames->PTS_valid && !Stream.s_flags.no_lip_sync)
          {
            /* Check PTS of display frame, to see if we need to set a pending skip or repeat */
            presentation_error old_presentation_error = Stream.video_presentation_error;
            Stream.video_PTS_error = Clk_Subtract (Clk_ConvertSTCtoPCR (Stream.video.latched_STC), Stream.display_frames->PTS);
            dprintf (("IRQ", "INT_VS: PTS_error = %c%08X\n", Stream.video_PTS_error.msb ? '1' : '0', Stream.video_PTS_error.lsw));
            if (Clk_Greater (Stream.video_PTS_error, one_frame_time))
            {
              Stream.video_presentation_error = pe_presented_late;
              if (old_presentation_error == pe_presented_late && /* only skip if there's a reasonable amount of data to skip */ ((VID_VBL & 0x3FFF) > 10))
              {
                /* Need to do a frame skip */
                Stream.video.fast_slow_state = fss_next_frame_fast;
                dprintf (("IRQ", "INT_VS: video.fast_slow_state = next_frame_fast\n"));
                Stream.video.since_open.fasts ++;
                Stream.video.since_reset.fasts ++;
              }
            }
            else
            {
              clk zero = { 0, 0 };
              if (Clk_Less (Clk_Add (Stream.video_PTS_error, one_frame_time), zero))
              {
                Stream.video_presentation_error = pe_presented_early;
                if (old_presentation_error == pe_presented_early)
                {
                  /* Need to do a frame repeat */
                  Stream.video.fast_slow_state = fss_next_frame_slow;
                  dprintf (("IRQ", "INT_VS: video.fast_slow_state = next_frame_slow\n"));
                  Stream.video.since_open.slows ++;
                  Stream.video.since_reset.slows ++;
                }
              }
              else
              {
                Stream.video_presentation_error = pe_acceptable;
              }
            }
          }
        }
        if (Stream.current_picture_structure == ps_second_field_picture || Stream.current_picture_structure == ps_frame_picture_extension)
        {
          /* First of two decodes within the PU */
          Stream.fields_till_next_AU = 1; /* ideally, we want this done in one field only, irrespective of pause etc. */
        }
        else
        {
          /* Last or only decode within the PU */
          /* Set countdown to next frame, taking slow-mo, pause etc into account */
          Stream.fields_till_next_AU = 2; /* default */
          if (Stream.display_frames->seq.progressive_sequence == true)
          {
            if (Stream.display_frames->repeat_first_field == true)
            {
              if (Stream.display_frames->top_field_first == true)
              {
                Stream.fields_till_next_AU = 6;
              }
              else
              {
                Stream.fields_till_next_AU = 4;
              }
            }
          }
          else
          {
            if (Stream.display_frames->repeat_first_field && Stream.display_frames->top_field_first)
            {
              Stream.fields_till_next_AU = 4; /* make this a long frame if slow-motion 3:2 pulldown */
            }
          }
          if (Stream.video.fast_slow_state == fss_next_frame_slow)
          {
            /* To implement video repeats, we simply extend the display time by 2 */
            Stream.fields_till_next_AU += 2;
            Stream.video.fast_slow_state = fss_normal_speed;
            dprintf (("IRQ", "INT_VS: video.fast_slow_state = normal_speed\n"));
          }
          Stream.fields_till_next_AU *= Stream.video.speed_indicator;
          if (Stream.fields_till_next_AU == 0)
          {
            Stream.fields_till_next_AU = -2;
            /* For pause mode, we rely upon this not reaching 0 for a long time! */
            /* We use -2 instead of -1 so that the field polarity can be preserved when exiting pause */
          }
          if (Stream.current_picture_structure == ps_first_field_picture || (Stream.current_picture_structure == ps_frame_picture && Stream.prev_picture_structure != ps_frame_picture))
          {
            Stream.fields_till_next_AU --; /* account for one field already having been taken up by the first decode in this PU */
          }
          if (Stream.display_frames->display_link->top_field_first != ((field + Stream.fields_till_next_AU) & 1))
          {
            /* Next frame starts on the opposite field - typically due to 3:2 pulldown */
            /* so we actually need to display for an odd number of fields. */
            /* Our current estimate will be too long if this is a top-field-first frame, and too short if this is a bottom-field-first frame */
            if (Stream.display_frames->top_field_first)
            {
              Stream.fields_till_next_AU --;
              if (Stream.fields_till_next_AU == 0)
              {
                Stream.fields_till_next_AU = 2;
              }
            }
            else
            {
              Stream.fields_till_next_AU ++;
            }
          }
        }
dprintf (("3:2", "rff = %d, tff = %d, next tff = %d\n", Stream.display_frames->repeat_first_field, Stream.display_frames->top_field_first, Stream.display_frames->display_link->top_field_first));
dprintf (("3:2", "fields_till_next_AU set to %d\n", Stream.fields_till_next_AU));
        
        /* Whatever the picture_structure, flag that we've stalled (only persists until the last instruction is written) */
        Stream.vid_flags.final_instruction_has_been_written_during_this_AU = false;
      }
    }
    /* See if we now fit the condition for writing the next instruction */
    static_PerhapsWriteNewInstruction ();
  }
  /* Clear out our bit in |video_ints| */
  *video_ints &= ~(field ? VIDEO_INT_VST : VIDEO_INT_VSB);
}

/******	static_INT_PSD() ***************************************************

Purpose:	Handles Pipeline Starting to Decode video IRQ
In:		Pointer to IRQ_Handler()'s |video_ints| variable
Notes:		This is enabled throughout normal play.

***************************************************************************/

static void static_INT_PSD (unsigned int *video_ints)
{
  dprintf (("IRQ", "INT_PSD\n"));
  /* If start-code hit has already happened then fake an interrupt */
  if (VID_STA_0 & VIDEO_INT_SCH>>0 != 0)
  {
    *video_ints |= VIDEO_INT_SCH;
  }
  VID_ITM_0 |= VIDEO_INT_SCH>>0; /* enable start-code hit IRQ */
  /* If the decoder latches the next task before we've decoded the next headers, wait with 1-field granularity (ie unset VID_TIS.RPT) */
  VID_TIS = 0;
  dprintf (("IRQ", "INT_PSD: RFP = %04X, FFP = %04X, BFP = %04X\n", VID_RFP & 0x3FFF, VID_FFP & 0x3FFF, VID_BFP & 0x3FFF));
  /* Clear out our bit in |video_ints| */
  *video_ints &= ~VIDEO_INT_PSD;
}

/******	static_INT_PID() ***************************************************

Purpose:	Handles Pipeline IDle video IRQ
In:		Pointer to IRQ_Handler()'s |video_ints| variable
Notes:		This is enabled throughout prefill and normal play. It is
		used to signal that one of the conditions for continuing
		decoding after a skip operation, or a decode during prefill,
		has become true.

***************************************************************************/

static void static_INT_PID (unsigned int *video_ints)
{
  dprintf (("IRQ", "INT_PID\n"));
  if (!Stream.vid_flags.had_phoney_pipeline_idle_IRQ)
  {
    /* We always get one extra interrupt at the very beginning of a stream */
    /* I suspect this is due to the termination of the initial sequence-header-search task */
    Stream.vid_flags.had_phoney_pipeline_idle_IRQ = true;
  }
  else
  {
    if (!Stream.vid_flags.last_instruction_was_a_decode)
    {
      /* We have just completed a skip operation */
      /* Remove unwanted frame from both decode and display lists (unless we're the first of a */
      /* field pair, in which case do it after the second picture, which will also be skipped) */
      if (Stream.current_picture_structure != ps_first_field_picture)
      {
        static_RemoveFrameFromList (Stream.decode_frames, false);
        static_RemoveFrameFromList (Stream.decode_frames, true);
        if (Stream.video.fast_slow_state == fss_next_frame_fast)
        {
          Stream.video.fast_slow_state = fss_this_frame_fast; /* flag that next decode has to be manually latched (yuck) */
          dprintf (("IRQ", "INT_PID: video.fast_slow_state = this_frame_fast\n"));
        }
      }
      /* Force a header search (won't have been done automatically in this case) */
      VID_HDS = VID_HDS_SOS | VID_HDS_HDS;
      VID_ITM_0 |= VIDEO_INT_SCH>>0; /* enable start-code hit IRQ */
    }
    Stream.vid_flags.this_frames_data_have_been_decoded = true;
    static_PerhapsWriteNewInstruction ();
  }
  /* Clear out our bit in |video_ints| */
  *video_ints &= ~VIDEO_INT_PID;
}

/******	static_INT_HFF() ***************************************************

Purpose:	Handles Header FIFO Full video IRQ
In:		Pointer to IRQ_Handler()'s |video_ints| variable
Notes:		This is enabled when the header FIFO has emptied during
		parsing of video headers. It is disabled when a slice header
		has been found, indicating the end of header parsing.

***************************************************************************/

static void static_INT_HFF (unsigned int *video_ints)
{
  dprintf (("IRQ", "INT_HFF\n"));
  /* Re-enter coroutine */
  Co_SwitchTo (&Stream.video.header_parser, video_ints);
  /* Clear out our bit in |video_ints| */
  *video_ints &= ~VIDEO_INT_HFF;
}

/******	static_INT_BBE() ***************************************************

Purpose:	Handles Bit Buffer Empty video IRQ
Notes:		This is enabled all the time, except during the parsing of the
		initial headers, and during play-out. It is used to count the
		number of video underruns that have occurred (hopefully none)

***************************************************************************/

static void static_INT_BBE (void)
{
  dprintf (("IRQ", "INT_BBE\n"));
  /* Don't count the underrun that happens naturally as part of play-out! */
  if (Stream.video.state != stream_closing)
  {
    Stream.video.since_open.underruns += 1;
    Stream.video.since_reset.underruns += 1;
  }
}

/******	static_INT_PER() ***************************************************

Purpose:	Handles Pipeline ERror video IRQ
Notes:		This is enabled throughout prefill and normal play, in order
		to count the occurrences of pipeline errors.

***************************************************************************/

static void static_INT_PER (void)
{
#if Debugging==1 && DebugDecoderErrors==1
  DWrite_Write0("PER ");
#endif
  dprintf (("IRQ", "INT_PER\n"));
  Stream.video.since_open.pipeline_errors += 1;
  Stream.video.since_reset.pipeline_errors += 1;
#if ManualPipelineResets==1
  STi3520L_PipelineReset ();
#endif
}

/******	static_INT_SER() ***************************************************

Purpose:	Handles Severe ERror video IRQ
Notes:		This is enabled throughout prefill and normal play, in order to
		count the occurrences of severe (macroblock overflow) errors.

***************************************************************************/

static void static_INT_SER (void)
{
#if Debugging==1 && DebugDecoderErrors==1
  DWrite_Write0("SER ");
#endif
  dprintf (("IRQ", "INT_SER\n"));
  Stream.video.since_open.severe_errors += 1;
  Stream.video.since_reset.severe_errors += 1;
#if ManualPipelineResets==1
  STi3520L_PipelineReset ();
#endif
}

/******	static_INT_PDE() ***************************************************

Purpose:	Handles Picture Decode Error video IRQ
Notes:		This is enabled throughout prefill and normal play, in order to
		count the occurrences of picture decode (macroblock underflow) errors.

***************************************************************************/

static void static_INT_PDE (void)
{
#if Debugging==1 && DebugDecoderErrors==1
  DWrite_Write0("PDE ");
#endif
  dprintf (("IRQ", "INT_PDE\n"));
  Stream.video.since_open.picture_decode_errors += 1;
  Stream.video.since_reset.picture_decode_errors += 1;
#if ManualPipelineResets==1
  STi3520L_PipelineReset ();
#endif
}

/******	static_INT_CSS() ***************************************************

Purpose:	Handles Change in Synchronisation Status audio IRQ
Notes:		This is enabled throughout prefill and normal play.

***************************************************************************/

static void static_INT_CSS (void)
{
  dprintf (("IRQ", "INT_CSS\n"));
  if ((AUD_SYS & 3) == 1)
  {
    Stream.audio.since_open.sync_losses += 1;
    Stream.audio.since_reset.sync_losses += 1;
  }
}

/******	static_INT_VHR() ***************************************************

Purpose:	Handles Valid Header Registered audio IRQ
Notes:		This is enabled throughout prefill and normal play.

***************************************************************************/

static void static_INT_VHR (void)
{
  sample_rate old_sampling_frequency = (sample_rate) Stream.audio_header.fields.sampling_frequency;
  audio_channel_type old_mode = (audio_channel_type) Stream.audio_header.fields.mode;
  
  dprintf (("IRQ", "INT_VHR\n"));
  
  /* Read new audio header from 3520 */
  Stream.audio_header.word = AUD_HDR;
  
  /* Unmute after a sampling frequency transition, if necessary */
  if (Stream.aud_flags.muted_across_sampling_frequency_transition)
  {
    Stream.aud_flags.muted_across_sampling_frequency_transition = false;
    IRQ_UpdateMuteState ();
  }
  
  /* If sample rate has changed, or if this is the first header, set up PCMCLK */
  /* Also mute for an audio frame, so as to mask the transitional glitch - but not on the first header! */
  /* Note that we are unable to use audio interrupt SFC for this purpose, it's as good as useless because */
  /* the 3520 doesn't reset its internal copy of the latest sampling frequency on any software resets! */
  if ((sample_rate) Stream.audio_header.fields.sampling_frequency != old_sampling_frequency || !Stream.aud_flags.first_header_received)
  {
    STi3520L_SetPCMClock ((sample_rate) Stream.audio_header.fields.sampling_frequency, false, false);
    if (Stream.aud_flags.first_header_received)
    {
      Stream.aud_flags.muted_across_sampling_frequency_transition = true;
      IRQ_UpdateMuteState ();
    }
  }
  
  /* If audio audio mode has changed, or if this is the first header, we may need to output left only, right only or both channels */
  if ((audio_channel_type) Stream.audio_header.fields.mode != old_mode || !Stream.aud_flags.first_header_received)
  {
    if (Stream.audio_header.fields.mode != ct_dual_mono || Stream.audio_parms.specifier == dc_both)
    {
      AUD_EXT = dc_both;
    }
    else
    {
      AUD_EXT = Stream.audio_parms.specifier ^ 3; /* left and right channel bits are swapped in AUD_EXT and dual_channel_specifier */
    }
  }
  
  /* Ensure this is true from now on */
  Stream.aud_flags.first_header_received = true;
}

/******	static_INT_VPR() ***************************************************

Purpose:	Handles Valid PTS Registered audio IRQ
Notes:		This is enabled throughout prefill and normal play.

***************************************************************************/

static void static_INT_VPR (void)
{
  dprintf (("IRQ", "INT_VPR\n"));
  /* Latch STC as soon as possible after receipt of interrupt */
  Stream.audio.latched_STC = Clk_ReadRawSTC ();
#ifdef DEBUGLIB
  {
    clk diff;
    static clk old_STC;
    diff = Clk_Subtract (Stream.audio.latched_STC, old_STC);
    dprintf (("Sync", "Audio PTS STC diff = %d\n", diff.lsw));
    old_STC = Stream.audio.latched_STC;
  }
#endif
  READ_AUD_PTS(Stream.audio_PTS);
  dprintf (("IRQ,Sync", "Audio PTS received: %c%08X\n", Stream.audio_PTS.msb ? '1' : '0', Stream.audio_PTS.lsw));
  Stream.audio.since_open.PTSs_out += 1;
  Stream.audio.since_reset.PTSs_out += 1;
  
  /* Now set up the reference clock */
  Stream.reference_STC = Stream.audio.latched_STC;
  Stream.reference_PCR = Stream.audio_PTS;
}

/******	static_INT_CRC() ***************************************************

Purpose:	Handles CRC error detected audio IRQ
Notes:		This is enabled throughout prefill and normal play.

***************************************************************************/

static void static_INT_CRC (void)
{
  dprintf (("IRQ", "INT_CRC\n"));
  Stream.audio.since_open.crc_errors += 1;
  Stream.audio.since_reset.crc_errors += 1;
}

/******	static_INT_POU() ***************************************************

Purpose:	Handles PCM Output buffer Underflow audio IRQ
Notes:		This is enabled throughout prefill and normal play.

***************************************************************************/

static void static_INT_POU (void)
{
  dprintf (("IRQ", "INT_POU\n"));
  Stream.audio.since_open.underruns += 1;
  Stream.audio.since_reset.underruns += 1;
  Stream.aud_flags.recovering_from_underrun = true;
  AUD_PLY = 0;
  IRQ_UpdateMuteState ();
  Stream.aud_flags.pcm_output_idle = true;
  if (Stream.aud_flags.eos_callback_requested)
  {
    Stream.aud_flags.eos_callback_requested = false;
    _swix (MPEGControl_Play, _INR(0,1), PlayFlags_AudioIdle, Stream.csh);
  }
}

/******	static_INT_FNP() ***************************************************

Purpose:	Handles First bit of New frame at PCM output audio IRQ
Notes:		This is enabled throughout prefill and normal play.

***************************************************************************/

static void static_INT_FNP (void)
{
  dprintf (("IRQ", "INT_FNP\n"));
  if (Stream.aud_flags.had_first_INT_FNP)
  {
    Stream.aud_flags.pcm_output_idle = false;
  }
  else
  {
    Stream.aud_flags.had_first_INT_FNP = true;
  }
}

/******	static_ProcessVideoHdr() *******************************************

Purpose:	Main coroutine entry point for video header parser
In:		Coroutine argument (address of IRQ_Handler()'s |video_ints| variable)

***************************************************************************/

static void static_ProcessVideoHdr (void *argument)
{
  unsigned int data;  /* for holding the results of static_GetVideoHdrBits */
  unsigned int *video_ints = argument;
  enum { unlocked, first_zero, second_zero, one } stage;
  while (true)
  {
    data = static_GetVideoHdrBits (8);
    stage = unlocked; /* Default start state for next header search */
    if (Stream.vid_flags.started_parsing_headers && data <= SLICE_START_CODE_MAX && data >= SLICE_START_CODE_MIN)
    {
      /* Now we've processed all the headers, take any necessary actions */
      static_ProcessVideoSliceHdr ();
      /* Sort out interrupt state between sets of headers */
      VID_ITM_8 &= ~(VIDEO_INT_HFF>>8); /* disable header fifo full IRQ, so we only return when the start-code detector has hit the next picture */
      *video_ints = (*video_ints | VID_ITS) & VID_ITM & ~VIDEO_INT_SCH & ~VIDEO_INT_HFF; /* clear the SCH and HFF bits in both VID_ITS and |video_ints| */
      /* Wait until the next set of headers arrive */
#if Debugging==1 && DebugVideoHeaderFIFO==1
      if (DADebug_WriteC != NULL)
      {
        DADebug_WriteC (0); DADebug_WriteC (0); DADebug_WriteC (0);
        DADebug_WriteC ('#'); DADebug_WriteC ('#'); DADebug_WriteC ('#');
      }
#endif
      Co_SwitchTo (Co_MainRoutine, NULL);
      /* By the time we return from Co_SwitchTo(), picture decoding is complete and an automatic header search has already been launched and completed. */
    }
    else
    {
      switch (data)
      {
        case SEQUENCE_HEADER_CODE:
          /* Start a new picture struct */
          if (!Stream.vid_flags.started_parsing_headers) static_NewPictureDecode ();
          if (Stream.vid_flags.sequence_headers_locked)
          {
            static_ProcessRepeatVideoSequenceHdr ();
          }
          else
          {
            static_ProcessVideoSequenceHdr ();
          }
          break;
        case GROUP_START_CODE:
          /* If this is the first header for this picture, start a new picture struct */
          if (!Stream.vid_flags.started_parsing_headers) static_NewPictureDecode ();
          static_ProcessVideoGOPHdr ();
          break;
        case PICTURE_START_CODE:
          /* If this is the first header for this picture, start a new picture struct */
          if (!Stream.vid_flags.started_parsing_headers) static_NewPictureDecode ();
          static_ProcessVideoPictureHdr ();
          break;
        case EXTENSION_START_CODE:
          switch (static_GetVideoHdrBits (4))
          {
            case SEQUENCE_EXTENSION:
              if (Stream.vid_flags.sequence_headers_being_parsed)
              {
                static_ProcessVideoSequenceExt ();
              }
              break;
            case SEQUENCE_DISPLAY_EXTENSION:
              if (Stream.vid_flags.sequence_headers_being_parsed)
              {
                static_ProcessVideoSequenceDisplayExt ();
              }
              break;
            case PICTURE_CODING_EXTENSION:
              static_ProcessVideoPictureCodingExt ();
              break;
            case QUANT_MATRIX_EXTENSION:
              static_ProcessVideoQuantMatrixExt ();
              break;
            case PICTURE_DISPLAY_EXTENSION:
              if (Stream.vid_flags.display_headers_present)
              {
                static_ProcessVideoPictureDisplayExt ();
              }
              break;
          }
          break;
        case USER_DATA_START_CODE:
          if (static_ProcessVideoUserData ())
          {
            stage = first_zero;
          }
          break;
        case SEQUENCE_END_CODE:
          Stream.vid_flags.sequence_headers_locked = false;
          break;
      }
      /* Unfortunately, we can't rely on an automatic header search at *this* point, because I've      */
      /* observed VID_SCM returning the wrong value from time to time, especially when the header FIFO */
      /* empties somewhere near a start code prefix. Instead, we have to perform the search ourselves: */
      static_GetVideoHdrBits (0xFF); /* realign to byte boundary */
      do
      {
        data = static_GetVideoHdrBits (8);
        if (stage == unlocked && data == 0) stage = first_zero;
        else if ((stage == first_zero || stage == second_zero) && data == 0) stage = second_zero;
        else if (stage == second_zero && data == 1) stage = one;
        else stage = unlocked;
      }
      while (stage != one);
    }
  }
}

/******	static_NewPictureDecode() ******************************************

Purpose:	Sets up frame struct when we start to decode a new picture

***************************************************************************/

static void static_NewPictureDecode (void)
{
  frame_t *new_frame;
  /* Unless the new picture is (probably) the second in a pair of field pictures, we need to start a new frame struct */
  if (Stream.current_picture_structure != ps_first_field_picture)
  {
    dprintf (("IRQ", "NewPictureDecode: new frame struct\n"));
    /* Remove a frame struct from the unused list (there will always be at least one unused at this point) */
    new_frame = Stream.unused_frames;
    Stream.unused_frames = new_frame->decode_link;
    /* Add it on the front of the decoding list */
    new_frame->decode_link = Stream.decode_frames;
    Stream.decode_frames = new_frame;
    Stream.decode_frames->on_decode_list = true;
    Stream.decode_frames->on_display_list = false;
    Stream.decode_frames->PTS_valid = false;
    /* Write picture parameters into the first picture_t struct */
    Stream.decode_frames->lastpic = &Stream.decode_frames->pic1;
    /* Initialise the CC data once per frame, in case field coding is used and user data is provided on a field-wise basis */
    *(unsigned int *)Stream.decode_frames->CC_odd_data = *(unsigned int *)Stream.decode_frames->CC_even_data = 0;
  }
  else
  {
    dprintf (("IRQ", "NewPictureDecode: reused frame struct\n"));
    /* Write picture parameters into the second picture_t struct */
    Stream.decode_frames->lastpic = &Stream.decode_frames->pic2;
  }
  Stream.vid_flags.started_parsing_headers = true;
#ifdef DEBUGLIB
  {
    frame_t *frame = Stream.decode_frames;
    int items = 0;
    dprintf (("IRQ", "NewPictureDecode: decode frames list = "));
    while (frame != NULL)
    {
      dprintf (("IRQ", "%d ", ((int) frame - (int) Stream.frame) / sizeof (frame_t)));
      frame = frame->decode_link;
      if (++items > 10) break;
    }
  }
  dprintf (("IRQ", "\n"));
#endif
}

/******	static_ProcessVideoSequenceHdr() ***********************************

Purpose:	Parses MPEG sequence_header()

***************************************************************************/

static void static_ProcessVideoSequenceHdr (void)
{
  dprintf (("IRQ", "ProcessVideoSequenceHdr\n"));
  /* Set decoding state flags */
  Stream.last_video_header_layer_decoded = lvhld_sequence;
  Stream.vid_flags.sequence_header_received_since_picture_header = true;
  Stream.vid_flags.sequence_headers_being_parsed = true;
  
  Stream.decode_frames->seq.mpeg2 = false;  /* assume MPEG-1 until we find a sequence_extension */
  Stream.decode_frames->seq.display_horizontal_size =
    Stream.decode_frames->seq.coded_horizontal_size = static_GetVideoHdrBits (12);
  Stream.decode_frames->seq.display_vertical_size =
    Stream.decode_frames->seq.coded_vertical_size = static_GetVideoHdrBits (12);
  WRITE_VID_DFS(((Stream.decode_frames->seq.coded_horizontal_size + 15) / 16) * ((Stream.decode_frames->seq.coded_vertical_size + 15) / 16));
  WRITE_VID_DFW((Stream.decode_frames->seq.coded_horizontal_size + 15) / 16);
  Stream.decode_frames->seq.aspect_ratio_information = static_GetVideoHdrBits (4);
  Stream.decode_frames->seq.frame_rate = frame_rate_table [static_GetVideoHdrBits (4)];
  Stream.decode_frames->seq.bit_rate = static_GetVideoHdrBits (18);
  static_GetVideoHdrBits (1);  /* skip marker bit */
  Stream.decode_frames->seq.vbv_buffer_size = static_GetVideoHdrBits (10);
  static_GetVideoHdrBits (1);  /* skip constrained parameters flag */
  /* The code for loading the quantization matrices is identical to that for processing quant_matrix_extension() */
  static_ProcessVideoQuantMatrixExt ();
  /* Set sensible values in case the sequence extensions are absent */
  Stream.decode_frames->seq.profile_and_level_indication = 0;  /* unknown */
  Stream.decode_frames->seq.progressive_sequence = true;       /* MPEG-1 is progressive only */
  Stream.decode_frames->seq.chroma_format = 1;                 /* MPEG-1 only uses 4:2:0 */
  Stream.decode_frames->seq.low_delay = false;                 /* typically */
  Stream.decode_frames->seq.video_format = 5;                  /* unspecified */
  Stream.decode_frames->seq.colour_primaries = 1;              /* ITU-R BT.709 */
  Stream.decode_frames->seq.transfer_characteristics = 1;      /* ITU-R BT.709 */
  Stream.decode_frames->seq.matrix_coefficients = 1;           /* ITU-R BT.709 */
  Stream.decode_frames->frame_centre_horizontal_offset [0] =
  Stream.decode_frames->frame_centre_horizontal_offset [1] =
  Stream.decode_frames->frame_centre_horizontal_offset [2] = 0;
  Stream.decode_frames->frame_centre_vertical_offset [0] =
  Stream.decode_frames->frame_centre_vertical_offset [1] =
  Stream.decode_frames->frame_centre_vertical_offset [2] = 0;
  Stream.decode_frames->pic1.active_format = 8; /* default AFD = 8 (as the coded frame) */
  Stream.decode_frames->pic2.active_format = 8;
}

/******	static_ProcessRepeatVideoSequenceHdr() *****************************

Purpose:	Parses subsequent occurrences of MPEG sequence_header()

***************************************************************************/

static void static_ProcessRepeatVideoSequenceHdr (void)
{
  bool corrupted = false;
  dprintf (("IRQ", "ProcessRepeatVideoSequenceHdr\n"));
  /* Set decoding state flags */
  Stream.last_video_header_layer_decoded = lvhld_sequence;
  Stream.vid_flags.sequence_header_received_since_picture_header = true;
  
  Stream.decode_frames->seq = Stream.decode_frames->decode_link->seq; /* keep this stuff the same */
  if (static_GetVideoHdrBits (12) != Stream.decode_frames->seq.coded_horizontal_size & 0xFFF) corrupted = true;
  if (static_GetVideoHdrBits (12) != Stream.decode_frames->seq.coded_vertical_size & 0xFFF) corrupted = true;
  if (static_GetVideoHdrBits (4) != Stream.decode_frames->seq.aspect_ratio_information) corrupted = true;
  static_GetVideoHdrBits (4); /* previous frame_rate not stored */
  if (static_GetVideoHdrBits (18) != Stream.decode_frames->seq.bit_rate & 0x3FFFF) corrupted = true;
  if (static_GetVideoHdrBits (1) != 1) corrupted = true;  /* marker bit */
  if (static_GetVideoHdrBits (10) != Stream.decode_frames->seq.vbv_buffer_size & 0x3FF) corrupted = true;
  static_GetVideoHdrBits (1);  /* constrained parameters flag not stored */
  if (corrupted)
  {
    Stream.video.since_open.seqhdr_errors ++;
    Stream.video.since_reset.seqhdr_errors ++;
  }
  else
  {
    /* The quant matrices are the only part of repeat sequence headers that are allowed to change */
    /* The code for loading the quantization matrices is identical to that for processing quant_matrix_extension() */
    static_ProcessVideoQuantMatrixExt ();
  }
  /* Some things need to be reset on repeat sequence headers */
  Stream.decode_frames->frame_centre_horizontal_offset [0] =
  Stream.decode_frames->frame_centre_horizontal_offset [1] =
  Stream.decode_frames->frame_centre_horizontal_offset [2] = 0;
  Stream.decode_frames->frame_centre_vertical_offset [0] =
  Stream.decode_frames->frame_centre_vertical_offset [1] =
  Stream.decode_frames->frame_centre_vertical_offset [2] = 0;
  Stream.decode_frames->pic1.active_format = 8; /* default AFD = 8 (as the coded frame) */
  Stream.decode_frames->pic2.active_format = 8;
}

/******	static_ProcessVideoSequenceExt() ***********************************

Purpose:	Parses MPEG sequence_extension()

***************************************************************************/

static void static_ProcessVideoSequenceExt (void)
{
  dprintf (("IRQ", "ProcessVideoSequenceExt\n"));
  Stream.decode_frames->seq.mpeg2 = true;
  Stream.decode_frames->seq.profile_and_level_indication = static_GetVideoHdrBits (8);
  Stream.decode_frames->seq.progressive_sequence = static_GetVideoHdrBits (1);
  Stream.decode_frames->seq.chroma_format = static_GetVideoHdrBits (2);
  Stream.decode_frames->seq.display_horizontal_size =
    Stream.decode_frames->seq.coded_horizontal_size |= static_GetVideoHdrBits (2) << 12;
  Stream.decode_frames->seq.display_vertical_size =
    Stream.decode_frames->seq.coded_vertical_size |= static_GetVideoHdrBits (2) << 12;
  WRITE_VID_DFS(((Stream.decode_frames->seq.coded_horizontal_size + 15) / 16) * ((Stream.decode_frames->seq.coded_vertical_size + 15) / 16));
  WRITE_VID_DFW((Stream.decode_frames->seq.coded_horizontal_size + 15) / 16);
  Stream.decode_frames->seq.bit_rate |= static_GetVideoHdrBits (12) << 18;
  static_GetVideoHdrBits (1);  /* skip marker bit */
  Stream.decode_frames->seq.vbv_buffer_size |= static_GetVideoHdrBits (8) << 10;
  Stream.decode_frames->seq.low_delay = static_GetVideoHdrBits (1);
  Stream.decode_frames->seq.frame_rate *= static_GetVideoHdrBits (2) + 1;
  Stream.decode_frames->seq.frame_rate /= static_GetVideoHdrBits (5) + 1;
}

/******	static_ProcessVideoSequenceDisplayExt() ****************************

Purpose:	Parses MPEG sequence_display_extension()

***************************************************************************/

static void static_ProcessVideoSequenceDisplayExt (void)
{
  dprintf (("IRQ", "ProcessVideoSequenceDisplayExt\n"));
  Stream.decode_frames->seq.video_format = static_GetVideoHdrBits (3);
  if (static_GetVideoHdrBits (1) == 1)
  {
    Stream.decode_frames->seq.colour_primaries = static_GetVideoHdrBits (8);
    Stream.decode_frames->seq.transfer_characteristics = static_GetVideoHdrBits (8);
    Stream.decode_frames->seq.matrix_coefficients = static_GetVideoHdrBits (8);
  }
  /* else the defaults from static_ProcessVideoSequenceHdr persist */
  Stream.decode_frames->seq.display_horizontal_size = static_GetVideoHdrBits (14);
  static_GetVideoHdrBits (1);  /* skip marker bit */
  Stream.decode_frames->seq.display_vertical_size = static_GetVideoHdrBits (14);
  /* Flag that picture display extensions are now valid */
  Stream.vid_flags.display_headers_present = true;
}

/******	static_ProcessVideoGOPHdr() ****************************************

Purpose:	Parses MPEG group_of_pictures_header()

***************************************************************************/

static void static_ProcessVideoGOPHdr (void)
{
  dprintf (("IRQ", "ProcessVideoGOPHdr\n"));
  /* Set decoding state flags */
  Stream.last_video_header_layer_decoded = lvhld_gop;
  Stream.vid_flags.gop_header_received_since_picture_header = true;
  
  Stream.decode_frames->gop.time_code = static_GetVideoHdrBits (25);
  Stream.decode_frames->gop.closed_gop = static_GetVideoHdrBits (1);
  Stream.decode_frames->gop.broken_link = static_GetVideoHdrBits (1);
}

/******	static_ProcessVideoPictureHdr() ************************************

Purpose:	Parses MPEG picture_header()

***************************************************************************/

static void static_ProcessVideoPictureHdr (void)
{
  dprintf (("IRQ", "ProcessVideoPictureHdr\n"));
  /* If necessary, clone sequence and GOP-related variables from the previously-decoded picture */
  if (Stream.current_picture_structure != ps_first_field_picture)
  {
    if (!Stream.vid_flags.sequence_header_received_since_picture_header)
    {
      Stream.decode_frames->seq = Stream.decode_frames->decode_link->seq;
      Stream.decode_frames->frame_centre_horizontal_offset [0] =
      Stream.decode_frames->frame_centre_horizontal_offset [1] =
      Stream.decode_frames->frame_centre_horizontal_offset [2] = Stream.decode_frames->decode_link->frame_centre_horizontal_offset [2];
      Stream.decode_frames->frame_centre_vertical_offset [0] =
      Stream.decode_frames->frame_centre_vertical_offset [1] =
      Stream.decode_frames->frame_centre_vertical_offset [2] = Stream.decode_frames->decode_link->frame_centre_vertical_offset [2];
      if (Stream.current_picture_structure != ps_first_field_picture)
      {
        Stream.decode_frames->lastpic->active_format = Stream.decode_frames->decode_link->lastpic->active_format;
      }
      else
      {
        Stream.decode_frames->lastpic->active_format = Stream.decode_frames->pic1.active_format;
      }
    }
    if (!Stream.vid_flags.gop_header_received_since_picture_header)
    {
      Stream.decode_frames->gop = Stream.decode_frames->decode_link->gop;
    }
  }
  if (Stream.vid_flags.sequence_header_received_since_picture_header)
  {
    /* Now that new sequence headers have been parsed, recalculate the aspect ratios and mark the display parameters as needing recalculating */
    if (Stream.decode_frames->seq.mpeg2 && Stream.decode_frames->seq.aspect_ratio_information != 1)
    {
      /* Specified aspect ratio is display aspect ratio */
      switch (Stream.decode_frames->seq.aspect_ratio_information)
      {
        default: /* illegal MPEG-2 syntax - but most of our test clips seem to be encoded this way :-( */
        case 2:  /* DAR = 4:3 */
          Stream.decode_frames->seq.display_aspect_ratio = ASPECT_4_TO_3;
          break;
        case 3:  /* DAR = 16:9 */
          Stream.decode_frames->seq.display_aspect_ratio = ASPECT_16_TO_9;
          break;
        case 4:  /* DAR = 2.21:1 */
          Stream.decode_frames->seq.display_aspect_ratio = ASPECT_2_21_TO_1;
          break;
      }
      /* IAR can be calcuated from DAR and the relevant sizes; the sizes are all 14-bit numbers, so it's safe to multiply them (saves an extra division) */
      Stream.decode_frames->seq.coded_aspect_ratio = Fixed_IntDivide (Stream.decode_frames->seq.coded_horizontal_size * Stream.decode_frames->seq.display_vertical_size,
          Stream.decode_frames->seq.coded_vertical_size * Stream.decode_frames->seq.display_horizontal_size);
      Stream.decode_frames->seq.coded_aspect_ratio = Fixed_FixedMultiply (Stream.decode_frames->seq.display_aspect_ratio, Stream.decode_frames->seq.coded_aspect_ratio);
    }
    else
    {
      /* Specified aspect ratio is sample (pixel) aspect ratio */
      Stream.decode_frames->seq.display_aspect_ratio = Fixed_IntDivide (Stream.decode_frames->seq.display_horizontal_size, Stream.decode_frames->seq.display_vertical_size);
      Stream.decode_frames->seq.display_aspect_ratio = Fixed_FixedMultiply (mpeg_1_pixel_aspect_ratio_table [Stream.decode_frames->seq.aspect_ratio_information], Stream.decode_frames->seq.display_aspect_ratio);
      Stream.decode_frames->seq.coded_aspect_ratio = Fixed_IntDivide (Stream.decode_frames->seq.coded_horizontal_size, Stream.decode_frames->seq.coded_vertical_size);
      Stream.decode_frames->seq.coded_aspect_ratio = Fixed_FixedMultiply (mpeg_1_pixel_aspect_ratio_table [Stream.decode_frames->seq.aspect_ratio_information], Stream.decode_frames->seq.coded_aspect_ratio);
    }
  }
  
  Stream.decode_frames->temporal_reference = static_GetVideoHdrBits (10);
  if (Stream.vid_flags.gop_header_received_since_picture_header)
  {
    /* MPEG requires that the first picture after a GOP header is intra coded */
    /* Forcing this to be the case gives us better error resilience */
    Stream.decode_frames->lastpic->picture_coding_type = I_PICTURE;
    static_GetVideoHdrBits (3); /* discard 3 bits */
  }
  else
  {
    Stream.decode_frames->lastpic->picture_coding_type = static_GetVideoHdrBits (3);
  }
  Stream.decode_frames->lastpic->vbv_delay = static_GetVideoHdrBits (16);
  if (Stream.decode_frames->lastpic->picture_coding_type == P_PICTURE ||
      Stream.decode_frames->lastpic->picture_coding_type == B_PICTURE)
  {
    /* The 3520 overloads MPEG-1 full_pel_forward_vector/forward_f_code and MPEG-2 f_code[0,0] */
    Stream.decode_frames->lastpic->forward_horizontal_f_code = static_GetVideoHdrBits (4);
    /* f_code[0,1] register not used in MPEG-1 mode */
    Stream.decode_frames->lastpic->forward_vertical_f_code = 0xF;
  }
  else
  {
    /* Forward f_codes not used for I pictures */
    Stream.decode_frames->lastpic->forward_horizontal_f_code =
      Stream.decode_frames->lastpic->forward_vertical_f_code = 0xF;
  }
  if (Stream.decode_frames->lastpic->picture_coding_type == B_PICTURE)
  {
    /* The 3520 overloads MPEG-1 full_pel_backward_vector/backward_f_code and MPEG-2 f_code[1,0] */
    Stream.decode_frames->lastpic->backward_horizontal_f_code = static_GetVideoHdrBits (4);
    /* f_code[1,1] register not used in MPEG-1 mode */
    Stream.decode_frames->lastpic->backward_vertical_f_code = 0xF;
  }
  else
  {
    /* Backward f_codes not used for I or P pictures */
    Stream.decode_frames->lastpic->backward_horizontal_f_code =
      Stream.decode_frames->lastpic->backward_vertical_f_code = 0xF;
  }
  /* Ignore extra_bit_picture and extra_information_picture */
  
  /* Get the PTS for this frame, if any */
  if (Stream.current_picture_structure != ps_first_field_picture)
  {
    unsigned int SCDcount;
    signed int difference;
    READ_VID_SCDcount(SCDcount);
    SCDcount = (SCDcount << 1) & 0xFFFFFF; /* adjust for this being measured in half-words */
    Stream.decode_frames->PTS_valid = false;
    /* We're interested in the last entry in the list (if any) which has a count lower than the current one */
    while (Stream.video_PTS_read_ptr != Stream.video_PTS_write_ptr)
    {
      difference = Stream.video_PTS_queue[Stream.video_PTS_read_ptr].count - SCDcount; /* signed 24-bit number */
      if ((difference & 0x800000) != 0) /* 24-bit sign comparison */
      {
        /* CDcount was lower than SCDcount, and is therefore a potential candidate for a match */
        Stream.decode_frames->PTS_valid = true;
        Stream.decode_frames->PTS = Stream.video_PTS_queue[Stream.video_PTS_read_ptr].PTS;
        Stream.video_PTS_read_ptr = (Stream.video_PTS_read_ptr + 1) % PTS_LIST_SIZE;
      }
      else
      {
        /* Read pointer is now pointing at an entry for some future frame */
        break;
      }
    }
    if (Stream.decode_frames->PTS_valid)
    {
      Stream.video.since_open.PTSs_out ++;
      Stream.video.since_reset.PTSs_out ++;
      dprintf (("IRQ", "ProcessVideoPictureHdr: New PTS received: %c%08X\n", Stream.decode_frames->PTS.msb ? '1' : '0', Stream.decode_frames->PTS.lsw));
    }
  }
  
  /* Set sensible values in case the picture extensions are absent */
  Stream.decode_frames->lastpic->intra_dc_precision = 0;  /**/
  Stream.decode_frames->lastpic->picture_structure = 0;   /**  these are the values the VID_PPR1/2 registers require in MPEG-1 mode */
  Stream.decode_frames->lastpic->picture_flags.byte = 0;  /**/
  Stream.decode_frames->top_field_first = true;
  Stream.decode_frames->repeat_first_field = false;
  Stream.decode_frames->progressive_frame = true;
  
  /* Get a new reconstruction frame pointer */
  if (Stream.decode_frames->lastpic->picture_coding_type == B_PICTURE)
  {
    Stream.decode_frames->frame_buffer = Stream.next_B_frame_buffer;
  }
  else
  {
    Stream.decode_frames->frame_buffer = Stream.next_anchor_frame_buffer;
  }
  
  /* Set up the predictor frame pointers (which depend upon picture_coding_type) */
  {
    frame_t *frame;
    int anchor_frames_found = 0;
    unsigned int BFP = SDRAM_FRAME_BUFFER_1_START/256; /* these are as good as anything for use as defaults */
    unsigned int FFP = SDRAM_FRAME_BUFFER_2_START/256; /* in the case of serious bitstream corruption!!     */
    for (frame = Stream.decode_frames->decode_link; frame != NULL; frame = frame->decode_link)
    {
      if (frame->lastpic->picture_coding_type != B_PICTURE)
      {
        anchor_frames_found++;
        /* For anchor frames, the most recent anchor frame is written to VID_FFP */
        if (Stream.decode_frames->lastpic->picture_coding_type != B_PICTURE && anchor_frames_found == 1) FFP = frame->frame_buffer;
        /* For B-frames, the most recent anchor frame is written to VID_BFP */
        if (Stream.decode_frames->lastpic->picture_coding_type == B_PICTURE && anchor_frames_found == 1) BFP = frame->frame_buffer;
        /* For B-frames, the second-most recent anchor frame is written to VID_FFP */
        if (Stream.decode_frames->lastpic->picture_coding_type == B_PICTURE && anchor_frames_found == 2) FFP = frame->frame_buffer;
      }
    }
    WRITE_VID_FFP(FFP); /* it seems that these writes will intermittently */
    WRITE_VID_BFP(BFP); /* fail if we do them in the opposite order!!     */
  }
  dprintf (("IRQ", "ProcessVideoPictureHdr: RFP = %04X, FFP = %04X, BFP = %04X\n", Stream.decode_frames->frame_buffer, VID_FFP & 0x3FFF, VID_BFP & 0x3FFF));
  
  /* Set decoding state flags */
  Stream.last_video_header_layer_decoded = lvhld_picture;
  Stream.vid_flags.sequence_header_received_since_picture_header = false;
  Stream.vid_flags.gop_header_received_since_picture_header = false;
}

/******	static_ProcessVideoPictureCodingExt() ******************************

Purpose:	Parses MPEG picture_coding_extension()

***************************************************************************/

static void static_ProcessVideoPictureCodingExt (void)
{
  dprintf (("IRQ", "ProcessVideoPictureCodingExt\n"));
  Stream.decode_frames->lastpic->forward_horizontal_f_code = static_GetVideoHdrBits (4);
  Stream.decode_frames->lastpic->forward_vertical_f_code = static_GetVideoHdrBits (4);
  Stream.decode_frames->lastpic->backward_horizontal_f_code = static_GetVideoHdrBits (4);
  Stream.decode_frames->lastpic->backward_vertical_f_code = static_GetVideoHdrBits (4);
  Stream.decode_frames->lastpic->intra_dc_precision = static_GetVideoHdrBits (2);
  Stream.decode_frames->lastpic->picture_structure = static_GetVideoHdrBits (2);
  Stream.decode_frames->lastpic->picture_flags.byte = static_GetVideoHdrBits (6);
  Stream.decode_frames->repeat_first_field = static_GetVideoHdrBits (1); /* used only for parsing picture_display_extension */
  static_GetVideoHdrBits (1); /* ignore chroma_420_type */
  Stream.decode_frames->progressive_frame = static_GetVideoHdrBits (1); /* used in WSS signal and deciding whether to freeze fields or frames */
  /* Ignore composite_display_flag, v_axis, field_sequence, sub_carrier, burst_amplitude and sub_carrier_phase */
}

/******	static_ProcessVideoQuantMatrixExt() ********************************

Purpose:	Parses MPEG quant_matrix_extension() or the quant matrix
		part of the sequence_header()

***************************************************************************/

static void static_ProcessVideoQuantMatrixExt (void)
{
  unsigned int coeff;   /* counter for loading quantization matrices */
  dprintf (("IRQ", "ProcessVideoQuantMatrixExt\n"));
  
  /* VERY IMPORTANT:                                                  */
  /* The data sheet is wrong, bit VID_HDS.QMI must be set for *intra* */
  /* quantizer matrices, and clear for *non-intra* quantizer matrices */
  
  if (static_GetVideoHdrBits (1) == 0)
  {
    /* Load default intra quantizer matrix (if necessary) */
    if (!Stream.vid_flags.default_intra_quantizer_matrix_loaded)
    {
      VID_HDS = VID_HDS_QMI; /* select intra table */
      for (coeff = 0; coeff < 64; coeff ++)
      {
        VID_QMW = default_intra_quantizer_matrix[coeff];
      }
      Stream.vid_flags.default_intra_quantizer_matrix_loaded = true;
    }
  }
  else
  {
    /* Load intra quantizer matrix from stream */
    VID_HDS = VID_HDS_QMI; /* select intra table */
    for (coeff = 0; coeff < 64; coeff ++)
    {
      VID_QMW = static_GetVideoHdrBits (8);
    }
    Stream.vid_flags.default_intra_quantizer_matrix_loaded = false;
  }
  if (static_GetVideoHdrBits (1) == 0)
  {
    /* Load default non-intra quantizer matrix (if necessary) */
    if (!Stream.vid_flags.default_non_intra_quantizer_matrix_loaded)
    {
      VID_HDS = 0; /* select non-intra table */
      for (coeff = 0; coeff < 64; coeff ++)
      {
        VID_QMW = 16; /* all coefficients of default non-intra matrix are 16 */
      }
      Stream.vid_flags.default_non_intra_quantizer_matrix_loaded = true;
    }
  }
  else
  {
    /* Load non-intra quantizer matrix from stream */
    VID_HDS = 0; /* select non-intra table */
    for (coeff = 0; coeff < 64; coeff ++)
    {
      VID_QMW = static_GetVideoHdrBits (8);
    }
    Stream.vid_flags.default_non_intra_quantizer_matrix_loaded = false;
  }
  
  /* Do nothing - if we're parsing sequence_header() then there's nothing else to do; if we're parsing quant_matrix_extension(), */
  /* then the following bits are chroma quant matrices, which will be skipped by the header search when we return. */
  /* Chroma quantizer matrices are not supported by the 3520, and are not required by Simple Profile or Main Profile. */
}

/******	static_ProcessVideoPictureDisplayExt() *****************************

Purpose:	Parses MPEG picture_display_extension()

***************************************************************************/

static void static_ProcessVideoPictureDisplayExt (void)
{
  unsigned char number_of_frame_centre_offsets;
  unsigned char offset_no;
  signed short horizontal_offset = 0;
  signed short vertical_offset = 0;
  dprintf (("IRQ", "ProcessVideoPictureDisplayExt\n"));
  /* Determine the number of frame centre offsets */
  if (Stream.decode_frames->seq.progressive_sequence == 1)
  {
    if (Stream.decode_frames->repeat_first_field == true)
    {
      if (Stream.decode_frames->lastpic->picture_flags.bits.top_field_first == 1)
      {
        number_of_frame_centre_offsets = 3;
      }
      else
      {
        number_of_frame_centre_offsets = 2;
      }
    }
    else
    {
      number_of_frame_centre_offsets = 1;
    }
  }
  else
  {
    if (Stream.decode_frames->lastpic->picture_structure == FRAME_PICTURE)
    {
      if (Stream.decode_frames->repeat_first_field == true)
      {
        number_of_frame_centre_offsets = 3;
      }
      else
      {
        number_of_frame_centre_offsets = 2;
      }
    }
    else
    {
      number_of_frame_centre_offsets = 1;
    }
  }
  
  /* Set up all three pairs of offsets, repeating the final coded pair if necessary, */
  /* in case the picture is displayed for longer than expected */
  for (offset_no = (Stream.next_picture_structure == ps_second_field_picture) ? 1 : 0; offset_no < 3; offset_no++)
  {
    if (offset_no < number_of_frame_centre_offsets)
    {
      horizontal_offset = (signed short) static_GetVideoHdrBits (16);
      static_GetVideoHdrBits (1); /* skip marker bit */
      vertical_offset = (signed short) static_GetVideoHdrBits (16);
      static_GetVideoHdrBits (1); /* skip marker bit */
    }
    Stream.decode_frames->frame_centre_horizontal_offset [offset_no] = horizontal_offset;
    Stream.decode_frames->frame_centre_vertical_offset [offset_no] = vertical_offset;
  }
}

/******	static_ProcessVideoUserData() **************************************

Purpose:	Parses MPEG user_data()
Out:		true if we've extracted an extra byte-aligned zero byte
		from the header FIFO

***************************************************************************/

static bool static_ProcessVideoUserData (void)
{
  /* This code is designed to expect DiviCom/C-Cube (closed captions) or DTG1 (AFD) format user data. */
  /* This is achieved by assuming DiviCom/C-Cube format, and treating DTG1 format as a special instance of it! */
  unsigned char dataLength;
  unsigned char dataType;
  unsigned char extDataType;
  bool first_chunk = true;
  bool DiviCom_format = true;
  dprintf (("IRQ", "ProcessVideoUserData\n"));
  do
  {
    dataLength = static_GetVideoHdrBits (8);
    if (dataLength == 0)
    {
      /* End of data - don't pull any more from header FIFO */
      return true;
    }
    dataType = static_GetVideoHdrBits (8);
    if (dataType == 0xFF)
    {
      extDataType = static_GetVideoHdrBits (8);
    }
    else
    {
      extDataType = 0xFF;
    }
    if (first_chunk)
    {
      /* Determine format of data */
      if (dataType == 0x54 && dataLength == 0x44)
      {
        /* This is highly unlikely to be used by DiviCom or C-Cube, but check the rest of the DTG1 magic word, to be sure */
        if (static_GetVideoHdrBits (16) == 0x4731)
        {
          /* It *is* DTG1 format */
          static_ProcessVideoDTG1UserData ();
        }
        /* If it wasn't DTG1 then we've messed things up by reading 16 bits. */
        /* If it was DTG1, then don't try to interpret any remaining user data. */
        /* Either way, we want to exit the function now. */
        return false;
      }
      else if ((dataType == 0x02 && dataLength == 0x05) ||
               (dataType == 0x04 && dataLength == 0x05) ||
               (dataType == 0x05 && dataLength == 0x01) ||
               (dataType == 0x06 && dataLength == 0x02) ||
               (dataType == 0x07 && dataLength == 0x01) ||
               (dataType == 0x08 && dataLength == 0x03) ||
               (dataType == 0x09 && dataLength == 0x02) ||
               (dataType == 0x09 && dataLength == 0x04) ||
               (dataType == 0x0A && dataLength == 0x02) ||
               (dataType == 0x0A && dataLength == 0x04))
      {
        /* Looks like DiviCom format */
        DiviCom_format = true;
      }
      else if ((dataType == 0x02 && dataLength == 0x06) ||
               (dataType == 0x04 && dataLength == 0x06) ||
               (dataType == 0x05 && dataLength == 0x02) ||
               (dataType == 0x06 && dataLength == 0x03) ||
               (dataType == 0x07 && dataLength == 0x02) ||
               (dataType == 0x08 && dataLength == 0x04) ||
               (dataType == 0x09 && dataLength == 0x03) ||
               (dataType == 0x0A && dataLength == 0x03))
      {
        /* Looks like C-Cube format */
        DiviCom_format = false;
      }
      else
      {
        /* Unknown format - just exit function (data will be skipped by the next header search) */
        return false;
      }
      first_chunk = false;
    }
    
    if (!DiviCom_format)
    {
      dataLength --; /* convert C-Cube to DiviCom format */
    }
    static_ProcessVideoDiviComUserData (dataLength, dataType, extDataType);
  }
  while (dataLength != 0);
  /* We won't actually get here */
  return true;
}

/******	static_ProcessVideoDTG1UserData() **********************************

Purpose:	Parses DTG1 format MPEG user_data()

***************************************************************************/

static void static_ProcessVideoDTG1UserData (void)
{
  bool active_format_flag;
  dprintf (("IRQ", "ProcessVideoDTG1UserData\n"));
  static_GetVideoHdrBits (1); /* skip zero bit */
  active_format_flag = static_GetVideoHdrBits (1);
  static_GetVideoHdrBits (6); /* skip reserved bits */
  if (active_format_flag == true)
  {
    static_GetVideoHdrBits (4); /* skip reserved bits */
    Stream.decode_frames->lastpic->active_format = static_GetVideoHdrBits (4);
  }
}

/******	static_ProcessVideoDiviComUserData() *******************************

Purpose:	Parses DiviCom format MPEG user_data()

***************************************************************************/

static void static_ProcessVideoDiviComUserData (unsigned char dataLength, unsigned char dataType, unsigned char extDataType)
{
  IGNORE(extDataType);
  dprintf (("IRQ", "ProcessVideoDiviComUserData\n"));
  if ((dataType == 0x09 || dataType == 0x0A) && (dataLength == 2 || dataLength == 4))
  {
    /* Closed captioning data! */
    char *buffer = (dataType == 0x09) ? Stream.decode_frames->CC_odd_data : Stream.decode_frames->CC_even_data;
    int index = 0;
    if (dataLength == 2 && (buffer[0] != 0 || buffer[1] != 0))
    {
      index = 2; /* cope with 3:2 pulldown data being split into two packets */
    }
    while (dataLength-- > 0)
    {
      buffer[index++] = static_GetVideoHdrBits (8);
    }
  }
  else
  {
    while (dataLength-- > 0)
    {
      static_GetVideoHdrBits (8); /* just skip the data for unsupported packets */
    }
  }
}

/******	static_ProcessVideoSliceHdr() **************************************

Purpose:	Deals with the completion of header parsing

***************************************************************************/

static void static_ProcessVideoSliceHdr (void)
{
  dprintf (("IRQ", "ProcessVideoSliceHdr\n"));
  
#ifdef DEBUGLIB
  {
    frame_t *frame = Stream.display_frames;
    int items = 0;
    dprintf (("IRQ", "ProcessVideoSliceHdr: display frames list = "));
    while (frame != NULL)
    {
      dprintf (("IRQ", "%d ", ((int) frame - (int) Stream.frame) / sizeof (frame_t)));
      frame = frame->display_link;
      if (++items > 10) break;
    }
  }
  dprintf (("IRQ", "\n"));
  {
    frame_t *frame = Stream.decode_frames;
    int items = 0;
    dprintf (("IRQ", "ProcessVideoSliceHdr: decode frames list = "));
    while (frame != NULL)
    {
      dprintf (("IRQ", "%d ", ((int) frame - (int) Stream.frame) / sizeof (frame_t)));
      frame = frame->decode_link;
      if (++items > 10) break;
    }
  }
  dprintf (("IRQ", "\n"));
#endif
  /* Add the frame into the display list (this depends upon the picture_coding_type too) */
  if (!Stream.decode_frames->on_display_list /* may be set if this is the second field picture, or if we lost data and re-used the struct */)
  {
    frame_t *frame_to_insert_after = NULL;
    frame_t *previous = NULL;
    frame_t *rover = Stream.display_frames;
    while (rover != NULL)
    {
      /* Anchor frames are added at the end of the list */
      if (Stream.decode_frames->lastpic->picture_coding_type != B_PICTURE && rover->display_link == NULL)
        frame_to_insert_after = rover;
      /* B-frames are added before the last anchor frame on the list */
      if (Stream.decode_frames->lastpic->picture_coding_type == B_PICTURE && rover->lastpic->picture_coding_type != B_PICTURE)
        frame_to_insert_after = previous;
      previous = rover;
      rover = rover->display_link;
    }
    if (frame_to_insert_after == NULL)
    {
      Stream.decode_frames->display_link = Stream.display_frames;
      Stream.display_frames = Stream.decode_frames;
    }
    else
    {
      Stream.decode_frames->display_link = frame_to_insert_after->display_link;
      frame_to_insert_after->display_link = Stream.decode_frames;
    }
    Stream.decode_frames->on_display_list = true;
  }
#ifdef DEBUGLIB
  {
    frame_t *frame = Stream.display_frames;
    int items = 0;
    const char *coding_names = "0IPB4567";
    dprintf (("IRQ", "ProcessVideoSliceHdr: display frames list = "));
    while (frame != NULL)
    {
      dprintf (("IRQ", "%d (%d,%c) ", ((int) frame - (int) Stream.frame) / sizeof (frame_t), frame->temporal_reference, coding_names[frame->pic1.picture_coding_type]));
      frame = frame->display_link;
      if (++items > 10) break;
    }
  }
  dprintf (("IRQ", "\n"));
#endif
  
  /* Set next_picture_structure and frame's top_field_first flag */
  if (Stream.decode_frames->lastpic->picture_structure == FRAME_PICTURE || Stream.decode_frames->lastpic->picture_structure == 0 /* MPEG-1 case */)
  {
    if (Stream.current_picture_structure == ps_first_field_picture)
    {
      /* We're obviously in the middle of some corrupt data - pretend we're a second-field picture to avoid data structures getting out of step */
      Stream.next_picture_structure = ps_second_field_picture;
      dprintf (("IRQ", "ProcessVideoSliceHdr: faked second_field_picture\n"));
    }
    else
    {
      Stream.next_picture_structure = ps_frame_picture;
      dprintf (("IRQ", "ProcessVideoSliceHdr: frame_picture\n"));
      Stream.decode_frames->top_field_first = Stream.decode_frames->lastpic->picture_flags.bits.top_field_first;
    }
  }
  else
  {
    switch (Stream.current_picture_structure)
    {
      case ps_first_field_picture:
        Stream.next_picture_structure = ps_second_field_picture;
        dprintf (("IRQ", "ProcessVideoSliceHdr: second_field_picture\n"));
        break;
      case ps_second_field_picture:
        Stream.next_picture_structure = ps_first_field_picture;
        dprintf (("IRQ", "ProcessVideoSliceHdr: first_field_picture\n"));
        /* Copy top_field_first from last frame (should be more resilient if an odd number of pictures are lost due to errors) */
        Stream.decode_frames->top_field_first = Stream.decode_frames->decode_link->top_field_first;
        break;
      case ps_frame_picture:
      case ps_frame_picture_extension:
        /* It's possible to switch between field and frame pictures within the same clip */
        Stream.next_picture_structure = ps_first_field_picture;
        dprintf (("IRQ", "ProcessVideoSliceHdr: first_field_picture\n"));
        /* The current field type must be used to determine top_field_first flag */
        Stream.decode_frames->top_field_first = (Stream.decode_frames->lastpic->picture_structure == TOP_FIELD_PICTURE);
        break;
    }
  }
  
  /* Deal with closed captioning */
  /* Unfortunately, since the closed captioning is tied to individual frames, we have to deal with it now,    */
  /* and descramble the decode order, just so that we don't lose data when we do a synchronisation frame skip */
  /* Only do this after parsing the headers for the last picture of each frame */
  /* To determine this, we use next_picture_structure as set up just above */
  if (Stream.next_picture_structure != ps_first_field_picture)
  {
    if (Stream.decode_frames->lastpic->picture_coding_type != B_PICTURE)
    {
      if (Stream.previous_anchor_frame != NULL)
      {
        static_ProcessCCData (1, 0, Stream.previous_anchor_frame->CC_odd_data);
        static_ProcessCCData (2, 1, Stream.previous_anchor_frame->CC_even_data);
      }
      Stream.previous_anchor_frame = Stream.decode_frames;
    }
    else
    {
      static_ProcessCCData (1, 0, Stream.decode_frames->CC_odd_data);
      static_ProcessCCData (2, 1, Stream.decode_frames->CC_even_data);
    }
  }
  
  /* Calculate the video buffer prefill threshold if we haven't done so already */
  if (VideoPrefillSize == -1)
  {
    VideoPrefillSize = (Stream.decode_frames->seq.bit_rate * Stream.decode_frames->lastpic->vbv_delay) / (2048 * 90000 / 400);
    /* The multiplication should be 32-bit safe, because even the 15Mbit/s theoretical maximum rate of */
    /* the 3520 equates to &9999 (16 bits) * 400bits/s, and vbv_delay is also a 16 bit value.          */
    /* Now cap the value by vbv_buffer_size, in case one of the above values was silly */
    VideoPrefillSize = MIN(VideoPrefillSize, Stream.decode_frames->seq.vbv_buffer_size * (16 * 1024 / 2048));
    /* Now cap the value by the size of our allocated bit buffer */
    VideoPrefillSize = MIN(VideoPrefillSize, (SDRAM_VIDEO_BIT_BUFFER_END - SDRAM_VIDEO_BIT_BUFFER_START - 256) / 256);
    VideoPrefillSize = MAX(VideoPrefillSize, 200);
    WRITE_VID_VBT(VideoPrefillSize);
  }
  
  if (Stream.video.state == stream_prefilling && Stream.video.prefill_state == ps_initialising)
  {
    Stream.video.prefill_state = ps_pre_decoding;
    dprintf (("IRQ", "ProcessVideoSliceHdr: video.prefill_state = pre_decoding\n"));
  }
  Stream.vid_flags.next_frames_headers_have_been_decoded = true;
  if (Stream.vid_flags.sequence_headers_being_parsed)
  {
    Stream.vid_flags.sequence_headers_being_parsed = false;
    Stream.vid_flags.sequence_headers_locked = true;
  }
  static_PerhapsWriteNewInstruction ();
}

/******	static_GetVideoHdrBits() *******************************************

Purpose:	Fetches the next few bits from the video header FIFO
In:		Number of bits to fetch (up to 32, may cross byte/word boundaries)
		or 0 to discard cached data and force a fresh data fetch next time
		or 0xFF to align to a byte boundary
Out:		Value of bits (unused bits are zero)
Notes:		The coroutine can exited through this function

***************************************************************************/

static unsigned int static_GetVideoHdrBits (unsigned char bits)
{
  static unsigned short fifo; /* 16 bits most recently read from the FIFO */
  static unsigned char cached_bits = 0; /* the number of bits from |fifo| that have not yet been returned */
  unsigned int data = 0; /* for building up the return value */
  unsigned char bits_this_time;
  if (bits == 0)
  {
    cached_bits = 0;
  }
  else if (bits == 0xFF)
  {
    cached_bits &= 8; /* align to byte boundary */
  }
  else
  {
    while (bits > 0)
    {
      if (cached_bits == 0)
      {
        if ((VID_STA_0 & (VIDEO_INT_HFE>>0)) != 0)
        {
          VID_ITM_8 |= VIDEO_INT_HFF>>8;  /* enable header fifo full IRQ */
          Co_SwitchTo (Co_MainRoutine, NULL);
        }
        READ_VID_HDF(fifo);
        cached_bits = 16;
//        dprintf (("IRQ", "GetVideoHdrBits: data = %02X %02X\n", fifo >> 8, fifo & 0xFF));
#if Debugging==1 && DebugVideoHeaderFIFO==1
        if (DADebug_WriteC != NULL)
        {
          DADebug_WriteC (fifo >> 8);
          DADebug_WriteC (fifo & 0xFF);
        }
#endif
      }
      bits_this_time = MIN(bits, cached_bits);
      data <<= bits_this_time;
      data |= (fifo >> (cached_bits - bits_this_time)) & ((1 << bits_this_time) - 1);
      bits -= bits_this_time;
      cached_bits -= bits_this_time;
    }
  }
  return data;
}

/******	static_PerhapsWriteNewInstruction() ********************************

Purpose:	May or may not write the next task instruction, depending
		upon whether all of the appropriate conditions have been
		met. Typically called whenever we suspect that one of the
		conditions has become satisfied, in case it was the last one
		we were waiting for.

***************************************************************************/

static void static_PerhapsWriteNewInstruction (void)
{
  bool time_to_write_instruction = false;
  bool use_FIS_bit = false;
  dprintf (("IRQ", "PerhapsWriteNewInstruction\n"));
  dprintf (("IRQ", "PerhapsWriteNewInstruction: VBL = %d bytes, VBT = %d bytes\n", (VID_VBL & 0x3FFF) * 256, (VID_VBT & 0x3FFF) * 256));
  if (Stream.vid_flags.next_frames_headers_have_been_decoded &&
      Stream.vid_flags.this_frames_data_have_been_decoded &&
      (VID_VBL & 0x3FFF) >= (VID_VBT & 0x3FFF))
  {
    if (Stream.video.state == stream_prefilling || (Stream.decode_frames->lastpic->picture_coding_type == B_PICTURE && Stream.video.fast_slow_state == fss_next_frame_fast))
    {
      time_to_write_instruction = true;
      use_FIS_bit = true;
    }
    else
    {
      if ((Stream.next_picture_structure != ps_first_field_picture || Stream.current_picture_structure == ps_frame_picture) && Stream.fields_till_next_AU == 1 ||
          (Stream.next_picture_structure == ps_first_field_picture && Stream.current_picture_structure != ps_frame_picture) && Stream.fields_into_this_PU == Stream.field_to_trigger_second_decode_on)
      {
        time_to_write_instruction = true;
      }
    }
  }
  
  if (time_to_write_instruction)
  {
    /* Check to see if we've just finished decoding the first or only picture of the prefill anchor frames */
    if (Stream.video.state == stream_prefilling && Stream.next_picture_structure != ps_first_field_picture && Stream.vid_flags.last_instruction_was_a_decode)
    {
      Stream.vid_flags.prefill_anchor_frames_decoded ++;
      dprintf (("IRQ", "PerhapsWriteNewInstruction: prefill_anchor_frames_decoded = %d\n", Stream.vid_flags.prefill_anchor_frames_decoded));
    }
    /* Are we ready to start display (discounting issues about synchronisation and field polarity) ? */
    if (Stream.video.state == stream_prefilling &&
        (Stream.vid_flags.prefill_anchor_frames_decoded == 2 ||
         (Stream.vid_flags.prefill_anchor_frames_decoded == 1 && Stream.rs_flags.trick_play_mode)))
    {
      /* Update state machine */
      Stream.video.prefill_state = ps_waiting_for_play_command;
      dprintf (("IRQ", "PerhapsWriteNewInstruction: video.prefill_state = waiting_for_play_command\n"));
      /* Enable vsync interrupts now - but don't take any action on them until we receive the play command.         */
      /* This is necessary so that we can clear the relevant bits in VID_ITS - otherwise, if it takes a long time   */
      /* for audio prefilling to complete, then the first interrupt will have both VID_ITS.VST and VID_ITS.VSB set! */
      /* Also enable the video underrun interrupt, to count any video underruns - hopefully there will be none.     */
      /* (We always get a couple of underruns during initial header parsing, so it's silly to count them.)          */
      VID_ITM_0 |= (VIDEO_INT_VSB>>0 | VIDEO_INT_VST>>0 | VIDEO_INT_BBE>>0);
      /* Tell control unit that we've finished - we may be re-entered via SWI MPEGVideo_Play at this point */
      _swix (MPEGControl_Play, _INR(0,1), StreamFlags_VideoPresent, Stream.csh);
    }
    else
    {
      if (Stream.current_picture_structure == ps_frame_picture && Stream.next_picture_structure != ps_frame_picture)
      {
        /* Moving into field-based video, we need to update display, but hold off decode by one field */
        /* Next time we trigger on fields_into_this_PU = 0, instead of fields_till_next_AU = 1 */
        Stream.fields_into_this_PU = -1;
        Stream.field_to_trigger_second_decode_on = 0;
        Stream.prev_picture_structure = Stream.current_picture_structure;
        Stream.current_picture_structure = ps_frame_picture_extension;
        Stream.vid_flags.final_instruction_has_been_written_during_this_AU = true; /* pretend it was an instruction */
        static_SetUpDisplayForNewAU ();
      }
      else
      {
        if (Stream.decode_frames->lastpic->picture_coding_type == B_PICTURE && (Stream.video.state == stream_prefilling || Stream.video.fast_slow_state == fss_next_frame_fast))
        {
          static_WriteSkipInstruction (use_FIS_bit);
        }
        else
        {
          static_SetUpDisplayForNewAU ();
          static_WriteDecodeInstruction (use_FIS_bit);
          /* Enable decoding if necessary */
          if (Stream.vid_flags.prefill_anchor_frames_decoded == 0)
          {
            dprintf (("IRQ", "PerhapsWriteNewInstruction: enabling decoding\n"));
            VID_CTL |= VID_CTL_EDC;
            VID_ITM_0 |= VIDEO_INT_PSD>>0; /* enable pipeline starting to decode video IRQ */
          }
        }
        /* Set state flags */
        Stream.vid_flags.next_frames_headers_have_been_decoded = false;
        Stream.vid_flags.this_frames_data_have_been_decoded = false;
        Stream.vid_flags.started_parsing_headers = false;
      }
    }
  }
}

/******	static_SetUpDisplayForNewAU() **************************************

Purpose:	Changes the display parameters to suit the next access unit (not presentation unit)

***************************************************************************/

static void static_SetUpDisplayForNewAU (void)
{
  dprintf (("IRQ", "SetUpDisplayForNewAU\n"));
  if (Stream.video.state == stream_open || Stream.video.state == stream_closing)
  {
    /* Advance the display frame, but not when initiating any second decode within a PU */
    /* Also do it at the start of a frame_picture_extension (but not at the end of it) */
    if (Stream.next_picture_structure == ps_second_field_picture ||
        (Stream.next_picture_structure == ps_frame_picture && Stream.current_picture_structure == ps_frame_picture) ||
        (Stream.current_picture_structure == ps_frame_picture_extension && Stream.fields_into_this_PU == -1))
    {
      /* Before we remove the struct for the currently displaying frame, check to see if anything   */
      /* is different in the new frame that would require the display parameters to be recalculated */
      if (!Stream.vid_flags.display_needs_setting_up) /* if already true, there's no point in checking */
      {
        if (Stream.display_frames->display_link->seq.coded_horizontal_size          != Stream.display_frames->seq.coded_horizontal_size          ||
            Stream.display_frames->display_link->seq.coded_vertical_size            != Stream.display_frames->seq.coded_vertical_size            ||
            Stream.display_frames->display_link->seq.display_horizontal_size        != Stream.display_frames->seq.display_horizontal_size        ||
            Stream.display_frames->display_link->seq.display_vertical_size          != Stream.display_frames->seq.display_vertical_size          ||
            Stream.display_frames->display_link->seq.aspect_ratio_information       != Stream.display_frames->seq.aspect_ratio_information       ||
            Stream.display_frames->display_link->progressive_frame                  != Stream.display_frames->progressive_frame                  ||
            Stream.display_frames->display_link->frame_centre_horizontal_offset [0] != Stream.display_frames->frame_centre_horizontal_offset [2] ||
            Stream.display_frames->display_link->frame_centre_vertical_offset [0]   != Stream.display_frames->frame_centre_vertical_offset [2]   ||
            Stream.display_frames->display_link->pic1.active_format                 != Stream.display_frames->lastpic->active_format             )
        {
          Stream.vid_flags.display_needs_setting_up = true;
        }
      }
      /* Remove the previously displayed frame from the display list (but not if there wasn't a previously decoded frame!) */
      if (Stream.vid_flags.first_frame_displayed == true)
      {
        dprintf (("IRQ", "SetUpDisplayForNewAU: removing frame %d from display list\n", ((int) Stream.display_frames - (int) Stream.frame) / sizeof (frame_t)));
        static_RemoveFrameFromList (Stream.display_frames, false);
      }
      dprintf (("IRQ", "SetUpDisplayForNewAU: about to display frame %d\n", ((int) Stream.display_frames - (int) Stream.frame) / sizeof (frame_t)));
      if (Stream.vid_flags.display_needs_setting_up)
      {
        static_PositionVideo (Stream.display_frames->frame_centre_horizontal_offset [0], Stream.display_frames->frame_centre_vertical_offset [0], Stream.display_frames->pic1.active_format);
      }
      static_SetDFP ();
      /* Set state flags/variables */
      Stream.fields_into_this_PU = -1; /* start an incrementing count of how many fields the frame has been displayed for */
      Stream.field_to_trigger_second_decode_on = 0; /* default, used unless first decode stalls */
      Stream.vid_flags.display_idle = false;
    }
    if (Stream.next_picture_structure == ps_frame_picture || (Stream.current_picture_structure == ps_frame_picture_extension && Stream.fields_into_this_PU == -1))
    {
      /* Always display a frame's first field at the first VSync */
      /* However, if the frame decode followed a field decode, then we're just coming up to the second (or fourth, sixth etc.) VSync of the PU */
      /* so we may need to display the opposite field (but only for progressive_frames or when in normal-speed play) */
      if (Stream.current_picture_structure != ps_frame_picture &&
          (Stream.display_frames->progressive_frame || (Stream.video.speed_indicator == 1 && Stream.rs_flags.trick_play_mode == false & Stream.vid_flags.up_scaling_used == false)))
      {
        static_SetNextDisplayField (!Stream.display_frames->top_field_first);
      }
      else
      {
        static_SetNextDisplayField (Stream.display_frames->top_field_first);
      }
    }
    else
    {
      /* When decoding a field, always display the field of the opposite polarity */
      static_SetNextDisplayField (Stream.decode_frames->lastpic->picture_structure == BOTTOM_FIELD_PICTURE);
    }
  }
}

/******	static_WriteDecodeInstruction() ************************************

Purpose:	Unconditionally writes the next task instruction, as a decode task
In:		Whether to set the FIS bit, to cause the instruction to be executed immediately

***************************************************************************/

static void static_WriteDecodeInstruction (bool FIS)
{
  unsigned char picture_coding_type;
  dprintf (("IRQ", "WriteDecodeInstruction%s\n", FIS ? ", FIS" : ""));
  /* debug-only code for stopping decode after a fixed number of pictures
  {
    static int count = 12;
    if (count-- == 0)
    {
      WRITE_VID_ITM(0);
      return;
    }
  } */
  
  /* Defer writing RFP until now, so that there's no chance that after the stream is closed, the 3520 is left with a   */
  /* combination of RFP and picture_coding_type that contradicts the strict B-frame / anchor-frame buffer assignments. */
  /* (Otherwise the 3520's internal B-frame pointers can be corrupted when the sequence header search is launched for the next stream.) */
  WRITE_VID_RFP(Stream.decode_frames->frame_buffer);
  
  /* Attempt at bitstream error resilience: if picture_coding_type is an illegal value, assume it's really an I-picture */
  picture_coding_type = Stream.decode_frames->lastpic->picture_coding_type;
  if (picture_coding_type < I_PICTURE || picture_coding_type > B_PICTURE) picture_coding_type = I_PICTURE;
  
  VID_PFH = (Stream.decode_frames->lastpic->backward_horizontal_f_code << 4) | Stream.decode_frames->lastpic->forward_horizontal_f_code;
  VID_PFV = (Stream.decode_frames->lastpic->backward_vertical_f_code << 4) | Stream.decode_frames->lastpic->forward_vertical_f_code;
  VID_PPR1 = (picture_coding_type << 4) | (Stream.decode_frames->lastpic->intra_dc_precision << 2) | Stream.decode_frames->lastpic->picture_structure;
  VID_PPR2 = Stream.decode_frames->lastpic->picture_flags.byte;
  if (Stream.video.fast_slow_state == fss_this_frame_fast)
  {
    /* When recovering from a frame skip, we have to kick the decoder after the following VSync to get it to start again! */
    Deferred_VID_TIS = (Stream.decode_frames->seq.mpeg2 ? VID_TIS_MP2 : 0) | ((Stream.next_picture_structure != ps_frame_picture || FIS) ? 0 : VID_TIS_RPT) | VID_TIS_FIS | VID_TIS_EXE;
  }
  else
  {
    if (Stream.rs_flags.trick_play_mode)
    {
      /* Make sure that the pipeline really is idle - seems to help enormously with trick play streams, */
      /* but unfortunately severely hinders error recovery for non-trick play streams. No idea why.     */
      STi3520L_PipelineReset ();
    }
    VID_TIS = (Stream.decode_frames->seq.mpeg2 ? VID_TIS_MP2 : 0) | ((Stream.next_picture_structure != ps_frame_picture || FIS) ? 0 : VID_TIS_RPT) | (FIS ? VID_TIS_FIS : 0) | VID_TIS_EXE;
  }
  Stream.vid_flags.last_instruction_was_a_decode = true;
  
  if (Stream.next_picture_structure != ps_second_field_picture)
  {
    if (Stream.decode_frames->decode_link != NULL) /* exempt the first frame, since there's no previous in this case! */
    {
      /* If we just finished decoding a B-frame, remove it from the decoding list, since it's never going to be used as a predictor, and its frame_centre_offsets have already been copied */
      if (Stream.decode_frames->decode_link->lastpic->picture_coding_type == B_PICTURE)
      {
        dprintf (("IRQ", "WriteDecodeInstruction: removing B-frame %d from decode list\n", ((int) Stream.decode_frames->decode_link - (int) Stream.frame) / sizeof (frame_t)));
        static_RemoveFrameFromList (Stream.decode_frames->decode_link, true);
      }
      /* If we are just about to decode an anchor frame, remove the third-last anchor frame from the decoding list, since it's no longer needed */
      if (Stream.decode_frames->lastpic->picture_coding_type != B_PICTURE)
      {
        frame_t *frame;
        frame_t *next_frame; /* because we are removing a frame from the list, we have to work this out first */
        int anchor_frames_found = 0;
        for (frame = Stream.decode_frames->decode_link /* first on the list is definitely an anchor frame */; frame != NULL; frame = next_frame)
        {
          next_frame = frame->decode_link;
          if (frame->lastpic->picture_coding_type != B_PICTURE)
          {
            anchor_frames_found++;
            if (anchor_frames_found >= 2)
            {
              dprintf (("IRQ", "WriteDecodeInstruction: removing anchor frame %d from decode list\n", ((int) frame - (int) Stream.frame) / sizeof (frame_t)));
              static_RemoveFrameFromList (frame, true);
            }
          }
        }
      }
    }
  }
  if (Stream.next_picture_structure != ps_first_field_picture)
  {
    /* Update appropriate next-frame_buffer pointer */
    if (Stream.decode_frames->lastpic->picture_coding_type == B_PICTURE)
    {
      Stream.next_B_frame_buffer ^= (SDRAM_FRAME_BUFFER_4_START/256 ^ SDRAM_FRAME_BUFFER_5_START/256); /* toggle B-frame storage */
    }
    else
    {
      Stream.next_anchor_frame_buffer += (SDRAM_FRAME_BUFFER_2_START/256 - SDRAM_FRAME_BUFFER_1_START/256); /* cycle anchor frames between buffers 1, 2 and 3 */
      if (Stream.next_anchor_frame_buffer == SDRAM_FRAME_BUFFER_4_START/256) Stream.next_anchor_frame_buffer = SDRAM_FRAME_BUFFER_1_START/256; /* loop back to buffer 1 */
    }
  }
  
  /* Set state flags/variables */
  Stream.vid_flags.final_instruction_has_been_written_during_this_AU = true;
  Stream.prev_picture_structure = Stream.current_picture_structure;
  Stream.current_picture_structure = Stream.next_picture_structure;
}

/******	static_WriteSkipInstruction() **************************************

Purpose:	Unconditionally writes the next task instruction, as a skip task
In:		Whether to set the FIS bit, to cause the instruction to be executed immediately

***************************************************************************/

static void static_WriteSkipInstruction (bool FIS)
{
  dprintf (("IRQ", "WriteSkipInstruction%s\n", FIS ? ", FIS" : ""));
  VID_TIS = (3 << VID_TIS_SKP_SHIFT) | (FIS ? VID_TIS_FIS : 0) | VID_TIS_EXE; 
  Stream.vid_flags.last_instruction_was_a_decode = false;
  Stream.prev_picture_structure = Stream.current_picture_structure;
  Stream.current_picture_structure = Stream.next_picture_structure;
}

/******	static_RemoveFrameFromList() ***************************************

Purpose:	Removes a frame struct from the decode or display frames lists;
		if it is no longer on either, then places it on the unused list
In:		Pointer to frame struct to remove; which list to remove it from

***************************************************************************/

static void static_RemoveFrameFromList (frame_t *removed_frame, bool D_list)
{
  frame_t *previous = NULL;
  frame_t *rover;
  if (D_list)
  {
    rover = Stream.decode_frames;
    while (rover != removed_frame && rover != NULL)
    {
      previous = rover;
      rover = rover->decode_link;
    }
    if (rover == NULL)
    {
      dprintf (("IRQ", "RemoveFrameFromList: WARNING - attempt to remove a frame from a list it wasn't on\n"));
      return; /* should never happen if I get the rest of the code right! */
    }
    if (previous == NULL) Stream.decode_frames = removed_frame->decode_link;
    else
    {
      previous->decode_link = removed_frame->decode_link;
    }
    removed_frame->decode_link = NULL;
    removed_frame->on_decode_list = false;
#ifdef DEBUGLIB
    {
      frame_t *frame = Stream.decode_frames;
      int items = 0;
      dprintf (("IRQ", "RemoveFrameFromList: decode frames list = "));
      while (frame != NULL)
      {
        dprintf (("IRQ", "%d ", ((int) frame - (int) Stream.frame) / sizeof (frame_t)));
        frame = frame->decode_link;
        if (++items > 10) break;
      }
    }
    dprintf (("IRQ", "\n"));
#endif
  }
  else
  {
    rover = Stream.display_frames;
    while (rover != removed_frame && rover != NULL)
    {
      previous = rover;
      rover = rover->display_link;
    }
    if (rover == NULL)
    {
      dprintf (("IRQ", "RemoveFrameFromList: WARNING - attempt to remove a frame from a list it wasn't on\n"));
      return; /* should never happen if I get the rest of the code right! */
    }
    if (previous == NULL) Stream.display_frames = removed_frame->display_link;
    else previous->display_link = removed_frame->display_link;
    removed_frame->display_link = NULL;
    removed_frame->on_display_list = false;
#ifdef DEBUGLIB
    {
      frame_t *frame = Stream.display_frames;
      int items = 0;
      dprintf (("IRQ", "RemoveFrameFromList: display frames list = "));
      while (frame != NULL)
      {
        dprintf (("IRQ", "%d ", ((int) frame - (int) Stream.frame) / sizeof (frame_t)));
        frame = frame->display_link;
        if (++items > 10) break;
      }
    }
    dprintf (("IRQ", "\n"));
#endif
  }
  
  if (!removed_frame->on_decode_list && !removed_frame->on_display_list)
  {
    /* Move frame struct to unused list */
    dprintf (("IRQ", "RemoveFrameFromList: returning frame %d to unused list\n", ((int) removed_frame - (int) Stream.frame) / sizeof (frame_t)));
    removed_frame->decode_link = Stream.unused_frames;
    Stream.unused_frames = removed_frame;
  }
}

/******	static_SetNextDisplayField() ***************************************

Purpose:	Sets up which field (top or bottom) is displayed,
		independently of the vertical filter configuration
In:		Top (or not-bottom) field flag
Notes:		Setting is not latched until the next VSync

***************************************************************************/

static void static_SetNextDisplayField (bool top_field)
{
  static const unsigned char dam_fld[7][2] =
  {
    {
      0 * (VID_DCF_FLD>>8) | 6 << (VID_DCF_DAM_SHIFT-8),
      1 * (VID_DCF_FLD>>8) | 6 << (VID_DCF_DAM_SHIFT-8)
    },
    {
      0 * (VID_DCF_FLD>>8) | 2 << (VID_DCF_DAM_SHIFT-8),
      1 * (VID_DCF_FLD>>8) | 2 << (VID_DCF_DAM_SHIFT-8)
    },
    {
      0 * (VID_DCF_FLD>>8) | 3 << (VID_DCF_DAM_SHIFT-8),
      1 * (VID_DCF_FLD>>8) | 7 << (VID_DCF_DAM_SHIFT-8)
    },
    {
      0 * (VID_DCF_FLD>>8) | 3 << (VID_DCF_DAM_SHIFT-8),
      1 * (VID_DCF_FLD>>8) | 3 << (VID_DCF_DAM_SHIFT-8)
    },
    {
      0 * (VID_DCF_FLD>>8) | 4 << (VID_DCF_DAM_SHIFT-8),
      0 * (VID_DCF_FLD>>8) | 4 << (VID_DCF_DAM_SHIFT-8)
    },
    {
      0 * (VID_DCF_FLD>>8) | 0 << (VID_DCF_DAM_SHIFT-8),
      0 * (VID_DCF_FLD>>8) | 0 << (VID_DCF_DAM_SHIFT-8)
    },
    {
      0 * (VID_DCF_FLD>>8) | 0 << (VID_DCF_DAM_SHIFT-8),
      1 * (VID_DCF_FLD>>8) | 0 << (VID_DCF_DAM_SHIFT-8)
    }
  };
  dprintf (("IRQ", "SetNextDisplayField: %s\n", top_field ? "top" : "bottom"));
  VID_DCF_8 = VID_DCF_8 & ~0x0F | dam_fld [VID_DCF_0 & 0x7] [top_field == false];
}

/******	static_PositionVideo() *********************************************

Purpose:	Sets up all video scaling/positioning parameters
In:		Encoded frame centre offsets; active format value

***************************************************************************/

static void static_PositionVideo (signed short frame_centre_horizontal_offset, signed short frame_centre_vertical_offset, unsigned char active_format)
{
  signed int centre_x /* in 256ths of a luma sample */;
  signed int centre_y /* in 256ths of a field scanline (down from top) */;
  fixed horizontal_downscaling;
  fixed vertical_upscaling;
  unsigned int source_width;  /* width of the coded area we're interested in (in samples) */
  unsigned int source_height; /* height of the coded area we're interested int (in samples) */
  fixed source_aspect = 0;    /* overall aspect ratio of the area we're interested in - only used when aspect preservation *is* required */
  unsigned int ideal_dest_width = 0; /* the post-scaling width we're aiming for (in luma samples or 720ths of the TV tube width) - only used when aspect preservation is *not* required */
  unsigned int ideal_dest_height;    /* the post-scaling height we're aiming for (in frame scanlines, or 576ths (PAL) / 480ths (NTSC) of the TV tube height) */
  bool preserve_aspect_ratio;      /* useful flag */
  bool using_pan_and_scan_vectors; /* useful flag */
  bool allow_tv [tvm_LIMIT]; /* intermediate flags, used to determine allowed items in scaling_mode[] below */
  static struct /* contains all the precalculated details needed to select the best combination of STB / TV scaling for a particular clip */
  {
    const fixed total_factor;
    const vertical_scaling_factor stb_factor;
    const tv_mode tv_factor;
    bool allowed;
  }
  scaling_mode [vsf_LIMIT * tvm_LIMIT] =
  {                                         /* factor */
    { 0x51111, vsf_x4,    tvm_16_9       }, /* 5.3333 */
    { 0x46EEF, vsf_x4,    tvm_14_9       }, /* 4.6667 */
    { 0x3CCCD, vsf_x4,    tvm_full_frame }, /* 4      */
    { 0x3CCCD, vsf_x4,    tvm_anamorphic }, /* 4      */
    { 0x2D99A, vsf_x4,    tvm_collapsed  }, /* 3      */
    { 0x28889, vsf_x2,    tvm_16_9       }, /* 2.6667 */
    { 0x23777, vsf_x2,    tvm_14_9       }, /* 2.3333 */
    { 0x1E666, vsf_x2,    tvm_full_frame }, /* 2      */
    { 0x1E666, vsf_x2,    tvm_anamorphic }, /* 2      */
    { 0x1E666, vsf_x1_5,  tvm_16_9       }, /* 2      */
    { 0x1A99A, vsf_x1_5,  tvm_14_9       }, /* 1.75   */
    { 0x16CCD, vsf_x2,    tvm_collapsed  }, /* 1.5    */
    { 0x16CCD, vsf_x1_5,  tvm_full_frame }, /* 1.5    */
    { 0x16CCD, vsf_x1_5,  tvm_anamorphic }, /* 1.5    */
    { 0x14444, vsf_x1,    tvm_16_9       }, /* 1.3333 */
    { 0x11BBC, vsf_x1,    tvm_14_9       }, /* 1.1667 */
    { 0x1119A, vsf_x1_5,  tvm_collapsed  }, /* 1.125  */
    { 0x0F333, vsf_x1,    tvm_full_frame }, /* 1      */
    { 0x0F333, vsf_x1,    tvm_anamorphic }, /* 1      */
    { 0x0F333, vsf_x0_75, tvm_16_9       }, /* 1      */
    { 0x0D4CD, vsf_x0_75, tvm_14_9       }, /* 0.875  */
    { 0x0B666, vsf_x1,    tvm_collapsed  }, /* 0.75   */
    { 0x0B666, vsf_x0_75, tvm_full_frame }, /* 0.75   */
    { 0x0B666, vsf_x0_75, tvm_anamorphic }, /* 0.75   */
    { 0x0A222, vsf_x0_5,  tvm_16_9       }, /* 0.6667 */
    { 0x08DDE, vsf_x0_5,  tvm_14_9       }, /* 0.5833 */
    { 0x088CD, vsf_x0_75, tvm_collapsed  }, /* 0.5625 */
    { 0x0799A, vsf_x0_5,  tvm_full_frame }, /* 0.5    */
    { 0x0799A, vsf_x0_5,  tvm_anamorphic }, /* 0.5    */
    { 0x05B33, vsf_x0_5,  tvm_collapsed  }  /* 0.375  */
  };
  unsigned int first_allowed_scaling_mode = -1; /* index into scaling_mode[] of first allowed mode */
  unsigned int last_allowed_scaling_mode = -1;  /* index into scaling_mode[] of last allowed mode */
  unsigned int mode;                            /* counter; later, the index of the selected mode */
  unsigned char group_settings [4] = { 0xFF, 0xFF, 0xFF, 0xFF };
  scart_feature scart_setting;
  
  dprintf (("IRQ", "PositionVideo\n"));
  
  /* Calculate the active area and protected area */
  if (active_formats[active_format].active == (fixed) 0)
  {
    /* Active format is as-the-coded-frame */
    Stream.active_aspect_ratio = Stream.protected_aspect_ratio = Stream.display_frames->seq.coded_aspect_ratio;
    Stream.active_area_is_at_top = false;
    Stream.active_horizontal_size = Stream.protected_horizontal_size = Stream.display_frames->seq.coded_horizontal_size;
    Stream.active_vertical_size = Stream.protected_vertical_size = Stream.display_frames->seq.coded_vertical_size;
  }
  else
  {
    fixed ratio_of_ratios;
    Stream.active_aspect_ratio = active_formats[active_format].active;
    Stream.protected_aspect_ratio = active_formats[active_format].protect;
    Stream.active_area_is_at_top = active_formats[active_format].top;
    ratio_of_ratios = Fixed_FixedDivide (Stream.active_aspect_ratio, Stream.display_frames->seq.coded_aspect_ratio);
    if (ratio_of_ratios > 0x10000)
    {
      /* Wider aspect: keep same width, but use reduced height */
      Stream.active_horizontal_size = Stream.display_frames->seq.coded_horizontal_size;
      Stream.active_vertical_size = Fixed_FixedDivide (Stream.display_frames->seq.coded_vertical_size << 16, ratio_of_ratios) >> 16;
    }
    else
    {
      /* Narrower aspect: keep same height, but use reduced width */
      Stream.active_horizontal_size = (Stream.display_frames->seq.coded_horizontal_size * ratio_of_ratios) >> 16;
      Stream.active_vertical_size = Stream.display_frames->seq.coded_vertical_size;
    }
    ratio_of_ratios = Fixed_FixedDivide (Stream.protected_aspect_ratio, Stream.active_aspect_ratio);
    if (ratio_of_ratios > 0x10000)
    {
      /* Wider aspect: keep same width, but use reduced height */
      Stream.protected_horizontal_size = Stream.active_horizontal_size;
      Stream.protected_vertical_size = Fixed_FixedDivide (Stream.active_vertical_size << 16, ratio_of_ratios) >> 16;
    }
    else
    {
      /* Narrower aspect: keep same height, but use reduced width */
      Stream.protected_horizontal_size = (Stream.active_horizontal_size * ratio_of_ratios) >> 16;
      Stream.protected_vertical_size = Stream.active_vertical_size;
    }
  }
  
  /* Calculate the required horizontal and vertical scaling for the given ScalingType */
  if (Stream.video_parms.scaling_type.type == st_free && Stream.video_parms.scaling_type.param.free.preference == vm_zoom && active_format == 8 /* ie not specified */ &&
      ((Stream.video_parms.scaling_type.param.free.ratio == tv_regular && Stream.display_frames->seq.display_aspect_ratio >= 0x15555) ||
       (Stream.video_parms.scaling_type.param.free.ratio == tv_widescreen && Stream.display_frames->seq.display_aspect_ratio >= 0x1C71C)))
  {
    using_pan_and_scan_vectors = true;
    source_width = Stream.display_frames->seq.display_horizontal_size;
    source_height = Stream.display_frames->seq.display_vertical_size;
    source_aspect = Stream.display_frames->seq.display_aspect_ratio;
    ideal_dest_height = (Module_Display525_60 ? 480 : 576);
    preserve_aspect_ratio = true;
  }
  else
  {
    using_pan_and_scan_vectors = false;
    switch (Stream.video_parms.scaling_type.type)
    {
      default:
      case st_free:
        switch (Stream.video_parms.scaling_type.param.free.preference)
        {
          default:
          case vm_box:
            source_width = Stream.protected_horizontal_size;
            source_height = Stream.protected_vertical_size;
            source_aspect = Stream.protected_aspect_ratio;
            if (source_aspect == 0) source_aspect = 0x15555;
            if (source_aspect > (Stream.video_parms.scaling_type.param.free.ratio == tv_widescreen ? 0x1C71C : 0x15555))
            {
              ideal_dest_height = (Module_Display525_60 ? 480 : 576) * (Stream.video_parms.scaling_type.param.free.ratio == tv_widescreen ? 0x1C71C : 0x15555) / source_aspect;
            }
            else
            {
              ideal_dest_height = (Module_Display525_60 ? 480 : 576);
            }
            preserve_aspect_ratio = true;
            break;
          case vm_zoom:
            source_width = Stream.active_horizontal_size;
            source_height = Stream.active_vertical_size;
            source_aspect = Stream.active_aspect_ratio;
            if (source_aspect == 0) source_aspect = 0x15555;
            if (source_aspect > (Stream.video_parms.scaling_type.param.free.ratio == tv_widescreen ? 0x1C71C : 0x15555))
            {
              ideal_dest_height = (Module_Display525_60 ? 480 : 576);
            }
            else
            {
              ideal_dest_height = (Module_Display525_60 ? 480 : 576) * (Stream.video_parms.scaling_type.param.free.ratio == tv_widescreen ? 0x1C71C : 0x15555) / source_aspect;
            }
            preserve_aspect_ratio = true;
            break;
          case vm_stretch:
            source_width = Stream.active_horizontal_size;
            source_height = Stream.active_vertical_size;
            ideal_dest_width = 720;
            ideal_dest_height = (Module_Display525_60 ? 480 : 576);
            preserve_aspect_ratio = false;
            break;
        }
        break;
      case st_fixed_width:
        source_width = Stream.active_horizontal_size;
        source_height = Stream.active_vertical_size;
        source_aspect = Stream.active_aspect_ratio;
        if (source_aspect == 0) source_aspect = 0x15555;
        ideal_dest_height = (Stream.video_parms.scaling_type.param.fixed_width.width >> 1) * (Module_Display525_60 ? 0x0E38E : 0x11111);
        if (Stream.video_parms.scaling_type.param.fixed_width.anamorphic)
        {
          ideal_dest_height = ideal_dest_height * 4 / (3 * source_aspect);
        }
        else
        {
          ideal_dest_height /= source_aspect;
        }
        preserve_aspect_ratio = true;
        break;
      case st_fixed_height:
        source_width = Stream.active_horizontal_size;
        source_height = Stream.active_vertical_size;
        source_aspect = Stream.active_aspect_ratio;
        if (source_aspect == 0) source_aspect = 0x15555;
        ideal_dest_height = Stream.video_parms.scaling_type.param.fixed_height.height >> 1;
        preserve_aspect_ratio = true;
        break;
      case st_fixed:
        source_width = Stream.active_horizontal_size;
        source_height = Stream.active_vertical_size;
        ideal_dest_width = Stream.video_parms.scaling_type.param.fixed.width >> 1;
        if (ideal_dest_width == 0) ideal_dest_width = 1;
        ideal_dest_height = Stream.video_parms.scaling_type.param.fixed.height >> 1;
        if (ideal_dest_height == 0) ideal_dest_height = 1;
        preserve_aspect_ratio = false;
        break;
    }
  }
  
  if (Module_Display525_60 || Stream.video_parms.scaling_type.type != st_free || (Stream.video_parms.scaling_type.type == st_free && Stream.video_parms.scaling_type.param.free.preference == vm_stretch))
  {
    allow_tv [tvm_collapsed]  = allow_tv [tvm_anamorphic] = allow_tv [tvm_14_9] = allow_tv [tvm_16_9] = false;
    allow_tv [tvm_full_frame] = true;
    if ((Stream.video_parms.scaling_type.type == st_fixed_width || Stream.video_parms.scaling_type.type == st_fixed_height) && Stream.video_parms.scaling_type.param.fixed_width.anamorphic)
    {
      allow_tv [tvm_full_frame] = false;
      allow_tv [tvm_anamorphic] = true;
    }
    if (Stream.video_parms.scaling_type.type == st_free && Stream.video_parms.scaling_type.param.free.preference == vm_stretch && Stream.video_parms.scaling_type.param.free.ratio == tv_widescreen)
    {
      allow_tv [tvm_full_frame] = false;
      allow_tv [tvm_anamorphic] = allow_tv [tvm_16_9] = true;
    }
  }
  else
  {
    allow_tv [tvm_collapsed]  = Stream.video_parms.scaling_type.param.free.ratio == tv_regular &&
                                (Stream.video_parms.scaling_type.param.free.use_SCART_pin_8 || Stream.video_parms.scaling_type.param.free.use_WSS) &&
                                Stream.video_parms.scaling_type.param.free.preference == vm_box &&
                                Stream.active_aspect_ratio >= 0x1B8E4 /* 15:9 */;
    allow_tv [tvm_full_frame] = Stream.video_parms.scaling_type.param.free.ratio == tv_regular ||
                                (Stream.video_parms.scaling_type.param.free.ratio == tv_widescreen &&
                                 (Stream.video_parms.scaling_type.param.free.use_SCART_pin_8 || Stream.video_parms.scaling_type.param.free.use_WSS) &&
                                 Stream.video_parms.scaling_type.param.free.preference == vm_box &&
                                 Stream.active_aspect_ratio <= 0x1638E /* 12:9 */);
    allow_tv [tvm_anamorphic] = Stream.video_parms.scaling_type.param.free.ratio == tv_widescreen;
    allow_tv [tvm_14_9]       = Stream.video_parms.scaling_type.param.free.ratio == tv_widescreen &&
                                Stream.video_parms.scaling_type.param.free.use_WSS &&
                                Stream.video_parms.scaling_type.param.free.preference == vm_box &&
                                Stream.active_aspect_ratio <= 0x19C72 /* 14:9 */;
    allow_tv [tvm_16_9]       = Stream.video_parms.scaling_type.param.free.ratio == tv_widescreen &&
                                Stream.video_parms.scaling_type.param.free.use_WSS;
  }
  
  for (mode = 0; mode < (vsf_LIMIT * tvm_LIMIT); mode++)
  {
    scaling_mode[mode].allowed = allow_tv[scaling_mode[mode].tv_factor] && (Stream.display_frames->progressive_frame || (scaling_mode[mode].stb_factor != vsf_x1_5 && scaling_mode[mode].stb_factor != vsf_x4));
    if (scaling_mode[mode].allowed)
    {
      last_allowed_scaling_mode = mode;
      if (first_allowed_scaling_mode == -1) first_allowed_scaling_mode = mode;
    }
  }
  
  vertical_upscaling = Fixed_IntDivide (ideal_dest_height, source_height);
  for (mode = first_allowed_scaling_mode; mode <= last_allowed_scaling_mode; mode++)
  {
    if (scaling_mode[mode].allowed && (vertical_upscaling > scaling_mode[mode].total_factor || mode == last_allowed_scaling_mode))
    {
      break;
    }
  }
  
  if (preserve_aspect_ratio)
  {
    fixed numerator;
    fixed denominator;
    numerator = (Module_Display525_60 ? 0x0E38E : 0x11111) * source_width;
    if (scaling_mode[mode].tv_factor == tvm_collapsed || scaling_mode[mode].tv_factor == tvm_anamorphic)
    {
      numerator = Fixed_FixedMultiply (numerator, 0x15555);
    }
    denominator = Fixed_FixedMultiply (source_aspect * source_height, upscaling_lut[scaling_mode[mode].stb_factor]);
    horizontal_downscaling = Fixed_FixedDivide (numerator, denominator);
  }
  else
  {
    horizontal_downscaling = Fixed_IntDivide (source_width - 1, ideal_dest_width - 1);
  }
  if (horizontal_downscaling > 0x20000) horizontal_downscaling = 0x20000;
  if (horizontal_downscaling < 0x100) horizontal_downscaling = 0x100;
  
  if (Stream.video_parms.scaling_type.type == st_free)
  {
    centre_x = 360 << 8;
    if (!active_formats[active_format].top)
    {
      /* Active area is at centre of coded area; position their common centre at the centre of the TV */
      centre_y = Module_Display525_60 ? (240 << 7) : (288 << 7);
    }
    else
    {
      /* Active area is at top of coded area; position their common top at the top of the TV */
      centre_y = (Stream.display_frames->seq.coded_vertical_size * upscaling_lut[scaling_mode[mode].stb_factor]) >> 10;
    }
    if (using_pan_and_scan_vectors)
    {
      bool horizontal_negative = (frame_centre_horizontal_offset < 0);
      bool vertical_negative = (frame_centre_vertical_offset < 0);
      fixed horizontal_offset = Fixed_FixedDivide (abs(frame_centre_horizontal_offset) << 4, horizontal_downscaling);
      fixed vertical_offset = Fixed_FixedMultiply (abs(frame_centre_vertical_offset) << 3, upscaling_lut[scaling_mode[mode].stb_factor]);
      if (horizontal_negative)
      {
        centre_x -= horizontal_offset;
      }
      else
      {
        centre_x += horizontal_offset;
      }
      if (vertical_negative)
      {
        centre_y -= vertical_offset;
      }
      else
      {
        centre_y += vertical_offset;
      }
    }
  }
  else
  {
    /* Use centre-x and centre-y from ScalingType parameters (in same place for ScalingTypes 1-3); adjust for VIDC borders and graphics origin */
    int in_block [3] = { 136, 137, -1 };
    int out_block [2];
    _swix (OS_ReadVduVariables, _INR(0,1), in_block, out_block);
    centre_x = (Stream.video_parms.scaling_type.param.fixed.x + Module_OriginXOffset + out_block [0]) * 128;
    centre_y = ((Module_Display525_60 ? 960 : 1152) - Stream.video_parms.scaling_type.param.fixed.y - Module_OriginYOffset - out_block [1]) * 64;
    if (active_formats[active_format].top)
    {
      /* Coded frame centre is below active area centre, so adjust centre_y appropriately */
      centre_y += ((Stream.display_frames->seq.coded_vertical_size - Stream.active_vertical_size) * upscaling_lut[scaling_mode[mode].stb_factor]) >> 7;
    }
  }
  
  /* Do lower-level work to set up scaling */
  static_SetUpScalingRegisters (centre_x, centre_y, horizontal_downscaling, scaling_mode[mode].stb_factor);
  
  /* Set up values for use in stat &1011, and issue UpCall &1002 if necessary */
  Stream.displayed_active_centre_x = centre_x >> 7;
  if (Stream.active_area_is_at_top)
  {
    Stream.displayed_active_centre_y = (centre_y - (((Stream.display_frames->seq.coded_vertical_size - Stream.active_vertical_size) * upscaling_lut[scaling_mode[mode].stb_factor]) >> 10)) >> 8;
  }
  else
  {
    Stream.displayed_active_centre_y = centre_y >> 8;
  }
  Stream.displayed_active_width = Fixed_FixedDivide (Stream.active_horizontal_size << 1, horizontal_downscaling);
  Stream.displayed_active_height = (Stream.active_vertical_size * upscaling_lut[scaling_mode[mode].stb_factor]) >> 17;
  if (Stream.active_horizontal_size != Stream.old_active_horizontal_size ||
      Stream.active_vertical_size != Stream.old_active_vertical_size ||
      Stream.active_aspect_ratio != Stream.old_active_aspect_ratio ||
      Stream.displayed_active_centre_x != Stream.old_displayed_active_centre_x ||
      Stream.displayed_active_centre_y != Stream.old_displayed_active_centre_y ||
      Stream.displayed_active_width != Stream.old_displayed_active_width ||
      Stream.displayed_active_height != Stream.old_displayed_active_height)
  {
    /* Use OS_CallAVector, because OS_UpCall enables interrupts */
    _swix (OS_CallAVector, _INR(0,3)|_IN(9), UpCall_MPEG, 0, Stream.csh, UpCallMPEG_ActiveArea, UpCallV);
  }
  Stream.old_active_horizontal_size = Stream.active_horizontal_size;
  Stream.old_active_vertical_size = Stream.active_vertical_size;
  Stream.old_active_aspect_ratio = Stream.active_aspect_ratio;
  Stream.old_displayed_active_centre_x = Stream.displayed_active_centre_x;
  Stream.old_displayed_active_centre_y = Stream.displayed_active_centre_y;
  Stream.old_displayed_active_width = Stream.displayed_active_width;
  Stream.old_displayed_active_height = Stream.displayed_active_height;
  
  /* Do overriding widescreen signalling, if necessary */
  if (Stream.video_parms.scaling_type.type == st_free && Stream.video_parms.scaling_type.param.free.use_SCART_pin_8)
  {
    if (scaling_mode[mode].tv_factor == tvm_collapsed || scaling_mode[mode].tv_factor == tvm_anamorphic)
    {
      scart_setting = scart_enable;
    }
    else
    {
      scart_setting = scart_disable;
    }
  }
  else
  {
    scart_setting = scart_dontcare;
  }
  WSS_SetSCART (NULL, NULL, NULL, &scart_setting);
  if (Stream.video_parms.scaling_type.type == st_free && Stream.video_parms.scaling_type.param.free.use_WSS)
  {
    switch (scaling_mode[mode].tv_factor)
    {
      default:
      case tvm_full_frame:
        group_settings [0] = 0 | 8;
        break;
      case tvm_collapsed:
      case tvm_anamorphic:
        group_settings [0] = 7;
        break;
      case tvm_14_9:
        if (active_formats[active_format].top)
        {
          group_settings [0] = 2;
        }
        else
        {
          group_settings [0] = 1;
        }
        break;
      case tvm_16_9:
        if (active_formats[active_format].top)
        {
          group_settings [0] = 4;
        }
        else
        {
          group_settings [0] = 3 | 8;
        }
        break;
    }
    group_settings [1] = Stream.display_frames->progressive_frame;
  }
  else
  {
    group_settings [0] = 0;
  }
  WSS_Set (group_settings, true, NULL, NULL);
  
  Stream.vid_flags.display_needs_setting_up = false;
}

/******	static_SetUpScalingRegisters() *************************************

Purpose:	Lower level than static_PositionVideo(), this takes the
		display_relative frame centre and the H/V scaling factors,
		and sets up the various registers to best achieve them
In:		Frame centre, in 256ths of a luma sample and 256ths of a
		field scanline; horizontal downscaling and vertical upscaling
		ratios, both of which have already been verified to be within
		the capabilities of the 3520

***************************************************************************/

static void static_SetUpScalingRegisters (signed int centre_x, signed int centre_y, fixed horizontal_downscaling, vertical_scaling_factor vertical_scaling_index)
{
  fixed vertical_upscaling = upscaling_lut [vertical_scaling_index];
  unsigned int display_width;  /* in 256ths of a luma sample */
  signed int display_x_start;  /* in 256ths of a luma sample */
  unsigned int pan_offset = 0; /* in 256ths of a luma sample */
  unsigned int real_display_x_start = 0; /* in whole luma samples */
  unsigned int real_display_x_end = 0;   /* in whole luma samples */
  unsigned int display_height; /* in 256ths of a field scanline */
  signed int display_y_start;  /* in 256ths of a field scanline (down from top) */
  unsigned int scan_offset = 0;      /* in scanlines output from vertical filter (field or frame scanlines) */
  unsigned int real_display_y_start; /* in whole field scanlines (down from top) */
  unsigned int real_display_y_end;   /* in whole field scanlines (down from top) */
  bool off_screen = false;
  unsigned int real_YDO = 0;
  unsigned int real_YDS = 0;
  static enum { none = 0, down = 1, up = 3 } UDS_lut [2][6] = { { down, none, none, none, up, none }, { down, none, none, none, none, up } };
  static unsigned char                       VFC_lut [2][6] = { { 0,    1,    0,    0xFF, 0,  0xFF }, { 2,    3,    2,    6,    6,    4  } };
  static enum { off = 0, on = (VID_DCF_LB>>16)|(VID_DCF_BFL>>16) } LB_lut [6] = { off,  on,   off,  on,   off,  off };
  bool filter_does_scaling;
  bool vertically_downscaled;
  bool letterbox_used;
  unsigned int lines_output_from_filter = Stream.display_frames->seq.coded_vertical_size;
  filter_does_scaling = (VFC_lut [Stream.display_frames->progressive_frame] [vertical_scaling_index] > 3);
  vertically_downscaled = (UDS_lut [Stream.display_frames->progressive_frame] [vertical_scaling_index] == down);
  letterbox_used = (LB_lut [vertical_scaling_index] == on);
  if (!filter_does_scaling) lines_output_from_filter /= 2;
  
  /* Calculate the "ideal" positions and scales */
  
  display_width = ((Stream.display_frames->seq.coded_horizontal_size - 1) << 16) / (horizontal_downscaling >> 8); /* in 256ths of a luma sample */
  display_x_start = centre_x - display_width / 2; /* in 256ths of a luma sample */
  display_height = ((Stream.display_frames->seq.coded_vertical_size / 2) * vertical_upscaling) >> 8; /* in 256ths of a field scanline */
  display_y_start = centre_y - display_height / 2; /* in 256ths of a field scanline */
  
  /* Calculate the actual horizontal parameters, adjusting for hardware deficiencies */
  
  if (display_x_start < 0)
  {
    real_display_x_start = 0;
  }
  else
  {
    real_display_x_start = ((display_x_start + 0x1FF) &~ 0x1FF) >> 8; /* start at first 2-sample boundary at or after the theoretical start */
    if (real_display_x_start >= 720)
    {
      off_screen = true;
    }
  }
  if ((display_x_start + (signed) display_width) <= 0)
  {
    off_screen = true;
  }
  else
  {
    real_display_x_end = (display_x_start + display_width) >> 8; /* automatically rounded down to the next whole luma sample */
    real_display_x_end = MIN(real_display_x_end, 708); /* Give the 3520 an extra .89 s gap between displaying the last sample and the HSync */
                                                       /* Without this, some combinations of horizontal scaling, left border and pan vector  */
                                                       /* can lead to horizontal dashes appearing in the resultant video!                    */
  }
  pan_offset = (real_display_x_start << 8) - display_x_start; /* 256ths of an (output) luma sample */
  pan_offset = Fixed_FixedMultiply (pan_offset, horizontal_downscaling); /* convert to 256ths of an input sample */
  if (letterbox_used && (pan_offset & 0x800) != 0)
  {
    /* The oh-so-wonderful 3520 corrupts the display if the letterbox circuitry is in use and it has to start in the latter half of a macroblock */
    unsigned int new_pan_offset_lwm = (pan_offset + 0x800) &~ 0x7FF;
    real_display_x_start += ((Fixed_IntDivide (new_pan_offset_lwm - pan_offset, horizontal_downscaling) + 0x1FF) &~ 0x1FF) >> 8;
    pan_offset = (real_display_x_start << 8) - display_x_start; /* 256ths of an (output) luma sample */
    pan_offset = Fixed_FixedMultiply (pan_offset, horizontal_downscaling); /* convert to 256ths of an input sample */
  }
  if (real_display_x_start >= real_display_x_end)
  {
    off_screen = true;
  }
  
  /* Calculate the actual vertical parameters, adjusting for hardware deficiencies */
  
  if (display_y_start < 0)
  {
    scan_offset = Fixed_FixedDivide (-display_y_start, vertical_upscaling); /* 256ths of an input sample */
    if (filter_does_scaling) scan_offset <<= 1; /* convert to 256ths lines output from the vertical filter */
    /* Round up the next highest whole number of scanlines */
    scan_offset = (scan_offset + 0xFF) >> 8;
    /* When downscaling, only scan even numbers of input lines */
    if (vertically_downscaled)
    {
      if ((scan_offset & 1) == 1) scan_offset += 1;
    }
    /* With 3/4 or 3/2 scaling, we get artifacts if VID_SCN isn't a multiple of 4 */
    else if (letterbox_used)
    {
      scan_offset = (scan_offset + 3) &~ 3;
    }
    if (scan_offset >= lines_output_from_filter)
    {
      /* In this case, the image is off the top of the screen */
      off_screen = true;
      real_display_y_start = 0;
    }
    else if (scan_offset < 0x200)
    {
      /* This is the normal case: convert back to 256ths of an output sample, and calculate real_display_y_start - may be beyond line 0! */
      unsigned int scaled_scan = Fixed_FixedMultiply (scan_offset << 8, vertical_upscaling);
      if (filter_does_scaling) scaled_scan >>= 1;
      real_display_y_start = (display_y_start + (signed) scaled_scan + 0x80) >> 8;
    }
    else
    {
      /* But when the size of VID_SCN is a limitation, just position the video as high as possible */
      real_display_y_start = 0;
      if (vertically_downscaled)
      {
        scan_offset = 0x1FE;
      }
      else if (letterbox_used)
      {
        scan_offset = 0x1FC;
      }
      else
      {
        scan_offset = 0x1FF;
      }
    }
  }
  else
  {
    real_display_y_start = (display_y_start + 0x80) >> 8;
    if (real_display_y_start > (Module_Display525_60 ? 240 : 288))
    {
      off_screen = true;
    }
  }
  real_display_y_end = Fixed_FixedMultiply (lines_output_from_filter - scan_offset, vertical_upscaling);
  if (filter_does_scaling) real_display_y_end >>= 1;
  real_display_y_end += real_display_y_start;
  if (vertical_scaling_index == vsf_x4) real_display_y_end -= 1; /* mask off the flickering last line that's present in this mode only */
  real_display_y_end = MIN(real_display_y_end, (Module_Display525_60 ? 240 : 288));
  
  /* Finally, write the registers */
  
  if (off_screen)
  {
    VID_DCF_0 &= ~VID_DCF_EVD>>0; /* disable display */
  }
  else
  {
    unsigned char UDS = (unsigned char) UDS_lut [Stream.display_frames->progressive_frame] [vertical_scaling_index];
    WRITE_VID_LSR((horizontal_downscaling >> 8) - 1);
    WRITE_VID_UDS(UDS);
    Stream.vid_flags.up_scaling_used = (UDS == 3);
    WRITE_VID_DCF_0((VID_DCF_0 & 0xF8) | VFC_lut [Stream.display_frames->progressive_frame] [vertical_scaling_index]);
    Deferred_VID_DCF_LB = LB_lut [vertical_scaling_index]; /* this bit isn't latched, so we have to write it on the next VSync */
    Deferred_VID_DCF_LB_Pending = true;
    if (Module_Display525_60)
    {
      WRITE_VID_XDO(102 + real_display_x_start - 1);   /* -1 to get the chroma phases right on this system */
      WRITE_VID_XDS(102 + real_display_x_end + 6 - 1);
      real_YDO = 21 + real_display_y_start;
      real_YDS = 21 + real_display_y_end - 1;
    }
    else
    {
      WRITE_VID_XDO(112 + real_display_x_start - 1);   /* -1 to get the chroma phases right on this system */
      WRITE_VID_XDS(112 + real_display_x_end + 6 - 1);
      real_YDO = 22 + real_display_y_start;
      real_YDS = 22 + real_display_y_end - 1;
    }
    if (real_YDO < 256)
    {
      DisplayFrameHackyScanOffset = 0;
      VID_YDO = real_YDO;
    }
    else
    {
      unsigned int size_of_row_pair = vertical_upscaling >> 12; /* 2 rows * 16 samples / 2 fields * fixed-point >> 16 */
      DisplayFrameHackyScanOffset = (real_YDO - 0xFF + size_of_row_pair - 1) / size_of_row_pair;
      VID_YDO = real_YDO - (DisplayFrameHackyScanOffset * size_of_row_pair);
    }
    if (real_YDS < 128)
    {
      VID_RYDS = 1;
      VID_YDS = real_YDS;
    }
    else
    {
      VID_RYDS = 0;
      VID_YDS = real_YDS - 128;
    }
    WRITE_VID_PAN(pan_offset >> 8);
    WRITE_VID_LSO(pan_offset);
    WRITE_VID_CSO(pan_offset >> 1);
    WRITE_VID_SCN(scan_offset);
    WRITE_VID_XFS(((Stream.display_frames->seq.coded_horizontal_size + 15) / 16) * ((Stream.display_frames->seq.coded_vertical_size + 15) / 16));
    WRITE_VID_XFW((Stream.display_frames->seq.coded_horizontal_size + 15) / 16);
    static_SetDFP ();
    VID_DCF_0 |= VID_DCF_EVD>>0; /* enable display */
  }
}

/******	static_SetDFP() ****************************************************

Purpose:	Sets VID_DFP and VID_XFA - part of the bodge to work around
		the fact that VID_YDO is only 8 bits wide

***************************************************************************/

static void static_SetDFP (void)
{
  if (DisplayFrameHackyScanOffset == 0)
  {
    WRITE_VID_DFP(Stream.display_frames->frame_buffer);
    WRITE_VID_XFA(0);
  }
  else
  {
    unsigned int luma_offset = ((Stream.display_frames->seq.coded_horizontal_size + 15) / 16) * DisplayFrameHackyScanOffset * 2;
    frame_t *best_frame = Stream.display_frames; /* emergency fallback */
    frame_t *rover;
    for (rover = Stream.display_frames; rover != NULL; rover = rover->display_link)
    {
      if (rover->lastpic->picture_coding_type != B_PICTURE)
      {
        best_frame = rover;
        break;
      }
    }
    WRITE_VID_DFP(best_frame->frame_buffer - luma_offset);
    WRITE_VID_XFA(luma_offset / 2);
  }
}

/******	static_ProcessCCData() *********************************************

Purpose:	Farms out the processing od closed caption data for an
		individual frame
In:		Pointer to the frame struct for the frame to process

***************************************************************************/

static void static_ProcessCCData (unsigned int handler, unsigned int flags, const char *data)
{
  if (!Stream.rs_flags.trick_play_mode)
  {
    unsigned int length = 4;
    if (data[2] == 0 && data[3] == 0)
    {
      length = 2;
      if (data[0] == 0 && data[1] == 0)
      {
        length = 0;
      }
    }
    if (length > 0)
    {
      MiscAsm_CallHandler (handler, flags, data, length);
    }
  }
}
