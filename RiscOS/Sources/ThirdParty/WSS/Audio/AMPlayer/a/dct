	AREA	|C$$code|, CODE, READONLY

;
; Inner dct64 loops solely in Assembler, using the fixed point multiplies
; (C) 2000 Robin Watts

#include "h.defs"

#include "a.asmhdr"

	EXPORT	dct64_1
	FUNC	dct64_1
	; r0 = fpl *samples
	; r1 = fpl **pnts
	; r2 = fpl *bufs

	STMFD	r13!,{r4-r11,r14}

	LDR	r6,[r1]			; r6 = pnts[0]
	MOV	r4,r0			; r4 = b1 = samples
	ADD	r5,r4,#32*4		; r5 = b2 = b1 + 32
	ADD	r6,r6,#16*4		; r6 = costab = pnts[0] + 16
					; r2 = bs = bufs

	MOV	r14,#16
loop
	LDR	r12,[r4],#4		; r12 = *b1++
	LDR	r11,[r5,#-4]!		; r11 = *--b2
	SUBS	r14,r14,#1
	ADD	r11,r12,r11
	STR	r11,[r2],#4		; *bs++
	BGT	loop

	MOV	r14,#16
loop2
	LDR	r12,[r4],#4		; r12 = *b1++
	LDR	r11,[r5,#-4]!		; r11 = *--b2
	LDR	r10,[r6,#-4]!
	SUB	r12,r11,r12
	SMULL	r10,r11,r12,r10
	SUBS	r14,r14,#1
	MOV	r10,r10,LSR#32-5	; r10 retired first
	ORR	r10,r10,r11,LSL#5
	STR	r10,[r2],#4		; *bs++
	BGT	loop2

	LDR	r6,[r1,#4]		; r6 = pnts[1]
	SUB	r4,r2,#32*4		; r4 = b1 = bufs
	ADD	r5,r4,#16*4		; r5 = b2 = b1 + 16
	ADD	r6,r6,#8*4		; r6 = pnts[1] + 8

	MOV	r14,#8
loop3
	LDR	r12,[r4],#4		; r12 = *b1++
	LDR	r11,[r5,#-4]!		; r11 = *--b2
	SUBS	r14,r14,#1
	ADD	r11,r12,r11
	STR	r11,[r2],#4		; *bs++
	BGT	loop3

	MOV	r14,#8
loop4
	LDR	r12,[r4],#4		; r12 = *b1++
	LDR	r11,[r5,#-4]!		; r11 = *--b2
	LDR	r10,[r6,#-4]!
	SUB	r12,r11,r12
	SMULL	r10,r11,r12,r10
	SUBS	r14,r14,#1
	MOV	r10,r10,LSR#32-5	; r10 retired first
	ORR	r10,r10,r11,LSL#5
	STR	r10,[r2],#4		; *bs++
	BGT	loop4

	ADD	r5,r5,#32*4		; r5 = b2 += 32
	ADD	r6,r6,#8*4		; r6 = costab += 8
	MOV	r14,#8
loop5
	LDR	r12,[r4],#4		; r12 = *b1++
	LDR	r11,[r5,#-4]!		; r11 = *--b2
	SUBS	r14,r14,#1
	ADD	r11,r12,r11
	STR	r11,[r2],#4		; *bs++
	BGT	loop5

	MOV	r14,#8
loop6
	LDR	r12,[r4],#4		; r12 = *b1++
	LDR	r11,[r5,#-4]!		; r11 = *--b2
	LDR	r10,[r6,#-4]!
	SUB	r12,r12,r11
	SMULL	r10,r11,r12,r10
	SUBS	r14,r14,#1
	MOV	r10,r10,LSR#32-5	; r10 retired first
	ORR	r10,r10,r11,LSL#5
	STR	r10,[r2],#4		; *bs++
	BGT	loop6

	SUB	r2,r2,#64*4		; r2 = bs = bufs
	ADD	r5,r4,#8*4		; r5 = b2 = b1 + 8
	LDR	r6,[r1,#8]		; r6 = costab = pnts[2]

	MOV	r9,#2			; r9 = j = 2
loop7
	MOV	r14,#4
loop8
	LDR	r12,[r4],#4		; r12 = *b1++
	LDR	r11,[r5,#-4]!		; r11 = *--b2
	SUBS	r14,r14,#1
	ADD	r12,r11,r12		; r11 = *b1++ + *--b2
	STR	r12,[r2],#4
	BGT	loop8

	MOV	r14,#4
	ADD	r6,r6,#16
loop9
	LDR	r12,[r4],#4		; r12 = *b1++
	LDR	r11,[r5,#-4]!		; r11 = *--b2
	LDR	r10,[r6,#-4]!		; r10 = costab[i]
	SUB	r12,r11,r12		; r11 = *--b2 + *b1++
	SMULL	r10,r11,r12,r10
	SUBS	r14,r14,#1
	MOV	r10,r10,LSR#32-5	; r10 retired first
	ORR	r10,r10,r11,LSL#5
	STR	r10,[r2],#4		; *bs++
	BGT	loop9

	ADD	r5,r5,#16*4		; r11 = b2 += 16

	MOV	r14,#4
loop10
	LDR	r12,[r4],#4		; r12 = *b1++
	LDR	r11,[r5,#-4]!		; r11 = *--b2
	SUBS	r14,r14,#1
	ADD	r12,r11,r12		; r11 = *b1++ + *--b2
	STR	r12,[r2],#4
	BGT	loop10

	MOV	r14,#4
	ADD	r6,r6,#16
loop11
	LDR	r12,[r4],#4		; r12 = *b1++
	LDR	r11,[r5,#-4]!		; r11 = *--b2
	LDR	r10,[r6,#-4]!		; r10 = costab[i]
	SUB	r12,r12,r11		; r11 = *b1++ - *--b2
	SMULL	r10,r11,r12,r10
	SUBS	r14,r14,#1
	MOV	r10,r10,LSR#32-5	; r10 retired first
	ORR	r10,r10,r11,LSL#5
	STR	r10,[r2],#4		; *bs++
	BGT	loop11

	ADD	r5,r5,#16*4

	SUBS	r9,r9,#1
	BGT	loop7

	SUB	r4,r2,#32*4		; r4 = b1 = bufs
	ADD	r5,r4,#4*4-4		; r5 = b1 + 4
	LDR	r6,[r1,#12]		; r6 = costab = pnts[3]

	MOV	r14,#4
	LDMIA	r6,{r6,r7}		; r6 = costab[0]
					; r7 = costab[1]
loop12
	LDMDA	r5!,{r9,r10}		; r10= *--b2
					; r9 = *--b2
	LDMIA	r4!,{r11,r12}
	ADD	r11,r10,r11		; r11 = *b1++ + *--b2
	ADD	r12,r9,r12		; r12 = *b1++ + *--b2
	LDR	r9,[r5],#-4		; r9 = *--b2
        LDR	r10,[r4],#4		; r10 = *b1++
        STMIA	r2!,{r11,r12}
        SUB	r10,r9,r10		; r10 = *--b2 - *b1++
        SMULL	r12,r11,r10,r7
	LDR	r9,[r5],#8*4-4		; r9 = *--b2
        LDR	r10,[r4],#4		; r10 = *b1++
	MOV	r12,r12,LSR#32-5
	ORR	r11,r12,r11,LSL#5
        SUB	r10,r9,r10		; r11 = *--b2 - *b1++
        SMULL	r10,r12,r6,r10
	; Stall
	MOV	r10,r10,LSR#32-5
	ORR	r12,r10,r12,LSL#5
	STMIA	r2!,{r11,r12}

	LDMDA	r5!,{r9,r10}		; r10= *--b2
					; r9 = *--b2
	LDMIA	r4!,{r11,r12}
	ADD	r11,r10,r11		; r11 = *b1++ + *--b2
	ADD	r12,r9,r12		; r12 = *b1++ + *--b2
	LDR	r9,[r5],#-4		; r9 = *--b2
        LDR	r10,[r4],#4		; r10 = *b1++
        STMIA	r2!,{r11,r12}
        SUB	r10,r10,r9		; r10 = *b1++ - *--b2
        SMULL	r12,r11,r10,r7
	LDR	r9,[r5],#8*4-4		; r9 = *--b2
        LDR	r10,[r4],#4		; r10 = *b1++
	MOV	r12,r12,LSR#32-5
	ORR	r11,r12,r11,LSL#5
        SUB	r10,r10,r9		; r11 = *b1++ - *--b2
        SMULL	r10,r12,r6,r10
	SUBS	r14,r14,#1
	MOV	r10,r10,LSR#32-5
	ORR	r12,r10,r12,LSL#5
	STMIA	r2!,{r11,r12}

	BGT	loop12

	LDR	r6,[r1,#16]		; r6 = costab[4]
	SUB	r2,r2,#64*4		; r2 = bs = bufs
	LDR	r12,[r6]		; r12= *costab
	MOV	r14,#8
loop13
	LDMIA	r4!,{r6,r7,r8,r9}	; r6 = v0
					; r7 = v1
					; r8 = v2
					; r9 = v3
	ADD	r6,r6,r7		; r6 = v0 + v1
	SUB	r7,r6,r7,LSL#1		; r7 = v0 - v1
	SMULL	r7,r11,r12,r7
	ADD	r9,r8,r9		; r8 = v2 + v3
	SUB	r10,r9,r8,LSL#1		; r9 = v3 - v2
	MOV	r7,r7,LSR#32-5
	ORR	r7,r7,r11,LSL#5
	SMULL	r10,r11,r12,r10
	SUBS	r14,r14,#1
	MOV	r10,r10,LSR#32-5
	ORR	r10,r10,r11,LSL#5
	STMIA	r2!,{r6,r7,r9,r10}
	BGT	loop13

	SUB	r4,r2,#8*4*4-8		; r4 = b1 = &bufs[2]
	MOV	r14,#8
loop14
	LDMIA	r4,{r6,r7}
	SUBS	r14,r14,#1
	ADD	r6,r6,r7		; b1[2] += b[3]
	STR	r6,[r4],#16		; b1 += 4
	BGT	loop14

	SUB	r4,r2,#8*4*4 - 4*4	; r4 = b1 = &bufs[4]
	MOV	r14,#4
loop15
	LDMIA	r4,{r6,r7,r8,r9}
	SUBS	r14,r14,#1
	ADD	r6,r6,r8		; b1[4] += b1[6]
	ADD	r8,r8,r7		; b1[6] += b1[5]
	ADD	r7,r7,r9		; b1[5] += b1[7]
	STMIA	r4,{r6,r7,r8}
	ADD	r4,r4,#8*4
	BGT	loop15

	SUB	r2,r2,#8*4*4 - 8*4	; r2 = b1 = &bufs[8]
	MOV	r14,#2
loop16
	LDMIA	r2,{r4,r5,r6,r7,r8,r9,r10,r11}
					; r4 = b1[8]
					; r5 = b1[9]
					; r6 = b1[10]
					; r7 = b1[11]
					; r8 = b1[12]
					; r9 = b1[13]
					; r10= b1[14]
					; r11= b1[15]
	ADD	r4,r4,r8		; b1[8]  += b1[12]
	ADD	r8,r8,r6		; b1[12] += b1[10]
	ADD	r6,r6,r10		; b1[10] += b1[14]
	ADD	r10,r10,r5		; b1[14] += b1[9]
	ADD	r5,r5,r9		; b1[9]  += b1[13]
	ADD	r9,r9,r7		; b1[13] += b1[11]
	ADD	r7,r7,r11		; b1[11] += b1[15]
	STMIA	r2,{r4,r5,r6,r7,r8,r9,r10}
	ADD	r2,r2,#16*4
	SUBS	r14,r14,#1
	BGT	loop16

 [ ATPCS32
	LDMFD	r13!,{r4-r11,PC}
 |
	LDMFD	r13!,{r4-r11,PC}^
 ]

	END
